## Lab2
### 2.1. 过滤和联接
SimpleDB OpIterator 类实现了关系代数的运算。现在，您将实现两个运算符，这两个运算符将使您能够执行比表扫描稍微有趣的查询。

-   _Filter_：此运算符仅返回满足指定为其构造函数一部分的 元组。因此，它会筛选出任何与谓词不匹配的元组。`Predicate`
    
-   _Join_：此运算符根据作为其构造函数的一部分传入的 a 来连接其两个子级的元组。我们只需要一个简单的嵌套循环联接，但您可以探索更有趣的联接实现。在实验室文章中描述您的实现。`JoinPredicate`

实现
-   src/java/simpledb/execution/Predicate.java
-   src/java/simpledb/execution/JoinPredicate.java
-   src/java/simpledb/execution/Filter.java
-   src/java/simpledb/execution/Join.java


Predicate.java(判断该条数据是否能通过条件查询)
```markdown
where t.getField(field) op operand
* @param field  
*            field number of passed in tuples to compare against.  
* @param op  
*            operation to use for comparison  
* @param operand  
*            field value to compare passed in tuples to
```
主要是完成`public boolean filter(Tuple t)`这个方法

JoinPredicate.java(内联查询)
```markdown
tuple1.getField(field1) == tuple2.getField(field2)
* @param field1  
*            The field index into the first tuple in the predicate  
* @param field2  
*            The field index into the second tuple in the predicate  
* @param op  
*            The operation to apply (as defined in Predicate.Op); either  
*            Predicate.Op.GREATER_THAN, Predicate.Op.LESS_THAN,  
*            Predicate.Op.EQUAL, Predicate.Op.GREATER_THAN_OR_EQ, or  
*            Predicate.Op.LESS_THAN_OR_EQ
```
主要是完成`public boolean filter(Tuple t)`这个方法

Filter.java(完成where的整体逻辑)
本质就是拿着Predicate对数据源整体数据一条条判断
```java
/**  
 * AbstractDbIterator.readNext implementation. Iterates over tuples from the * child operator, applying the predicate to them and returning those that * pass the predicate (i.e. for which the Predicate.filter() returns true.) * * @return The next tuple that passes the filter, or null if there are no  
 *         more tuples * @see Predicate#filter  
 */protected Tuple fetchNext() throws NoSuchElementException,  
        TransactionAbortedException, DbException {  
  
    while(opIterator.hasNext()){  
        Tuple tuple = opIterator.next();  
        if(tuple != null && predicate.filter(tuple)){  
            return tuple;  
        }  
    }  
    return null;  
}
```

Join.java
对传进来的两个数据源进行笛卡尔积,过程中拿着JoinPredicate进行判断
当然了,由于是内连接,设计两张表数据的连接,所以存储的tuple的tupleDesc还要调用TupleDesc.merge
```java
/**  
 * Returns the next tuple generated by the join, or null if there are no * more tuples. Logically, this is the next tuple in r1 cross r2 that * satisfies the join predicate. There are many possible implementations; * the simplest is a nested loops join. * <p>  
 * Note that the tuples returned from this particular implementation of Join * are simply the concatenation of joining tuples from the left and right * relation. Therefore, if an equality predicate is used there will be two * copies of the join attribute in the results. (Removing such duplicate * columns can be done with an additional projection operator if needed.) * <p>  
 * For example, if one tuple is {1,2,3} and the other tuple is {1,5,6}, * joined on equality of the first column, then this returns {1,2,3,1,5,6}. * * @return The next matching tuple.  
 * @see JoinPredicate#filter  
 */protected Tuple fetchNext() throws TransactionAbortedException, DbException {  
    //TODO : 可能存在逻辑漏洞? 但我目前没发现  
  
  
 TupleDesc tupleDesc1 = child1.getTupleDesc();  
    TupleDesc tupleDesc2 = child2.getTupleDesc();  
  
    while(tuple1 != null || child1.hasNext()){  
        if(tuple1 == null){  
            tuple1 = child1.next();  
        }  
  
        if(!child2.hasNext()){  
            if(child1.hasNext()){  
                child2.rewind();  
                tuple1 = child1.next();  
            }else{  
                return null;  
            }  
        }  
  
        while(child2.hasNext()){  
            Tuple tuple2 = child2.next();  
  
            if(joinPredicate.filter(tuple1,tuple2)){  
                Tuple res = new Tuple(getTupleDesc());  
                for(int i = 0; i < tuple1.getTupleDesc().numFields(); i++){  
                    res.setField(i, tuple1.getField(i));  
                }  
                for(int i = 0; i < tuple2.getTupleDesc().numFields(); i++){  
                    res.setField(tuple1.getTupleDesc().numFields() + i, tuple2.getField(i));  
                }  
                return res;  
            }  
        }  
    }  
  
    return null;  
}
```

### 2.2. 聚合和分组
另一个 SimpleDB 运算符使用子句实现基本 SQL 聚合。您应该实现五个 SQL 聚合 （， 、 ， ）， 并支持分组。您只需要支持单个字段的聚合，并按单个字段进行分组。`GROUP BY``COUNT``SUM``AVG``MIN``MAX`

为了计算聚合，我们使用一个接口，该接口将新元组合并到聚合的现有计算中。在施工期间被告知它应该使用什么操作进行聚合。随后，客户端代码应调用子迭代器中的每个元组。合并所有元组后，客户端可以检索聚合结果的 OpIterator。结果中的每个元组都是一对形式，除非按字段分组的值为，在这种情况下，结果是该形式的单个元组。`Aggregator``Aggregator``Aggregator.mergeTupleIntoGroup()``(groupValue, aggregateValue)``Aggregator.NO_GROUPING``(aggregateValue)`

实现
-   src/java/simpledb/execution/IntegerAggregator.java
-   src/java/simpledb/execution/StringAggregator.java
-   src/java/simpledb/execution/Aggregate.java

IntegerAggregator.java
不要慌! gbfield代表 group by tuple的第x个属性进行分组；gbfieldtype 代表 第x个属性的类型
afield是 tuple 第 x 条属性要进行聚合操作
利用what 和 策略模式完成 handler的选择

用 handler中的concurrentHash存储结果（比较方便，毕竟聚合分组结果是 key ： value）
调用 handler的mergeTupleIntoGroup方法进行传入的tuple的判断
```java
    /**
     * Aggregate constructor
     * 
     * @param gbfield
     *            the 0-based index of the group-by field in the tuple, or
     *            NO_GROUPING if there is no grouping
     * @param gbfieldtype
     *            the type of the group by field (e.g., Type.INT_TYPE), or null
     *            if there is no grouping
     * @param afield
     *            the 0-based index of the aggregate field in the tuple
     * @param what
     *            the aggregation operator
     */
```
```java
    /**
     * Merge a new tuple into the aggregate, grouping as indicated in the
     * constructor
     * 
     * @param tup
     *            the Tuple containing an aggregate field and a group-by field
     */
    public void mergeTupleIntoGroup(Tuple tup) {

        //如果groub by 的值不为null && 分组的属性类型不一致
        if(gbfieldtype != null && !(tup.getField(gbfieldIndex).getType().equals(gbfieldtype))){
            throw new IllegalArgumentException("Given tuple has wrong type");
        }

        String key;
        if(gbfieldIndex == NO_GROUPING){
            key = NO_GROUPING_KEY;
        }else{
            key = tup.getField(gbfieldIndex).toString();
        }

        gbHandler.handle(key, tup.getField(afieldIndex));
    }
```
```java
    /**
     * Create a OpIterator over group aggregate results.
     * 
     * @return a OpIterator whose tuples are the pair (groupVal, aggregateVal)
     *         if using group, or a single (aggregateVal) if no grouping. The
     *         aggregateVal is determined by the type of aggregate specified in
     *         the constructor.
     */
    public OpIterator iterator() {

        Map<String, Integer> results = gbHandler.getGbResult();
        Type[] types;
        String[] names;
        TupleDesc tupleDesc;

        List<Tuple> tuples = new ArrayList<>();
        
        // 没有分组，结果自然只有一条
        if(gbfieldIndex == NO_GROUPING){
            types = new Type[]{Type.INT_TYPE};
            names = new String[]{"aggregateVal"};
            tupleDesc = new TupleDesc(types, names);

            Integer res = results.get(NO_GROUPING_KEY);
            Tuple tuple = new Tuple(tupleDesc);
            tuple.setField(0, new IntField(res));
            tuples.add(tuple);
        }else{
            
            // 有分组，答案可能有多条
            types = new Type[]{gbfieldtype,Type.INT_TYPE};
            names = new String[]{"groupVal","aggregateVal"};
            tupleDesc = new TupleDesc(types,names);
            for(Map.Entry<String,Integer> entry:results.entrySet()){
                Tuple tuple = new Tuple(tupleDesc);
                if(gbfieldtype==Type.INT_TYPE){
                    tuple.setField(0,new IntField(Integer.parseInt(entry.getKey())));
                }else{
                    tuple.setField(0,new StringField(entry.getKey(),entry.getKey().length()));
                }
                //由于integer 是具有 max,min,avg等聚合函数, string 就只有count
                tuple.setField(1,new IntField(entry.getValue()));
                tuples.add(tuple);
            }
        }


        //这里用TupleIterator, 思考为什么? 我认为原因类似 Integer包装类, 可以更加灵活,eg:记录tupleDesc等信息
        return new TupleIterator(tupleDesc,tuples);
```


Aggregate.java
```java
    private OpIterator child;
    private int afieldIdx;
    private int gfieldIdx;
    private Aggregator.Op aop;

    private Aggregator aggregator;
    private OpIterator resIterator;
    private TupleDesc aggreDesc;


	public Aggregate(OpIterator child, int afield, int gfield, Aggregator.Op aop){
		...

		// 分组值的判断
        Type gbType = gfield == Aggregator.NO_GROUPING? null : child.getTupleDesc().getFieldType(gfield);

		//聚合类的选择
        switch (child.getTupleDesc().getFieldType(afield)){
            case INT_TYPE:
                aggregator = new IntegerAggregator(gfieldIdx, gbType, afieldIdx, aop);
                break;
            case STRING_TYPE:
                aggregator = new StringAggregator(gfieldIdx, gbType, afieldIdx, aop);
                break;
            default:
                throw new IllegalArgumentException("Unknown type");
        }
	}
```

```java
 public void open() throws NoSuchElementException, DbException,
            TransactionAbortedException {
        super.open();

        //数据源打开,开始进行聚合分组计算
        try {
            child.open();

            while (child.hasNext()) {
                aggregator.mergeTupleIntoGroup(child.next());
            }
        } catch (DbException e) {
            e.printStackTrace();
        } catch (TransactionAbortedException e) {
            e.printStackTrace();
        } finally {
            child.close();
        }

		//这里遍历的数据类似于快照的数据
        resIterator = aggregator.iterator();
        resIterator.open();
    }
```
## Lab3
### 3.1

## Lab4
迭代器在遍历时只能通过迭代器修改,否则将抛出异常,modcount对不上
但flushAllPages要刷的可能很多,不可能这个方法中发现脏页,再flush这个方法中通过遍历去找这个脏页,所以用一个List装脏页,然后依次取脏页给flush这个方法,直接根据pid就可以找到对应的脏页,刷到磁盘中


玛德,这个放page的是linkedList,自带LFU算法,每一次get都会改变page的位置
```java
    /**
     * Flush all dirty pages to disk.
     * NB: Be careful using this routine -- it writes dirty data to disk so will
     *     break simpledb if running in NO STEAL mode.
     */
    public synchronized void flushAllPages() throws IOException {

//
//        for(Map.Entry<Integer,Page> entry : pages.entrySet()){
//            Page page = entry.getValue();
//            if(page.isDirty() != null){
//                flushPage(page.getId());
//            }
//        }

        List<Page> dirtyPage = new ArrayList<>();
        for(Page page : pages.values()) {
            if (page.isDirty() != null) {
                dirtyPage.add(page);
            }
        }

        for (int i = 0; i < dirtyPage.size(); i++) {
            flushPage(dirtyPage.get(i).getId());
        }
    }

    /** Remove the specific page id from the buffer pool.
        Needed by the recovery manager to ensure that the
        buffer pool doesn't keep a rolled back page in its
        cache.
        
        Also used by B+ tree files to ensure that deleted pages
        are removed from the cache so they can be reused safely
    */
    public synchronized void discardPage(PageId pid) {
        // some code goes here
        // not necessary for lab1
    }

    /**
     * Flushes a certain page to disk
     * @param pid an ID indicating the page to flush
     */
    private synchronized void flushPage(PageId pid) throws IOException {

        Page page = pages.get(pid.hashCode());
        DbFile dbFile = Database.getCatalog().getDatabaseFile(page.getId().getTableId());
        dbFile.writePage(page);
        page.markDirty(false,null);

//        Page page = null;
//        for(Map.Entry<Integer, Page> entry : pages.entrySet()) {
//            Integer key = entry.getKey();
//            if (key == pid.hashCode()) {
//                page = entry.getValue();
//                break;
//            }
//        }
////
//        DbFile file = Database.getCatalog().getDatabaseFile(page.getId().getTableId());
////        //将脏页保存下来再刷入磁盘
////        Database.getLogFile().logWrite(page.isDirty(), page.getBeforeImage(), page);
////        Database.getLogFile().force();
//        file.writePage(page);
//        page.markDirty(false, null);
    }

    /** Write all pages of the specified transaction to disk.
     */
    public synchronized  void flushPages(TransactionId tid) throws IOException {

        for(Page page : pages.values()){
            flushPage(page.getId());
        }
//        for(Map.Entry<Integer,Page> entry : pages.entrySet()){
//            Page page = entry.getValue();
//
//            page.setBeforeImage();
//
//            if(page.isDirty() == tid){
//                flushPage(page.getId());
//            }
//        }
    }
```





## 锁的粒度

```java
LockManager是由一个 ConcurrentHashMap<??,ConcurrentHashMap<TransactionId, PageLock>>维护的,

?? 
1.换为 PageId,就是页级锁
2.换为 RecordId就是tuple锁

开始写LockManager


```