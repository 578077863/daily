# 网络层
## ICMP协议

**ICMP互联网控制报文协议**，主要就是确定IP报文是否到达目标主机，如果ip包被丢弃了，那么为什么会被丢弃（端口无效还是主机不可达…），死也要死的有意义。被用于主机与路由器之间沟通彼此的网络层信息。

> ICMP通常被认为是IP的一部分，但是它和TCP/UDP一样，是作为IP的有效载荷而存在的。

Icmp主要分两类：出错，没出错。  
**没出错**就去就是正常的**请求应答**，通常是作为一个探测的目的去使用，如ping命令。**出错**就是回传**差错报告**报文

## ping

**Ping（packet Internet groper分组网间探测）**的输出包含三部分：  
【1】对主机对应的IP地址进行域名解析，向该主机发送的数据包大小  
【2】来自主机的响应（ICMP序列号-响应的数据包大小-请求往返耗时-IP数据报的ttl设置）  
【3】ping整体请求/响应概览，最小、平均、最大往返时间  
![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2F875f4754284342d5a95d0b97413c903d.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZHJvaWRzYW5zZmFsbGJhY2s%2Cshadow_50%2Ctext_Q1NETiBA4p2A56We6Iqx4p2A%2Csize_20%2Ccolor_FFFFFF%2Ct_70%2Cg_se%2Cx_16)Ping命令基于ICMP实现，客户端向服务器发出ICMP **echo请求报文**并等待接收**echo回应报文**，程序会按时间和成功响应的次数估算丢包率和网络时延。  
ping用于检查网络的连通性或者网络连接速度。  
（Telnet用来探测指定ip是否开放指定端口，可以看作最简单的端口探测方式）

**echo（reply/request）请求应答报文**用于判断一个ip报文是否达到对端。是实现Ping的原理  
当路由器无法将ip报文送到目标ip地址时，则会回传一个**目的不可达ICMP差错报文**，并且将原因保存在ICMP头中（协议自解释的代码字段）

Ping执行的时候，源主机构造多个**ICMP echo request（回传请求）报文**，并且通过序号字段标识。报文数据部分插入了**发送时间**，用于计算RTT。然后交给网络层构造一个ip数据报发出，如果到达目标主机则目标主机构造多个**ICMP echo reply（回送响应）报文**发送回去，与源报文序号一一对应。  
如果在规定的时间内，源主机没有收到ICMP应答报文，说明目标主机不可达，如果接收到ICMP Echo reply则说明目标主机可达

> IP的TTL字段每经过一次路由器就会减一（这里的减一，本质上是不断在网络节点更新ip头部），直到减少为0则丢弃包，并传回一个ICMP超时信息。

总结：ping程序基于ICMP echo request报文和ICMP echo reply报文。

**两台电脑ping不通的可能原因**：  
【1】检查**网络连接**是否正常，检查网卡驱动是否正确安装  
【2】查看局域设置，检查IP地址是否正确  
【3】查看**防火墙设置**（部分防火墙过滤ICMP）  
【4】检查是否存在第三方软件拦截  
【5】两条设备之间的**网络延时**是否够大（例如路由设置不合理），导致ICMP报文总是超时

## traceroute

traceRoute是基于ICMP另一个报文类型：差错报告报文。

该程序**故意设置特殊的TTL**，来追踪去往目的主机过程中经过的路由器。  
程序将ip报文的TTL设置为1，并且依次递增。同时发送UDP报文，一旦超时将接收ICMP超时报文（每经过一跳，TTL就减一，因此TTL减一的前提是遇到了路由器，而且TTL每次自增一个单位，因此**每当traceRoute程序收到一个ICMP差错报文就说明发现了一个路由器**），**最终的结果就是某个ip报文成功到达目标主机，而此时程序收到了若干个ICMP超时报文**，分别对应一个路由器（不精准，因为有的路由器不返回ICMP报文）。

traceRoute发送的是UDP报文，而路由器看不见传输层，路由器只关心TTL是否为1，如果路由器收到TTL为1的ip报文，则丢弃它并回传一个ICMP超时报文。如果UDP报文达到目标主机，将会返回一个**端口不可达ICMP报文**，因为**发送的UDP报文将目标端口写入了一个不可能到达的端口号（大于3000）**。  
当程序接收到一个端口不可达ICMP报文时，它就知道了发送的UDP达到了目的主机

traceRoute还有另外一个作用：**故意设置不分片，来发现路径的MTU（最大传输单元）——路径MTU发现**。

首先发送方主机发送IP数据报时，禁止IP数据报分片，一旦数据报长度大于路径MTU时就会分片，**如果数据报禁止被分片就会丢弃这个数据包，并回传ICMP差错报文，包含MTU**。消息类型“**需要分片但禁止分片**”，发送方依次调节包大小来确定合适的MTU（包含IP头），最终发送方可以推断出合适的MTU

> 每个不同类型的数据链路的使用目的不同，可承载的MTU也不同。  
> 常见的数据链路是以太网，MTU为1500字节。当IP数据包大小大于MTU时就会被分片。经过分片之后的ip数据包再被重组的时候，只能在目标主机上进行，路由器不会进行重组。一旦某个分片丢失，整个IP数据包都会作废，因此TCP引入了MSS，仅在传输层进行分配，而IP层不分片。

## IP地址

> 这玩意硬记肯定不好记，要知道划分原理

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2Ff37ced5eff9c4426b86658e74220b9c5.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZHJvaWRzYW5zZmFsbGJhY2s%2Cshadow_50%2Ctext_Q1NETiBA4p2A56We6Iqx4p2A%2Csize_20%2Ccolor_FFFFFF%2Ct_70%2Cg_se%2Cx_16)私有IP地址：  
![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2F7389b60432dd4e979d9cac3e370dabf0.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZHJvaWRzYW5zZmFsbGJhY2s%2Cshadow_50%2Ctext_Q1NETiBA4p2A56We6Iqx4p2A%2Csize_20%2Ccolor_FFFFFF%2Ct_70%2Cg_se%2Cx_16)

A：  
**A类地址由1字节网络地址和3字节主机地址组成，网络号高位固定为0**。其中全0和全1分别作为保留地址  
1.0.0.1 126.255.255.254  
(0000 0001.00000000.00000000.00000001~01111110.111111111.11111111.11111110）

B：  
**B类地址由2字节网络地址和2字节主机地址组成，网络号高位固定为10**  
128.1.0.1 191.255.255.254

C：  
**C类地址由3字节网络地址和1字节主机地址组成，网络号高位固定为110**  
192.0.1.1 223.255.255.254

> 全0的网络号代指本网络（this），全0的主机号代指本主机。全1的网络号作为回环地址（127.0.0.1），全1的主机号表示该网络上的所有主机。

**划分子网掩码**：  
划分子网：**从网络的主机号借用若干位作为子网号**，于是二级IP地址在本单位内部就变成三级IP地址。**对外仍然表现为一个网络**。  
通过子网掩码可以得到网络地址——**子网掩码和IP地址诸位相与**，就可以得到子网网络地址。

路由器中：  
【1】目的网络地址  
【2】下一跳地址（gate ）  
【3】目的网络子网掩码

**无分类编址CIDR**消除了传统的A/B/C类地址以及划分子网的概念以及划分子网的概念。重新将IP地址划分成了两个部分ip地址::={ <网络前缀> <主机号> }

CIDR记法：**ip地址/网络前缀** 如128.14.32.0/20（后面的数字就是地址掩码中1的个数）  
CIDR把网络前缀都相同的连续IP地址组成一个CIDR地址块  
![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2F8086cc4e22814a4c8111ec07036c7815.png)

前缀有多少位，地址掩码中就有多少个1。（CIDR编址中，子网号可以全0或者全1）

> 使用CIDR的时候，查找路由表可能得到几个匹配结果，应该选择**最长网络前缀的路由**。前缀越长，地址块越小，主机号越小，路由越具体。【选择多个匹配地址中的更具体的一方】

## 路由交换

Ip数据报到达路由器，路由器查找路由表，根据**路由选择协议**找到一个合适的端口将数据报发出。一个ip数据报到达路由器要经过拆包和重新装包，更新跳数和TTL等属性。其中不变的最终ip地址，变化的是源地址和链路层的MAC源、目标地址等。

当路由器接收都一个数据报的时候：  
【1】从IP数据报中提前**目的ip地址**，**对路由器直接相连的各个网络进行逐一检查**——**使用各网络的子网掩码与目标id地址诸位相与**。  
【2】判断目的IP地址所在网络是否与当前路由器直连，是则**直接交付**。  
【3】检查路由表中是否有目的IP地址的特定主机路径，有则重新加上头部更新TTL、MAC等，发送到下一跳路由器。  
【4】逐条检查路由表，如果找到匹配路由，则转发给下一跳路由器，如果设置了**默认路由**，最终会发给默认路由器，若干没有合适路由器，则丢弃，并通过回传ICMP报文报告转发分组出错。

> 注意：当路由器收到一个待转发的数据报，在从路由表得到下一跳路由器的IP地址后，不是把这个地址填入IP数据报，而是送交**数据链路层的网络接口软件**，网络接口软件负责把下一跳路由器的IP地址转换成**硬件地址（使用ARP协议）**，并将此硬件地址放在数据链路层的MAC帧的首部，然后根据这个硬件地址找到下一跳路由器。  
> **（IP不负责运输，只负责路由，数据链路上运输依赖MAC地址，运输就是不断更新mac地址的过程）**

MAC寻址参考地址转发表（记录MAC地址），而IP寻址参考路由控制表（记录网络号）

## ARP协议

用于帮助主机或路由器完成**IP地址到MAC地址的映射工作**——  
解决下一跳往哪里走的问题

> ARP协议在TCP/IP模型属于网络层，在OSI七层模型属于链路层，主要就是工作在两个层次之间。用于IP地址到MAC地址的映射工作。

**ARP协议自动进行**  
主机或路由器要和本网络上的**另一个已知IP地址的主机或路由器**进行通信时，ARP协议就会自动把这个IP地址解析为数据链路层所需要的硬件地址，这个解析过程是自动的，主机的用户对这种地址解析过程的不知道的。

每一条主机都有设有一个**ARP高速缓存**，它就是一个保存了本局域网下，各个主机或路由器的IP地址->MAC地址的映射集合。

使用过程：检查ARP高速缓存，有对应表项则写入MAC帧，没有则用**目的MAC地址为FF-FF-FF-FF-FF-FF的帧**==（广播帧）==封装ARP请求分组，**同一局域网中所有主机都能收到该请求**。目前主机收到请求后就会**向源主机单播一个ARP响应分组**，源主机收到后将此映射写入ARP缓存（10-20min更新一次）

总结：下一跳路由器的MAC地址通过ARP协议获取。Arp协议会在以太网中以广播的形式找到目标IP地址对应的目标MAC地址并将其放入ARP缓存，这个缓存只有几分钟，一旦失效就会重新获取。

> 也不一定是目标路由器的MAC地址，可能是距离目标路由器物理上最近的交换机的MAC地址，反正最终IP报文能够通过链路层最终送到目标路由器的网卡接口





# 传输层

## 对传输层的理解

首先说一下我对传输层的理解，传输层的报文传输是**端到端**的，逻辑上（仅仅关注传输层），这个“端”指的是**端口port**，运输层不考虑数据的路由，那是网络层负责的。因为端口是进程的唯一标识，如果只考虑横向的传输层，一个进程向另一个进程发送数据，其实就是一个端口向另一个端口发送数据。因此**传输层为运行在不同主机的应用进程提供了逻辑通信**。

从实现（编程）上讲，这个“端”其实是socket（或者说出是ip地址和端口的组合），说具体点应该是**socket描述符对应的数据结构**。传输层为应用层封装了数据传输的具体细节，在应用程序看来，只不过是调用了某个函数，请求就被发送出去了，这个函数就是socketAPI（套接字）。当运输层的数据到达后，交付给套接字，应用进程在从套接字中获取数据，同理，如果应用程序想要与对端的应用程序通信也要委托套接字发出去。这样看来，套接字才是实际上的“邮箱”。

> 套接字：端口和ip地址拼接着一起，你可以当成媒介、插座甚至是邮箱。（应用层和传输层之间的插槽、门关）

传输层也总是被看作应用进程之间的通信，其实可以更具体一点来说：  
应用进程所在主机的ip地址就是该进程的源ip地址，而应用进程将数据存放在与端口绑定的队列（或者说socket的发送缓存），这个端口就是源端口，其实也可以具体理解为**应用进程中的socket监听一个端口**。创建一个通信端点需要指定目标ip地址和目标端口以及协议。一旦端点被创建，它就同时具有五元组的信息  
**（协议，源ip地址，源端口，目标ip地址，目标端口）**  
因此，传输层两端的通信，从不同角度，可以视为端口之间、套接字之间、进程之间。（套接字监听端口、进程包含若干个套接字）

> socket描述符可以看作一个特殊的文件描述符，两端需要通信时，需要通过系统调用创建一个socket描述符，返回值是一个整形数字，这个数字就是一个标识，真正的数据结构维护于内核空间。如果把socket看作一个文件，那么跨主机通信就可以简单理解为写文件（send）和读文件（receive）的过程

不管是UDP还是TCP他们是只是对传输的各种细节做了不同的规范，端对端传输的本质是不会变的。用户进程会监听一个端口，一旦需要发送数据，数据便从端口发出，而当数据达到目标端口后，监听端口的应用进程就会接受发来的数据。（此处的描述仅针对传输层）

> 端口号是给传输层用的，它可以用于标明一项支持“发送数据”和“接收数据”的服务，服务器端通常使用一些熟知端口号，如web服务器提供http传输服务，使用协议http80。有了端口号，传输层才能知道哪些应用程序需要“数据发送”服务，哪些应用程序需要“接收数据”服务。

  
对面试官的回答： 传输层为应用提供端到端的数据传输服务，使得应用层不用关心数据传输的细节，只需要关心对数据内容的封装。另一方面，传输层不关心运输层报文的路由，这由下面的网络层负责，在传输层开来，两端的数据传输就好像在同一台主机进行的一样。

> 上面的问题通常只是一个引子，不用答的太多，没必要扯出具体的协议，只要点出传输层的作用即可。




## UDP和TCP是什么，区别是什么？

上面说过，**传输层本质上提供端到端之间的数据传输**，而UDP和TCP只是具体规定了：报文格式是什么、报文丢了怎么办、传输出错了怎么办等传输细节问题。需要注意到的是，**UDP和TCP只不过是对“数据传输”这个大概念进入一步规范了而已，他们本身也是协议，并不是实现**。如果面试官问你“如何进行重传策略、报文大小如何规定”等问题，往往是希望你能够说出你认为正确的多种方式或者可能存在的实现方式，而不是只说出某个方式。（就比如TCP的接收方窗口超时后重传整个窗口的分组还是重传“第N个”分组，和具体平台的实现有关，并不是一定的）

> 回答这个问题之前，最好先使用一句话去描述一下两个协议，然后再回答区别，回答时可能不能全部想起来。二者的区别可以从以下几个关键词下手：  
> 连接、可靠性、传输单位、分片、拥塞控制、流量控制、分用等。以上这些词起始是可以互相关联起来的。  
> 比如：因为UDP是不在传输层进行分片的，因此IP层总是对其进行分片，而因为IP层分片后，一旦某个分片在某一个链路中丢失，整个udp报文就作废了。而TCP在传输层大小由协议控制，通常不会在IP层分片，如果某个分组对应的IP报文在链路上丢失，仍然可以在传输层控制重传。

【1】对于TCP，数据报是通过**五元组**分用的，而对于UDP，数据报是通过**三元组**（UDP协议标识，目的端口，目的ip地址）分用的。  
同一个端口可以被多个socket端点监听，这些端点可以属于同一个用户进程，也可以分别对应一个用户进程（用进程和线程关系理解一下比较好）。  
当一个TCP报文到达目标主机，根据五元组定位一个TCPsocket端点。而一个UDP报文达到主机后，根据三元组定位一个UDPsocket端点。  
【2】分用的不同特点同时也影响了TCP与UDP不同的通信方式。TCP只能支持一对一通信，而UDP支持一对一、一对多、多对一、多对多。具体如何表现后面会说。

> 这里解释一下分用和复用，这里只考虑传输层（这里不要考虑网络层，只考虑横向的，ip地址总是能通过指定或者剥离报文获得），数据在传输层通道中传输只需要填上控制首部（**源端口，目的端口**），这时他们**统一看作传输层数据报文，这些来自不同用户进程的数据被封装为运输层报文后可以从源端口发出后，共用一条传输层通道（逻辑上的、横向的通道），或者说共用一种运输层协议进行传输，这就是复用**。逻辑上的起点就是端口port。  
> 而分用，就是报文通过公共逻辑通道传输到目的端口port后，应用程序发现监听的端口到达数据，会取出这些数据。

> 这里我再解释一下，从逻辑上理解。“逻辑上的运输层通道”隐藏了很多细节如网络层的路由交换、物理上作为比特流传输等  
> 而实现上，数据从socket发送缓存发出，被目标主机的网卡接收，数据经由内核缓存最终存入目标进程的socket接收缓存内。

【3】TCP正式传输数据前，需要三次握手建立连接，同时需要维护连接的建立和释放。而UDP则不需要建立连接，UDP客户端可以直接构建指明对端的UDP报文发送出去。

> 如果用socket编程来描述区别的话：  
> 首先TCP客户端委托socket向服务器发送请求前，需要使用connect调用与服务器建立连接，而服务器总是有至少有一个线程用于监听客户端发来的请求。服务器首先会建立一个监听套接字(源ip和源端口是通配符)，一旦收到客户端请求，便调用accept（）从连接队列（accept queue）中消费一个连接，并委托一个工作线程去与客户端通信，此时工作线程使用的套接字是一个新建的通信套接字（这里的套接字才是用于通信的，监听套接字不用于通信），虽然端口和监听套接字一样，但是源ip和源端口不一样，因此是不同的端点。  
> 可以看见，以上的过程都是TCP对连接的管理。而UDP客户端不用事先connect（），而是可以直接构造一个指定目标端口和ip地址的报文，然后通过socket发出。而UDP服务器启动后便可以接收用户的请求，而且是无状态的，它不知道和谁进行通信。通常创建一个负责接收数据的线程。

【4】TCP保证可靠传输，UDP则仅保证最大努力的交付。  
TCP和UDP的共同点就是，他们都会使用校验和的方式进行**差错校验**。ip协议也是进最大努力交付的，ip校验和仅对头部进行校验，而且每经过一个路由器，TTL减一，这个校验和都是重新计算的。这里的最大努力可以理解为保证传送达到的报文本身是不会出错的，但是不保证报文是不丢失的。  
udp校验和包含 头部+数据+伪头部 三个部分。  
同时，UDP和TCP接收方如果发现报文出现差错，会丢弃报文。不同的是UDP发送方不关心报文是否正确到达，不会进行重传，而UDP发送方甚至不关心报文是谁发来的，仅仅向源ip地址回送一个ICMP差错报文。而TCP接收方丢弃报文后，会发送一个冗余确认报文，接收方接收若干个冗余确认报文后会重传出错的报文。（快重传或者超时重传）

> 做个补充，TCP的可靠主要体现在几方面：有序（序号和确认机制）、不丢失（序号是基础，其次是重传机制）、无差错（校验和）、不重复（确认机制，不接收已确认序号的报文中的数据）

【5】如果TCP发送端向一个不存在的端口发送连接请求，将回传一个RST报文（这个回传的RST是根据发送的SYN报文构造的）。而如果是UDP发送端，则回传一个ICMP端口不可达差错报文。  
【6】头部大小。如果不考虑选项字段，TCP首部占20字节，而UDP只占8字节，TCP需要很多字段去存放控制信息  
【7】分片也是一个很重要的区别。因为UDP是基于数据报的，应用程序委托的数据，UDP仅仅加上一个控制首部就发送出去了，因此UDP的数据部分进本取决于用户。理论上IPV4最大长度为65535（包含ip头与udp头），但是如果UDP报文过大，可能大致接收方缓冲区空间不够，而被丢弃。另一方面，UDP报文很有可能被IP层分片，这有可能导致某个ip分片丢失而导致整个UDP报文无效。同时，为了防止缓存被用尽，每个路由器都会维持一个分片超时计时器，一旦数据报的分片没有即使达到，数据报（所有分片）将被丢弃。因此，尽量应该防止ip层分片。  
TCP将上级委托的数据看作一个无结构的字节流，忽略其边界，每次将多少个字节作为一个分组发出，由TCP协议动态决定。分组的数据部分不会超过MSS（最大报文段长度），通常TCP产生的报文（分组+首部）不会再IP层被分片，而且一旦丢失，都会在传输层进行重传。  
【8】最核心的就是二者的传输单元。TCP是面向字节流的，UDP是面向数据报的。UDP将上层传来的数据保留边界，不做任何拆分，加上控制首部发送出去，不做任何可靠的保证。可能发送端发送了5个包，接收方只收到了4个包。而TCP将上层传来的数据忽略边界，看作一条长长的字节流，同时保证接收端收到的也一定是一模一样的流，上层传入的数据，首先会存入发送缓存，多个字节为一个分组发送出去，一个发送窗口可以发送多个分组，可用窗口为零时就不能发送了，需要等待发送窗口中已发送但未确认的分组被确认。发出去的报文中的数据最终达到接收方的接收缓存，接收方一次呈递若干个连续的字节。说白了，你应用层托付的数据包，TCP保证正确地交给接收方，但是中间会分为几个TCP报文运输那可说不准（TCP只是将多个数据看作一个字节流，但是传输的过程肯定是以TCP报文的形式传输的）。  
（接收方并不是等整条流都达到了再向上呈递，而是将连续的、经过确认的流向上呈递）  
【9】UDP是不保证流量控制和拥塞控制的。udp接收方不对发送方进行确认，甚至不关心谁正在发送，基于UDP协议本身，无法进行控制（不是不能实现，可以在接收方向发送方回传的报文约定某些字段，算是对UDP进行扩展，虽然不保证能到达）。

> ip分片保证ip数据报不大于链路最大传输单元MTU，而每个链路的MTU不总是相同的，因此每经过一个路由器，ip数据报会经过重组和再分片。对于是否分片的问题，各层通常具有路径MTU发现功能去判断

对面试官的回答:
TCP是面向连接的，提供可靠传输服务的传输层协议，UDP是无连接的，提供尽最大努力交付服务的传输层协议。  
TCP是面向字节流的，它将应用层传递的数据看作有序、无结构的字节流，并且保证这些字节流有序、无重复、不丢失地到达对端。UDP是面向数据报的，它对上层传递的数据报保留边界，既不拆分也不合并，不保证数据报的可靠传输。  
TCP报文基于5元组分解（分用），而UDP基于三元组分解。这也影响了**TCP仅支持单播**，而UDP支持单播、多播和组播。  
TCP报文大小由TCP协议决定，通常体现在流量控制和拥塞控制上，收到确认报文窗口值和网络拥塞情况影响，不会超过MSS大小，一般也不会在IP层分片。而UDP不提供拥塞控制和流量控制，UDP报文的数据部分通常由用户程序决定，一般会在IP层分片。  
（面向连接、可靠性、通信特点、传输单元一定要说，分片、头部大小选说）




## UDP如何进行一对多、多对多

UDP数据报的目的地址可以是一个单播地址，也可以是一个广播或组播地址。如果UDP数据报的目的地址是一个广播或者组播地址，同时这个地址同时有多个端点，那么数据报会被复制多份，每一个端点都将收到一个该数据报的拷贝。（一对多）。如果这些接收方可以向多个主机开放，那么也可以看作多对多通信（多对多）  
如果发送方的目的地址是一个单播地址，而接收方对远端ip进行限制（指明具体客户机的ip或端点来过滤掉不符合的流量），只允许一台客户机可以与之通信，这就是一对一通信（一对一），如果接收方对所有用户机进行开放，也可以看作多对一通信（多对一）。

> udp收到一个ip数据报后，会执行基于目的端口的包过滤，有时也会执行关于源端口的包过滤。如果没有进程监听目的端口，数据包被丢弃并回传ICMP端口不可达报文。如果有差错直接丢弃，不做出任何其他动作

_对面试官的回答:_  
发送方可以使用单播或多播地址作为目标ip地址，同时允许接收方对一台或多台发送方主机开放，实现对应效果

> 补充:最常见的UDP攻击就是通过耗尽接收方缓存或链路容量导致服务瘫痪的DOS攻击，多播的特性也可以令UDP报文用于攻击，一个恶意的UDP发送方将报文的源IP地址修改成被攻击者的ip地址，并且将目标ip地址设置为一个 广播地址。UDP报文发出后，源ip地址（被攻击者）将被多个UDP回应报文的流量冲击。这种攻击类型属于放大攻击，通过小部分流量，导致其他系统产生更多流量。

> TCP的套接字包含监听套接字（ServerSocket）和通信套接字(socket)，其中监听套接字位于服务器，它的远程ip和端口一般是通配符表示。真正用于通信的是通信套接字，而监听套接字相当于一个宾馆前台，记录请求的临时信息，不进行真正的通信，同时系统为每个监听套接字维护两个队列（SYN/ACCEPT）。监听套接字和通信套接字的（本地ip,本地端口）是一样的（源ip和源端口不同）。一旦客户端的报文过来，如果有通信套接字与之对应，报文将通过5元组定位到具体的套接字，如果没有通信套接字与之对应，可能这是一个SYN请求建立连接报文，则会定位到监听套接字，三次握手后，连接（记录连接的对象）被放入accept队列，服务器调用accept（）将连接从accept 队列取出并为之创建通信套接字（新打开一个socket描述符）和单独的线程。之后便通过这个新建的通信套接字与之进行通信。  
> 通信套接字是有状态的，拥有完整的五元组。而监听套接字也称为欢迎套接字，无状态。而UDP套接字是无状态的，而且可以通信，相当于一个监听套接字即接收请求，又负责通信  
> 注意：以上只是一种进程TCP通信的实现方式。目的是实现TCP的面向连接的性质。如果不使用监听套接字，两个通信套接字互相connect（）理论上也是可行的。

---

## 各种的应用场景是什么

答应用场景，首先要想一下UDP和TCP分别有哪些优点。  
问完区别之后，基本紧接着就会问你二者的优点或应用场景  
对于TCP，优点就是可靠。UDP相对来说，优点就是快速、效率高、不需要额外维护连接所付出的代价，另外一点就是，UDP可以实现广播和组播，这时TCP办不到的。

_对面试官的回答:_

TCP应用于对准确性要求高、实时性要求低的场景。如文件传输（FTP、HTTP）、信息传递（DNS区域传送）等  
UDP应用于对准确性要求不高，要求实时性，对传输速度敏感，以及一些需要用到组播的场景。如：实时通话、网络语音电话、在线视频、多媒体等

当然了，也可以展开一下，拿实时流式媒体举个例子。TCP可以保证流媒体的播放音质，但是通常需要花费一定是时间才可以正常播放，而且传输实时数据时可能由于重传丢失分组而造成时延。因此对于容忍丢失、速度敏感的实时数据通常采用UDP协议传输。而对于直播、实况之类的实现，通常是一对多的通信，数据流通常是边录制边发送，对传输速度、时延也很敏感

## 总结

本文主要对UDP面试常问的点进行了总结。问UDP无非是为了引出TCP，因此二者的对比经常被问到，通常通过对比会引出优缺点。如果面试官想要继续往下引导，通常会问Telnet 、DNS 、HTTP 、HTTPS等应用层服务的熟知端口号是多少。之后，博主会接着写一写自己对各种知识点的理解，来强化学习。



## MSS和MTU
mtu是网络传输最大报文包，mss是网络传输数据最大值。

具体分析如下：

1、mss加包头数据就等于mtu. 简单说拿TCP包做例子。 报文传输1400字节的数据的话，那么mss就是1400，再加上20字节IP包头，20字节tcp包头，那么mtu就是1400+20+20. 当然传输的时候其他的协议还要加些包头在前面，总之mtu就是总的最后发出去的报文大小。mss就是你需要发出去的数据大小。

2、MSS: Maxitum Segment Size 最大分段大小 2.MSS最大传输大小的缩写，是TCP协议里面的一个概念。 3.MSS就是TCP数据包每次能够传输的最大数据分段。

3、为了达到最佳的传输效能TCP协议在建立连接的时候通常要协商双方的MSS值，这个值TCP协议在实现的时候往往用MTU值代替（需要减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes）所以往往MSS为1460。通讯双方会根据双方提供的MSS值得最小值确定为这次连接的最大MSS值。


```markdown

1.对传输层的理解
往上看,传输层为应用层提供端到端的数据传输服务，使得应用层不用关心数据传输的细节，只需要关心对数据内容的封装。往下看，传输层不关心运输层报文的路由，这由下面的网络层负责，在传输层看来，两端的数据传输就好像在同一台主机进行的一样。


2.UDP和TCP是什么，区别是什么？
TCP是面向连接的，提供可靠传输服务的传输层协议，UDP是无连接的，提供尽最大努力交付服务的传输层协议。
TCP是面向字节流的，它将应用层传递的数据看作有序、无结构的字节流，并且保证这些字节流有序、无重复、不丢失地到达对端。UDP是面向数据报的，它对上层传递的数据报保留边界，既不拆分也不合并，不保证数据报的可靠传输。
TCP报文基于5元组分解（分用），而UDP基于三元组分解。这也影响了TCP仅支持单播，而UDP支持单播、多播和组播。
TCP报文大小由TCP协议决定，通常体现在流量控制和拥塞控制上，收到确认报文窗口值和网络拥塞情况影响，不会超过MSS大小，一般也不会在IP层分片。而UDP不提供拥塞控制和流量控制，UDP报文的数据部分通常由用户程序决定，一般会在IP层分片。
（面向连接、可靠性、通信特点、传输单元一定要说，分片、头部大小选说）

3.UDP如何进行一对多、多对多
发送方可以使用单播或多播地址作为目标ip地址，同时允许接收方对一台或多台发送方主机开放，实现对应效果

4.各种的应用场景是什么
TCP应用于对准确性要求高、实时性要求低的场景。如文件传输（FTP、HTTP）、信息传递（DNS区域传送）等
UDP应用于对准确性要求不高，要求实时性，对传输速度敏感，以及一些需要用到组播的场景。如：实时通话、网络语音电话、在线视频、多媒体等

当然了，也可以展开一下，拿实时流式媒体举个例子。TCP可以保证流媒体的播放音质，但是通常需要花费一定是时间才可以正常播放，而且传输实时数据时可能由于重传丢失分组而造成时延。因此对于容忍丢失、速度敏感的实时数据通常采用UDP协议传输。而对于直播、实况之类的实现，通常是一对多的通信，数据流通常是边录制边发送，对传输速度、时延也很敏感


5.谈谈滑动窗口协议吧
为什么会有窗口的概念，因为TCP不允许发送方一次发送太多数据，这个“窗口”的概念，在TCP中通过“窗口字段”来表示。**从这个角度看，滑动窗口协议像是用来控制发送流量的。**
滑动？什么时候滑动？TCP滑动窗口协议中把窗口具体分为发送方窗口和接收方窗口，发送方窗口中又可以分为已发送但未确认和未发送的数据（字节），接收方窗口表示可以接收但是未达到的数据。一旦数据全部发送完毕，那么发送方必须等待接收方的确认报文。而一旦发送方报文到达接收方，接收方将接收报文中的数据，存入接收缓冲区并给出确认，此时接收方的接收窗口左边界右移。而当发送方接收到接收方的确认报文，他将释放相应的缓冲区，同时发送窗口左边界右移。 **从这个角度来看，滑动窗口协议有具有一定可靠传输的意义。（保存未确认分组不滑动，是为了进行重传）**

6.聊聊可靠传输
绝对的可靠，或者说理想的可靠传输是一个怎么样的传输过程？一句话：**字节流从一端发出，一模一样的到达另一端。**  
再拆解一下，从两端发出去的肯定不能是一条长字节，字节组合为分组，分组加上控制首部作为报文段
**绝对的可靠：报文有序到达、报文无差错、报文不重复、报文不丢失**
我们不是神，只能尽量让报文传输达到“可靠”。
【1】报文有序到达可能保证（ARQ），但是吞吐量太低了。我们可以退一步，不要求传输过程的有序，但是要求有序的接收——序号机制+累加确认
【2】我们无法做到传输过程中无差错，但是我们让接收方主动丢弃出错的包，然后发送方重传出错报文，就可以达到“接收无差错”——校验和+重传机制
【3】同理，对于不重复、我们无法做到传输的过程不产生重复的包，但是我们可以保证不接收重复的包——序号机制
【4】我们无法保证过程中不丢包，但是我们通过重传或者冗余ACK感知丢包事件，从而触发重传行为，另一方面，通过滑动窗口机制保证如果已发送的包未被确认就不能向右滑动，必须保存包的副本。——超时/快 重传机制 + 滑动窗口的滑动策略

_我们通过先分析绝对的可靠传输是什么样的，然后提出具体的TCP实现策略对其相对的实现，来展示我们对可靠传输的思考。如果可以具体结合例子具体解释那么就更好了。如果可以的话， 可还有举例ARQ/GBN/SR是如何实现可靠传输的。_

ARQ自动重传协议基于超时重传、一单位窗口、一应一答模式的，也称为停等协议，每个报文都有序列号，确认号只需要0和1，提供校验和。ARQ可以实现可靠传输，但是吞吐量太小了。
GBN和SR则是基于多单位窗口的，流水线式发送报文，累加确认序号，其中确认号为期待对方下一个报文段的第一个数据字节的序号。其中GBN不报错失序分组，则SR保存，同时GBN总是重传整个发送窗口的分组，而SR只选择重传某个缺失分组。
TCP的重传可以按照GBN实现，也可以按照SR实现，TCP是可以保存无序分组的。如果TCP在建立连接时声明了SACK选项（任意一方），则基于SR的重传机制。
同时TCP具有流量控制和拥塞控制的功能，它的滑动窗口不是固定的，而是可变的，TCP接收方窗口的右边界滑动方向和大小取决于接收方报文的窗口值字段。


7.聊聊流量控制
流量控制：这里的流量指的是传输层层面的流量，接收方以某种方式通告对端，要求对端发送速率慢一点。这里强调的对端，也就是说中间的网络节点是感受不到的，TCP接收方通过窗口字段通告TCP发送方的发送窗口。
流量控制要避免的——接收缓存暂时无法接受新到达的分组，导致分组被抛弃，额、因而出现大量的丢失重传。
当发送方发出的报文段到达接收方时，接收方返回一个确认报文并且给出窗口值。当发送方收到确认报文后，根据确认号移动发送窗口的左边界，根据窗口值移动发送窗口的右边界。
一旦发送窗口大小为0，那么发送方就不能发送任何数据了，必须等待对端确认报文中窗口值的通告。如果确认报文丢失，那么双方将陷入死等。因此TCP引入了**持续计时器**，这个计时器是一直启动的，因此某种程度上也是一种资源上的开销。

_谈论流量控制，首先要知道什么是流量控制，重点区别拥塞控制，而且要通过窗口值说明TCP如何进行流量控制。然后通过使用一个报文的发送与确认分析发送窗口的变化，来说明流量控制的具体表现。最后还可以说明一下窗口值为0的特殊情况如何处理。_

8.拥塞控制
拥塞就是堵，你可以认为这里的堵指的是“传输通道”这个抽象的隧道堵住了.这个抽象的“通道”下具体可以表现为路由器的接收缓存不够用了、链路过载等。说白了，网络中注入的数据报文太多了，路由器的缓存装不下、链路的带宽不够分，结果要么是数据包中途被丢弃，要么是数据报因为延迟无法及时达到对端，对端不知道中间发送了什么，它只知道这个包没有及时到达（丢包了）。
流量控制关注的是传输层的两端，要求发送方的发送速率能够匹配接收方的接收速率。而拥塞控制关注的是两端中间的“传输通道”（或者说下层网络和链路的承受能力），要求发送方调节发送速率，顾及中间通道的承受能力。（这里的传输通道不仅是两端之间的通道，是整个传输层通信共用的抽象通道）

在TCP的具体实现中（这里指对拥塞控制的进一步具体化），（发送窗口）窗口字段的值取决于拥塞窗口和接收窗口的最小值。

8.1 重传
TCP拥塞控制中，以丢包作为“网络拥塞”的标志，而丢包又可以通过“超时”和“大量失序报文到达”来表现，因此TCP拥塞控制的算法和丢包重传的时机紧密相关。
```


对于TCP,数据报是通过 **五元组(协议，源ip地址，源端口，目标ip地址，目标端口)** 分用

UDP,数据包是通过 **三元组(UDP协议标识，目的端口，目的ip地址)** 分用



# 背诵
## 简述OSI七层协议 || 简述TCP/IP五层协议
OSI七层协议包括：物理层，数据链路层，网络层，传输层，会话层，表示层， 应用层

TCP/IP五层协议包括：物理层，数据链路层，网络层，传输层，应用层

### 物理层有什么作用
**点到点通信的问题,主要解决两台物理机之间的通信，通过二进制比特流的传输来实现**，二进制数据表现为电流电压上的强弱，到达目的地再转化为二进制机器码。网卡、集线器工作在这一层。


### 数据链路层有什么作用
**在不可靠的物理介质上提供可靠的传输，接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层**。这一层在物理层提供的比特流的基础上，通过差错控制、流量控制方法，使有差错的物理线路变为无差错的数据链路。提供物理地址寻址功能。交换机工作在这一层。

### 网络层有什么作用
**将网络地址翻译成对应的物理地址，并决定如何将数据从发送方路由到接收方**，通过路由选择算法为分组通过通信子网选择最佳路径。路由器工作在这一层。

### 传输层有什么作用
传输层提供了进程间的逻辑通信，传输层向高层用户屏蔽了下面网络层的核心细节，使应用程序看起来像是在两个传输层实体之间有一条端到端的逻辑通信信道。

### 会话层有什么作用
建立会话：身份验证，权限鉴定等；保持会话：对该会话进行维护，在会话维持期间两者可以随时使用这条会话传输局；断开会话：当应用程序或应用层规定的超时时间到期后，OSI会话层才会释放这条会话。

### 表示层有什么作用
对数据格式进行编译，对收到或发出的数据根据应用层的特征进行处理，如处理为文字、图片、音频、视频、文档等，还可以对压缩文件进行解压缩、对加密文件进行解密等。

### 应用层有什么作用
提供应用层协议，如HTTP协议，FTP协议等等，方便应用程序之间进行通信。


## TCP与UDP区别

>回答这个问题之前，最好先使用一句话去描述一下两个协议，然后再回答区别，回答时可能不能全部想起来。二者的区别可以从以下几个关键词下手：
>连接、可靠性、传输单位、分片、拥塞控制、流量控制、分用等。以上这些词起始是可以互相关联起来的。
>比如：因为UDP是不在传输层进行分片的，因此IP层总是对其进行分片，而因为IP层分片后，一旦某个分片在某一个链路中丢失，整个udp报文就作废了。而TCP在传输层大小由协议控制，通常不会在IP层分片，如果某个分组对应的IP报文在链路上丢失，仍然可以在传输层控制重传。


传输层本质上提供端到端之间的数据传输，而UDP和TCP只是具体规定了：报文格式是什么、报文丢了怎么办、传输出错了怎么办等传输细节问题。需要注意到的是，UDP和TCP只不过是对“数据传输”这个大概念进入一步规范了而已，他们本身也是协议

1. TCP是面向连接的，提供可靠传输服务的传输层协议，UDP是无连接的，尽最大努力交付的传输层协议。
2. TCP是面向字节流的，它将应用层传递的数据看作有序、无结构的字节流，并且保证这些字节流有序、无重复、不丢失地到达对端。UDP是面向数据报的，它对上层传递的数据报保留边界，既不拆分也不合并，不保证数据报的可靠传输。
3. TCP报文基于5元组分解（分用），而UDP基于三元组分解。这也影响了TCP仅支持单播，而UDP支持单播、多播和组播。(**（协议，源ip地址，源端口，目标ip地址，目标端口）**,**三元组**（UDP协议标识，目的端口，目的ip地址）)
4. TCP报文大小由TCP协议决定，通常体现在流量控制和拥塞控制上，受到确认报文窗口值和网络拥塞情况影响，不会超过MSS大小，一般也不会在IP层分片。而UDP不提供拥塞控制和流量控制，UDP报文的数据部分通常由用户程序决定，一般会在IP层分片。
（面向连接、可靠性、通信特点、传输单元一定要说，分片、头部大小选说）


#### 各种的应用场景是什么
答应用场景，首先要想一下UDP和TCP分别有哪些优点。
问完区别之后，基本紧接着就会问你二者的优点或应用场景
对于TCP，优点就是可靠。UDP相对来说，优点就是快速、效率高、不需要额外维护连接所付出的代价，另外一点就是，UDP可以实现广播和组播，这时TCP办不到的。

**对面试官的回答:**

TCP应用于对准确性要求高、实时性要求低的场景。如文件传输（FTP、HTTP）、信息传递（DNS区域传送）等
UDP应用于对准确性要求不高，要求实时性，对传输速度敏感，以及一些需要用到组播的场景。如：实时通话、网络语音电话、在线视频、多媒体等

当然了，也可以展开一下，拿实时流式媒体举个例子。TCP可以保证流媒体的播放音质，但是通常需要花费一定是时间才可以正常播放，而且传输实时数据时可能由于重传丢失分组而造成时延。因此对于容忍丢失、速度敏感的实时数据通常采用UDP协议传输。而对于直播、实况之类的实现，通常是一对多的通信，数据流通常是边录制边发送，对传输速度、时延也很敏感


#### 为何TCP可靠 & 为何UDP不可靠
TCP有三次握手建立连接，四次挥手关闭连接的机制。除此之外还有滑动窗口和拥塞控制算法。最最关键的是还保留超时重传的机制。对于每份报文也存在校验，保证每份报文可靠性。




UDP面向数据报无连接的，数据报发出去，就不保留数据备份了。仅仅在IP数据报头部加入校验和复用。UDP没有服务器和客户端的概念。UDP报文过长的话是交给IP切成小段，如果某段报废报文就废了。



可靠传输？这里的可靠没有加任何限定词，那么请自问一下：绝对的可靠，或者说理想的可靠传输是一个怎么样的传输过程？一句话：字节流从一端发出，一模一样的到达另一端。
再拆解一下，从两端发出去的肯定不能是一条长字节，字节组合为分组，分组加上控制首部作为报文段。绝对的可靠：报文有序到达、报文无差错、报文不重复、报文不丢失

我们不是神，只能尽量让报文传输达到“可靠”。
1. 报文有序到达可能保证（ARQ），但是吞吐量太低了。我们可以退一步，不要求传输过程的有序，但是要求有序的接收——序号机制+累加确认
2. 我们无法做到传输过程中无差错，但是我们让接收方主动丢弃出错的包，然后发送方重传出错报文，就可以达到“接收无差错”——校验和+重传机制
3. 同理，对于不重复、我们无法做到传输的过程不产生重复的包，但是我们可以保证不接收重复的包——序号机制
4. 我们无法保证过程中不丢包，但是我们通过超时重传或者冗余ACK让发送方感知丢包事件，从而触发重传行为，另一方面，通过滑动窗口机制保证如果发送方已发送的包未被确认就不能向右滑动，必须保存包的副本。——超时/快 重传机制 + 滑动窗口的滑动策略

我们通过先分析绝对的可靠传输是什么样的，然后提出具体的TCP实现策略对其相对的实现，来展示我们对可靠传输的思考。如果可以具体结合例子具体解释那么就更好了。如果可以的话， 可还有举例ARQ/GBN/SR是如何实现可靠传输的。

ARQ自动重传协议基于超时重传、一单位窗口、一应一答模式的，也称为停等协议，每个报文都有序列号，确认号只需要0和1，提供校验和。ARQ可以实现可靠传输，但是吞吐量太小了。
GBN和SR则是基于多单位窗口的，流水线式发送报文，累加确认序号，其中确认号为期待对方下一个报文段的第一个数据字节的序号。其中GBN不报错失序分组，则SR保存，同时GBN总是重传整个发送窗口的分组，而SR只选择重传某个缺失分组。
TCP的重传可以按照GBN实现，也可以按照SR实现，TCP是可以保存无序分组的。如果TCP在建立连接时声明了SACK选项（任意一方），则基于SR的重传机制。
同时TCP具有流量控制和拥塞控制的功能，它的滑动窗口不是固定的，而是可变的，TCP接收方窗口的右边界滑动方向和大小取决于接收方报文的窗口值字段。


#### TCP粘包现象 及 处理方法
TCP是面向流协议，发送的单位是字节流，因此会将多个小尺寸数据被封装在一个tcp报文中发出去的可能性。可以简单的理解成客户端调用了两次send，服务器端一个recv就把信息都读出来了。

固定发送信息长度，或在两个信息之间加入分隔符。


#### TCP协议的滑动窗口,流量控制 和 拥塞控制
滑动窗口是传输层进行流量控制的一种措施，接收方通过通告发送方自己的窗口大小，从而控制发送方的发送速度，防止发送方发送速度过快而导致自己被淹没。


流量控制：这里的流量指的是传输层层面的流量，接收方以某种方式通告对端，调整发送速率（要求对端发送速率慢一点）。这里强调的对端，也就是说中间的网络节点是感受不到的，TCP接收方通过窗口字段通告TCP发送方的发送窗口。

==流量控制的目的==，要避免的情况——接收方缓存暂时无法接受新到达的分组，导致分组被抛弃，因而出现大量的丢失重传。
当发送方发出的报文段到达接收方时，接收方返回一个确认报文并且给出窗口值。当发送方收到确认报文后，根据确认号移动发送窗口的左边界，根据窗口值移动发送窗口的右边界。

一旦发送窗口大小为0，那么发送方就不能发送任何数据了，必须等待对端确认报文中窗口值的通告。如果确认报文丢失，那么双方将陷入死等。因此TCP引入了**持续计时器**，这个计时器是一直启动的，因此某种程度上也是一种资源上的开销。
>持续计时器基于指数退避计算超时时间，而且一旦超时就会发生**零窗口探测报文**（强制对端返回包含窗口值的ACK报文），探测报文包含一字节的数据，因此它是可以被重传的。如果收到的确认报文中窗口值仍然为0，将重置计时器，如果探测报文丢失，就将超时时间加倍（2/4/8…超过设置的阈值，就单方面断开连接，异常断开情况）

**糊涂窗口综合症**：如果接收⽅腾出⼏个字节并告诉发送⽅现在有⼏个字节的窗⼝，⽽发送⽅会义⽆反顾地发送这⼏个字节， 这就是糊涂窗⼝综合症
我们的 TCP + IP 头有 40 个字节，为了传输那⼏个字节的数据，要达上这么⼤的开销，这太不经济 了

解决办法：
接收⽅通常的策略如下: 
当「窗⼝⼤⼩」⼩于 min( MSS，缓存空间/2 ) ，也就是⼩于 MSS 与 1/2 缓存⼤⼩中的最⼩值时，就会向发送⽅通 告窗⼝为 0 ，也就阻⽌了发送⽅再发数据过来。 等到接收⽅处理了⼀些数据后，窗⼝⼤⼩ >= MSS，或者接收⽅缓存空间有⼀半可以使⽤，就可以把窗⼝打开让发 送⽅发送数据过来。


发送⽅通常的策略: 
使⽤ Nagle 算法，该算法的思路是延时处理，它满⾜以下两个条件中的⼀条才可以发送数据： 要等到窗⼝⼤⼩ >= MSS 或是 数据⼤⼩ >= MSS 收到之前发送数据的 ack 回包 只要没满⾜上⾯条件中的⼀条，发送⽅⼀直在囤积数据，直到满⾜上⾯的发送条件。 另外，Nagle 算法默认是打开的，如果对于⼀些需要⼩数据包交互的场景的程序，⽐如，telnet 或 ssh 这样的交互 性⽐较强的程序，则需要关闭 Nagle 算法。 可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应⽤ ⾃⼰的特点来关闭）


_谈论流量控制，首先要知道什么是流量控制，重点区别拥塞控制，而且要通过窗口值说明TCP如何进行流量控制。然后通过使用一个报文的发送与确认分析发送窗口的变化，来说明流量控制的具体表现。最后还可以说明一下窗口值为0的特殊情况如何处理。_





拥塞控制：拥塞就是堵，你可以认为这里的堵指的是“传输通道”这个抽象的隧道堵住了。这个抽象的“通道”下具体可以表现为路由器的接收缓存不够用了、链路过载等。说白了，网络中注入的数据报文太多了，路由器的缓存装不下、链路的带宽不够分，结果要么是数据包中途被丢弃，要么是数据报因为延迟无法及时达到对端，对端不知道中间发送了什么，它只知道这个包没有及时到达（丢包了）。
**因此流量控制关注的是传输层的两端，要求发送方的发送速率能够匹配接收方的接收速率。而拥塞控制关注的是两端中间的“传输通道”（或者说下层网络和链路的承受能力），要求发送方调节发送速率，顾及中间通道的承受能力。**（这里的传输通道不仅是两端之间的通道，是整个传输层通信共用的抽象通道）
在TCP的具体实现中（这里指对拥塞控制的进一步具体化），（发送窗口）窗口字段的值取决于拥塞窗口和接收窗口的最小值。


拥塞是指一个或者多个交换点的数据报超载，TCP又会有重传机制，导致过载。为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量.

当cwnd < ssthresh 时，使用慢开始算法。当cwnd > ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。当cwnd = ssthresh 时，即可使用慢开始算法，也可使用拥塞避免算法。

慢开始：由小到大逐渐增加拥塞窗口的大小，每接一次报文，cwnd指数增加。

拥塞避免：cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1。

快恢复之前的策略：发送方判断网络出现拥塞，就把ssthresh设置为出现拥塞时发送方窗口值的一半，继续执行慢开始，之后进行拥塞避免。

快恢复：发送方判断网络出现拥塞，就把 cwnd设置为原来的一半，，ssthresh等于cwnd，之后进行拥塞避免。

##### 重传

TCP拥塞控制中，以丢包作为“网络拥塞”的标志，而丢包又可以通过“超时”和“大量失序报文到达”来表现，因此TCP拥塞控制的算法和丢包重传的时机紧密相关。

TCP的重传方式分为**超时重传**和**快重传**。
其中超时重传时间的设置参考报文**往返时间RTT**，这个值通过时间戳选项计算出来，超时重传的时间是基于**二进制指数回退策略**（超时时间第一次R，再次超时2R，再次超时4R…）。发生超时重传的场景通常是网络拥塞十分严重（总是发生路由器丢包）或者断网情况（报文压根无法发出去）。（一旦收到确认报文就重置超时时间为R）

如果在超时重传定时器溢出之前，接收到连续的三个重复冗余ACK，发送端便知晓哪个报文段在传输过程中丢失了，于是重发该报文段，不需要等待超时重传定时器溢出再发送该报文。
>发送一个新的数据报时，设置一个计时器，如果及时接收则取消，之后发送方再发送新的数据报时，再重新设置新的计时器。这在停等协议是非常容易理解的，但是对于流水线式的非停等协议，计时器可能是基于窗口的，因此一旦重传就是以窗口为单位的就不奇怪了。TCP有各种各样的实现，目前我的理解是：TCP超时重传的范围是窗口，但是有SACK选项使得其可以仅重传部分分组。

而对于网络拥塞不是那么严重时，通常基于快重传策略。TCP收到一个报文后，总是对接收缓存中最后一个有序字节的序号给出确认，而且发送方一次可以发出多个报文段，当发送方收到了若干个（默认3个）重复确认的ACK报文时，将快速重传丢失的报文。（若干个冗余报文总是在超时之前到达发送方）
TCP虽然是基于累积确认的，因此TCP允许接收方**延迟确认**。但是**前提是按序接收分组**。一旦接收方收到了一个失序分组，它必须立即给出重复确认。如果是SACK，重复确认报文还包含了“空缺的是那个报文”的信息。
使用SACK可以更快的填补空缺，而且可以减少不必要的重传，因为一个RTT不再是仅仅快速重传冗余ACK希望重传的报文，而是重传具体的几个缺口报文。而且SACK还可以避免伪重传（刚收到3个重复ACK，那边就到达了）

总结：超时重传是基于时间驱动的，而快重传是基于事件驱动的。
>当超时重传时到底重传所有发送窗口的分组，还是重传某个分组和具体的TCP实现有关，如果TCP创建连接的时候声明了SACK选项，那么TCP便是基于选择重传的，其中SACK在正式传输时，包含了接收方已经成功接收的数据块的序列号范围，发送方可以更加精确的重传“缺口”分组，同时快重传也会参考SACK，这个选项一般是默认开启的

>纯ACK（不包含任何数据的ACK报文）不会被重传，因此如果三次握手丢失了第三个报文，而且不含任何数据，则超时重传的总是第二个SYN+ACK报文，其中SYN占一个序列号。
>重传计时器对任何占用序列号的报文都会计时，超时时间RTO，通常是RTT的两倍。重传计时器是方法方维护的（两端都有发送方的角色），因此如果一个报文丢失，两边都有可能重传，只不过谁先重传不得而知。




_谈论拥塞控制，首先要知道拥塞控制的关注点，以及与流量控制的区别，然后谈TCP中的具体体现。另外就是简单谈谈拥塞控制的四种算法。_


#### TCP三次握手过程 和 四次挥手
1.  第一次握手:客户端将标志位SYN置为1，随机产生一个序列号seq=x，并将该数据包发送给服务端，客户端 进入syn_sent状态，等待服务端确认。
    
2.  第二次握手:服务端收到数据包后由标志位SYN=1知道客户端请求建立连接，服务端将标志位SYN和 ACK都置为1，ack=x+1,随机产生一个值seq=y，并将该数据包发送给客户端以确认连接请求，服务端进入syn_recv状态。
    
3.  第三次握手:客户端收到确认后检查,如果正确则将标志位ACK为1，ack=y+1，并将该数据包发送给服务端，服务端进行检查如果正确则连接建立成功，客户端和服务端进入established状态，完成三次握手，随后客户端和服务端之间可以开始传输 数据了


Q：请你说一下描述一下三次握手吧
A：好的。传输层是端对端的通信，基于TCP协议的两端在正式进行数据传输之前，需要进行一个建立状态的动作。假设现在有两台主机分别运行着客户端进程和服务器进程，其中服务器进程处于监听状态。当客户端希望与服务端通信时，客户端进程主动向服务器进程发送同步报文，其中同步位置1，同时选取一个初始序号。发送完毕客户端处于SYN-SENT状态。当报文到达对端后，服务器向客户端发送同步确认报文，其中同步位和确认位置1，也选取一个初始序号，同时确认号设为下一个期望接收的字节的序号。发送完毕服务器处于SYN-RECV状态。当报文达到客户端时，客户端进入established状态并且返回一个确认报文，当服务器收到确认报文后也进入established状态，此时三次握手完毕。


**丢包问题分析**
前两次报文携带序号，因此本身是可以被重传的。而第三次报文不携带数据的情况下就是一个纯ACK不占用序号。因此不会被重传。而且三次握手不涉及数据的传输，不存在快重传。
如果第一次丢包，超时后客户端会重传。而第二次丢包后，两方都会重传，一般是客户端由于得不到确认而先进行重传，如果仍然丢失，则服务器也会由于得不到确认而超时重传。
>第一次丢包可以看作断网了，发不出去自然得不到确认。而第二次丢包可以看作客户端加了防火墙，报文到达不了客户端。

第三次报文发出后，客户端就进入established状态了。丢包可以分为两种情况：如果是纯ACK，那么客户端不会重传，而等待第二个报文重传。如果携带数据那么客户端会重传（但是一般还是服务器更早超时）。如果客户端先发出纯ACK紧接着发送数据（不是捎带），那么即使第一个ACK丢失，第二个数据报文到达，服务器依然可以转为established状态。（数据报文ACK=1的意义的体现之一：**累积确认，只要第二个报文的序号在数据报文确认号范围内**，就相当于被确认了）如果第三个报文丢失，客户端什么也不做，而服务器的重传报文一直丢失，那么当服务器重传达到一定次数可能会主动断开连接（异常断开，不执行挥手动作），此时客户端处于连接的半开状态，它不知道另一方已经断开连接了，它一直等待。（如果一直不发报文，就一直等待，直到客户端一方的保活计时器超时，才主动发送探测报文）。由于对方已经关闭连接，会返回一个RST，之后连接被拆除。

##### 为什么TCP握手需要三次，两次行不行？
不行。TCP进行可靠传输的关键就在于维护一个序列号，三次握手的过程即是通信双方相互告知序列号起始值， 并确认对方已经收到了序列号起始值。

如果只是两次握手， 至多只有客户端的起始序列号能被确认， 服务器端的序列号则得不到确认。

##### TCP三次握手异常情况分析
RTO：重传超时时间

###### 第一次握手的SYN丢包了，会发生什么
有个内核参数规定的第一次握手SYN超时重传次数 `tcp_syn_retries`
每次超时时间RTO是指数上涨的，当超过最大重传次数后，客户端不再发送SYN包

###### 第二次
服务端收到客户端的SYN包，那就会回ACK,SYN包，由于客户端一直没回ACK包，服务端在超时后，重传ACK,SYN包，接着客户端超时重传的SYN包又抵达服务端，服务端收到后，超时定时器就重新计时，然后返回ACK,SYN包，所以相当于服务端的超时定时器只被触发一次，然后又重置了
最后客户端SYN重传次数达到5次（默认值），就不再发送SYN包了

服务端重传ACK，SYN次数由 `tco_synack_retries`内核参数限制，默认是5

###### 第三次
服务端重传ACK,SYN次数超过限制，主动断开

客户端向服务端发送数据包时，由于服务器的TCP连接已经退出，所以数据包一直在超时重传，并传15次，由`tcp_retries2`指定，默认15次

_如果客户端不发送数据，也就无法超时重传，那什么时候才会断开 ESTABLESHED状态_
这里就涉及到TCP的保活机制

定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP保活机制会开始作用，每隔一个时间间隔就发送一个探测报文，如果连续几个探测报文都没有得到响应，就认为当前的TCP连接已经死亡，系统内核将错误信息通知给上层应用程序


在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值： 

`net.ipv4.tcp_keepalive_time=7200 `
`net.ipv4.tcp_keepalive_intvl=75 `
`net.ipv4.tcp_keepalive_probes=9`

tcp_keepalive_time=7200：表示保活时间是 7200 秒（2⼩时），也就 2 ⼩时内如果没有任何连接相关的活 动，则会启动保活机制 tcp_keepalive_intvl=75：表示每次检测间隔 75 秒； tcp_keepalive_probes=9：表示检测 9 次⽆响应，认为对⽅是不可达的，从⽽中断本次的连接。 
也就是说在 Linux 系统中，最少需要经过 2 ⼩时 11 分 15 秒才可以发现⼀个「死亡」连接。


这个时间是有点⻓的，我们也可以根据实际的需求，对以上的保活相关的参数进⾏设置。 
如果开启了 TCP 保活，需要考虑以下⼏种情况： 
第⼀种，对端程序是正常⼯作的。当 TCP 保活的探测报⽂发送给对端, 对端会正常响应，这样 TCP 保活时间会被 重置，等待下⼀个 TCP 保活时间的到来。 
第⼆种，对端程序崩溃并重启。当 TCP 保活的探测报⽂发送给对端后，对端是可以响应的，但由于没有该连接的 有效信息，会产⽣⼀个 RST 报⽂，这样很快就会发现 TCP 连接已经被重置。 
第三种，是对端程序崩溃，或对端由于其他原因导致报⽂不可达。当 TCP 保活的探测报⽂发送给对端后，⽯沉⼤ 海，没有响应，连续⼏次，达到保活探测次数后，TCP 会报告该 TCP 连接已经死亡。


##### 简述半连接队列
TCP握手中，当服务器处于SYN_RCVD 状态，服务器会把此种状态下请求连接放在一个队列里，该队列称为半连接队列。

##### SYN洪泛攻击的解决方案
SYN攻击即利用TCP协议缺陷，通过发送大量的半连接请求，占用半连接队列，耗费CPU和内存资源。

优化方式：

1.  缩短SYN Timeout时间
    
2.  记录IP，若连续受到某个IP的重复SYN报文，从这个IP地址来的包会被一概丢弃。

3.SYN Cookie
```markdown
根据这个SYN包计算出一个cookie值。这个cookie作为将要返回的SYN ACK包的初始序列号。当客户端返回一个ACK包时，根据包头信息计算cookie，与返回的确认序列号(初始序列号 + 1)进行对比，如果相同，则是一个正常连接，然后，分配资源，建立连接。
```


SYN洪泛攻击使得服务器瘫痪的主要原因，是由于服务器收到第一个握手报文后总是要将连接对象存入半连接队列，一旦半连接队列满了就会拒绝后面的同步报文。TCP提出了SYNcookie策略。服务器如果开启了SYNcookie功能，则在第一次收到同步报文后不会保存连接信息，而是根据该同步报文的信息生成一个数字，作为同步确认报文的初始序号ISN。如果服务器可以成功收到客户端的确认报文，且确认号是正确的，那么会直接将其放入全连接队列。这种情况下，非法的第一次握手报文将不能对服务器造成实质性的伤害。

##### 三次握手报文可以携带数据吗
HTTP报文传输之前需要在传输层进行三次握手建立连接，也就是说真正的数据传输发生在三次握手之后。不过客户端在接收到第二个握手报文之后就已经进入established状态了，所以第三次握手报文是可以携带数据。（如果理解为捎带传输也没问题）
>如果TCP规定第一次握手可以携带数据，那么服务器收到攻击的风险很大。当服务器收到第一次握手报文，它需要将连接信息对象存储在半连接队列，这是可能被“SYN洪泛攻击”的，而如果一次报文如果存在数据，那么服务器有必须开辟内存接收它。这使得服务器更容易收到攻击。而第二次包含数据也没有必要，此时服务器只是知道“好像有人和我通信”，但是这可能是一个伪造的IP或者过期的同步报文，它必须收到第三次报文后（进入established）才能确认客户端的身份。

如果问“为啥不能二次握手”，原因也是类似的——没有收到对方的确认，无法明确对方的身份就草草地进入established状态，很可能造成很大的代价：【1】如果这是一个过期连接，那么对方返回RST，服务器就不得不拆除无效的连接【2】如果对方是攻击者，那么服务器和一个“虚假的端点”建立了连接，服务器如果维护大量无效的连接，那么将极大耗费系统资源（系统能够打开的套接字有限、每个端点都需要维护变量和缓冲区、全连接队列被迅速占满导致无法正常服务合法用户，这一点主要还是因为“套接字有限”）


三次握手不能省略，客户端和服务器都需要保存对方的各种信息“状态”，而且通信的初始序号、选择重传选项和最大报文段等都是在三次握手中协商出来的，同时也包含了两端分别作为发送方第一次的窗口值、超时时间等  
**三次握手本质上就是客户端和服务端分别互换接收方和发送方角色进行的一次通信前的协商。**

>在正式发送数据之前，TCP的三次握手至少存在一个RTT的时延（前两个报文的一来一回）

##### TCP四次挥手过程
1.  第一次挥手：客户端发送一个FIN，用来关闭客户端到服务端的数据传送，客户端进入fin_wait_1状态。
    
2.  第二次挥手：服务端收到FIN后，发送一个ACK给客户端，确认序号为收到序号+1，服务端进入Close_wait状态。此时TCP连接处于半关闭状态，即客户端已经没有要发送的数据了，但服务端若发送数据，则客户端仍要接收。
    
3.  第三次挥手：服务端发送一个FIN，用来关闭服务端到客户端的数据传送，服务端进入Last_ack状态。
    
4.  第四次挥手：客户端收到FIN后，客户端进入Time_wait状态，接着发送一个ACK给服务端，确认后，服务端进入Closed状态，完成四次挥手。


![[Pasted image 20220301214855.png]]
>上图中第二次和第三次挥手报文几乎一模一样，其实可以合并为一次（三次挥手）

Q：描述一下四次挥手吧
A：好的。建立TCP连接的客户端和服务器，任意一方都可以主动关闭连接。假设客户端主动断开连接，客户端向服务器发送终止报文，并处于FIN-WAIT1状态，当服务器收到终止报文后返回一个确认报文，并处于CLOSE-WAIT状态。当客户端收到确认报文，进入FIN-WAIT2状态。服务器可能会继续传输一段数据，传输完毕后发送终止报文，并且等待客户端的最后一个确认报文，随后进入LAST-ACK状态。客户端收到客户端的终止报文后，发送确认报文，随后进入TIME-WAIT状态，持续2MSL后进入CLOSE状态。当服务器收到确认报文后进入CLOSE状态，系统将回收相应资源（套接字、变量和缓冲区占用的内存等）
>这里可以展开，发送FIN报文前通常使用系统调用close（），该调用同时关闭套接字的读写缓冲区，之后即使对端传来数据，本地也只确认不保存。而被动关闭方收到FIN报文后，就会在读缓冲区尾部插入一个EOF，一旦读取到EOF就知道对方不进行发送数据了。此时服务器进入closeWait——等待close调用，当服务器将写缓冲区的数据发送完毕后，就调用close（）。
>系统调用shutdown（）比close（）更加灵活，可以选择全部关闭、关读连接、关写连接的功能。
>FIN-WAIT1状态中主动关闭方可以重传占用序号的终止报文，而FIN-WAIT2中主动关闭方只能返回不占用序号的确认报文，此时主动关闭方的发送通道已经关闭（接收通道是否关闭看具体调用，如果close（）那么FIN-WAIT2很短暂），等待“对端发送通道关闭”的通知


##### 为什么不是三次挥手
可以是三次挥手，前提是被动关闭方收到主动方的终止报文后，没有要发送的数据，就可以将确认报文和终止报文复用一个报文段发出。（TCP是全双工通信），如果被动关闭方的发送缓冲区仍然有数据未发送，那么被动方则需要先对主动关闭方的终止报文进行确认，然后发送完毕剩余数据部分，再发送自己的终止报文。

##### timeWait的意义，为什么是2MSL，大量timeWait的危害和原因，大量closeWait的原因和危害。
MSL即报文最大生存时间。设置2MSL可以保证上一次连接的报文已经在网络中消失，不会出现与新TCP连接报文冲突的情况。

1. 当主动关闭方收到被动关闭方的终止报文就会启动一个2MSL的计时器，其中MSL是报文在网络中的最大存活时间。2MSL确保最后一个确认报文能够达到被动关闭方，且一旦发生重传，被动关闭方的终止报文能够达到主动关闭方。2MSL分别对应确认报文和终止报文的存活时间。
2. timeWait的存在保证连接被复用后，历史报文全部过期。而且保证主动关闭方一定可以收到被动关闭方重传终止的报文。

>MSL是报文段的最大存活时间，由时间等待计时器控制。而TTL是ip报文的存活时间记录在ip首部，单位是跳数，每经过一个路由器都会重新计算TTL，一旦过期就会被路由器丢弃并回传超时ICMP报文，MSL通常大于等于TTL变为0的时间。

客户端和服务器都可以是主动关闭连接的一方，如果客户端积攒大量timeWait问题不大，因为如果客户端想再次建立连接，便重新选取一个端口即可（无法立刻使用的socket占用一部分内存，但是端口够用）。而服务器积攒大量timeWait会导致系统保持大量打开的套接字描述符，而系统中可以打开的套接字描述符是有限的，全连接队列会溢出，而accept（）阻塞无法返回，无法正常服务。（执行完close()，线程可以回收，但是操作系统暂时无法回收socket_fd）。
如果服务器主动断开连接，而客户端又向服务器重新发起连接，原端口是无法立刻复用的（五元组不可用）。

**总结：客户端出现大量timeWait的极端情况就是导致端口耗尽，客户端无法与服务器建立连接（因为服务器两个ip/port和客户端的ip一定，能否建立连接看客户端的port），而对于服务器来说主要的影响就是无法及时释放套接字占用的内存，而且能够打开的套接字有限，如果无法及时复用套接字资源则影响服务质量。**


什么是 close_wait：关闭 TCP 连接过程中，第 1 次挥手服务器接收客户端的 FIN 报文段，第 2 次挥手时，服务器发送了 ACK 报文段之后，服务器会进入 close_wait 状态。通常，CLOSE_WAIT 状态在服务器停留时间很短，如果你发现大量的 CLOSE_WAIT 状态，那么就意味着被动关闭的一方没有及时发出 FIN 包，一般有如下几种可能：

-   程序问题：如果代码层面忘记了 close 相应的 socket 连接，那么自然不会发出 FIN 包，从而导致 CLOSE_WAIT 累积；或者代码不严谨，出现死循环之类的问题，导致即便后面写了 close 也永远执行不到。
-   响应太慢或者超时设置过小：如果连接双方不和谐，一方不耐烦直接 timeout，另一方却还在忙于耗时逻辑(我方忙与读或者写，没有关闭连接)，就会导致 close 被延后。响应太慢是首要问题，不过换个角度看，也可能是 timeout 设置过小。

程序问题：说的具体一点，**服务器端的代码，没有写 close 函数关闭 socket 连接，也就不会发出 FIN 报文段；或者出现死循环，服务器端的代码永远执行不到 close。**



而closeWait的大量出现基本就是程序的编程问题，被动关闭方收到对端终止报文后，无法正常调用close（）（如死锁、阻塞或压根没写close（）），导致端点一直处于closeWait状态。
比如Telnet和服务器建立连接，然后异常关闭Telnet，这时如果close（）不是写在finally块中，则无法退出，而一直处于closeWait状态。
closeWait大量出现和timeWait大量出现的危害差不多，无法释放socket，占用内存资源等。
解决：close（）写在finally块中、使用一个后台线程监听异常事件，一旦出现大量closeWait就对目标端点主动close（）。
>如果客户端程序崩溃，OS会发送FIN，当客户端收到服务器消息时，由于socket已经关闭会触发RST响应。而如果客户端主机崩溃，则不会发送任何信息，对端想要感知到需要传输层保活机制或者应用层传输心跳报文


###### 如何优化timewait

1. 如下的 Linux 内核参数开启后，则可以复⽤处于 TIME_WAIT 的 socket 为新的连接所⽤。
   `net.ipv4.tcp_tw_reuse = 1`
   有⼀点需要注意的是，tcp_tw_reuse 功能只能⽤客户端（连接发起⽅），因为开启了该功能，在调⽤ connect() 函数时，内核会随机找⼀个 time_wait 状态超过 1 秒的连接给新的连接复⽤。 
   使⽤这个选项，还有⼀个**前提，需要打开对 TCP 时间戳的⽀持，即 这个时间戳的字段是在 TCP 头部的「选项」⾥，⽤于记录 TCP 发送⽅的当前时间戳和从对端接收到的最新时间 戳。** 由于引⼊了时间戳，我们在前⾯提到的 **2MSL 问题就不复存在了**，因为重复的数据包会因为时间戳过期被⾃然丢 弃。`net.ipv4.tcp_timestamps=1（默认即为 1）`

2. 设置 socket 选项，来设置调⽤ close 关闭连接⾏为。
   如果 l_onoff 为⾮ 0， 且 l_linger 值为 0，那么调⽤ close 后，会⽴该发送⼀个 RST 标志给对端，该 TCP 连接 将跳过四次挥⼿，也就跳过了 TIME_WAIT 状态，直接关闭。 但这为跨越 TIME_WAIT 状态提供了⼀个可能，不过是⼀个⾮常危险的⾏为，不值得提倡


>[一次即时通讯项目TIME_WAIT过多引发的记录 - 简书 (jianshu.com)](https://www.jianshu.com/p/a2938fc35573)

##### 为什么TCP挥手需要4次
主要原因是当服务端收到客户端的 FIN 数据包后，服务端可能还有数据没发完，不会立即close。

所以服务端会先将 ACK 发过去告诉客户端我收到你的断开请求了，但请再给我一点时间，这段时间用来发送剩下的数据报文，发完之后再将 FIN 包发给客户端表示现在可以断了。之后客户端需要收到 FIN 包后发送 ACK 确认断开信息给服务端。



##### TCP优化


#### 记录
能描述一下三次握手的过程吗

A：嗯，好的，那我就那mysql举一个例子吧。我现在有一个mysql客户端，远端有一个mysql服务器，现在我去连接远端的服务器，这时我客户端底层的套接字就调用了connect去和服务器底层监听套接字建立连接，这时就涉及一个三次握手。
我客户端先发出一个同步报文，告诉服务器我想和你通信，咱们约定个初始序号吧，然后选择了一个初始序号，这个初始序号是随机的而且是不重复的，这里我就以0为例了。客户端发完这个报文后就进入同步报文已发送状态，等待服务器响应。服务器收到这个同步报文后，知道了“客户端与我通信，而且从0开始发送报文”，于是服务器生成一个确认报文，这里服务器也会和客户端进行一个同步，但是考虑到优化，这里的同步报文和确认报文是合在一起的，因此服务器发出的这个报文同时具有确认和同步的双重意义，因此它也选择了一个初始序列号，这里我也以0为例，因为第一个报文占用一个序号/字节，因此第二个报文的确认号是1，意思是，告诉客户端，我收到你发出的第一个报文了，你可以接着发送下一个报文。服务器发送完后进入同步已接收，等待客户端的确认报文。客户端收到这第二个报文后，它就知道了，客户端到服务器方向已经同步了，这个同步就是“你走的每一步我都知道”，而且服务器的初始序号为0，然后客户端发送一个确认报文，确认报文也是1，告诉服务器我已经收到你第二个报文了，然后进入连接建立状态，服务器收到这个报文后也进入连接建立状态。连接建立完毕后mysql客户端的connect调用就会返回，mysql服务器的accept()也会返回一个套接字的描述符，mysql服务器会创建一个子线程使用这个套接字与mysql客户端通信，这个返回的套接字描述符就可以看作mysql服务器对客户端状态的保存和描述。


#### QQ场景
https://www.cnblogs.com/zhuleixiao/p/9205065.html


## DNS协议
DNS协议是基于UDP的应用层协议，它的功能是根据用户输入的域名，解析出该域名对应的IP地址，从而给客户端进行访问。

DNS（domain name system 域名系统），首先，它是一项应用层服务，可以将互联网中的主机名解析为IP地址，通常DNS是由多台DNS服务器搭建起来的分布式数据库，因此一个完整的域名解析的过程就是一个查询分布式数据库的过程。

DNS协议由两部分组成：
【1】用于根据主机名查询对应IP地址的请求和响应协议（域名解析）
【2】域名服务器之间用于交换资源记录的协议（区域传输）
DNS协议运行在UDP上，熟知端口号为53。

### DNS解析过程

Mail.cctv.com. ————> 三级域名.二级域名.顶级域名.根域名。根域名就是一个点“.”

域名服务器分类  
【1】根域名服务器（告诉本地域名服务器/LDNS下一步找哪一个顶级域名服务器）  
【2】顶级域名服务器（管理在该域名服务器注册的所有二级域名）  
【3】权威域名服务器（负责一个区的域名服务器）  
【4】本地域名服务器（属性->ipv4…指定的“首选DNS”就是LDNS）

主机向本地域名服务器发送请求后，进行递归查询——我替你问  
而域名服务器向其他域名服务器查询则采用迭代查询——你问别人

为了提高DNS查询效率，并减轻根域名服务器的负荷以及网络中DNS查询报文 的数量，在域名服务器中广泛使用了**高速缓存域名服务器**。

【1】浏览器正式向DNS服务器发送域名解析请求之前，首先查询**浏览器的域名系统缓存**，没有则去**本地域名解析文件hosts**查看，没有才会进行查询（dns）

> Http1.1增加了host，用于实现虚拟主机，可以用来做负载均衡（同一台主机服务器部署多个网站，TCP连接到同一端点（目标主机的服务器端口），但是会根据host分发到具体的网站）

【2】客户端先查询**本地域名服务器（LDNS）**，每个LDNS服务器都有缓存  
【3】本地域名服务器没查到，则去查询**根域服务器**。根域服务器会返回**顶级域名服务器**的ip地址，顶级域名服务器返回目标区域的权威域名服务器的ip地址。最终**权威域名服务器**会给出最终结果。

> 本地服务器是递归查询，根域、顶级域名服务器是迭代查询，因为根域、顶级域名服务器处于高层次域，可以被所有子服务器发送解析请求，而本地服务器通常只对应一个ISP范围的用户。所以低层次服务器能够承受递归查询的代价，而高层次的服务器无法承受


#### 细节
共同实现DNS分布式数据库的所有DNS服务器存储了**资源记录**，每个DNS回答报文都会保证至少一条资源记录。  
一个资源记录就是一个四元组（name,value,type,ttl），记录可以简单的看作一个附带过期时间的键值对（name->value），其中键值对的意义取决于type。  
【1】A记录，代表主机名到IP地址的映射（a.b.c.com -> 145.33.91.123）  
【2】NS记录，代表顶层域到该域的**权威DNS服务器主机名**的映射（a.com -> dns.a.com），用于沿着查询链来路由能够解析域名的DNS服务器  
【3】CNAME记录，将**别名**映射为一个主机名对应的**规范主机名**（a.com->aa.a.com）  
【4】MX记录，返回邮件服务器的规范主机名（a.com -> mail.a.com）  
【5】PTR反向解析，根据ip地址查询域名

DNS进行**域名解析**服务时，使用UDP协议，主要是为了效率（缩短整体的响应时间），而且报文的内容一般都不长，如果太长会被直接截断（因为DNS没有能够标识报文ID的字段，默认请求和应答都是使用一个报文）  
一些TCP实现上，当收到一个截断的UDP响应包，一般会使用TCP重新请求发起DNS请求

> 如果需要防范DNS欺骗，需要对DNS报文进行数字签名，但是这造成DNS报文负载变大，不得不使用TCP进行传输（DNS over HTTPS）

由于DNS是一个分布式的系统，而且防止故障通常需要多台服务器搭建集群，一旦涉及数据同步就需要进行TCP传输数据（这个过程称为**区域传送**），因为数据同步要求数据的准确性和传输可靠性，而且数据量较多。

> 当一个区域的辅助域名服务器启动时，需要从主域名服务器传送区域信息，区域传输可能由计时器引起，也可能因DNS通知引起。完整区域传输通常使用UDP因为可能超过单个UDP报文上限。增量区域传输通常优先采用UDP，如果响应消息太大就会切换为TCP。  
> 当使用UDP时，解析器和服务器应用程序必须在应用层保证可靠性。

DNS也是实现**负载均衡**的一种方案。  
为了舒缓主服务的压力，部署一些从服务器，并且这些服务器具有不同的IP地址，这些IP地址与同一个规范主机名相关联，DNS数据库存储这些IP地址集合，当一个DNS请求过来，DNS服务器（或者使用一个缓存的DNS记录的代理服务器）根据某种算法，选取某个IP地址进行回应，以达到反向代理和负载均衡的效果。

> 一个URL 协议://域名:端口号/请求资源地址?参数  
> 如果**协议、端口、域名**任何一个不同则认为**跨域**


## 理解CDN

```markdown
CDN的全称是`Content Delivery Network`，即`内容分发网络`，它应用了 HTTP 协议里的缓存和代理技术，代替源站响应客户端的请求。CDN 是构建在现有网络基础之上的网络，它依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户`就近`获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有`内容存储`和`分发技术`。

打比方说你要去亚马逊上买书，之前你只能通过购物网站购买后从美国发货过海关等重重关卡送到你的家里，现在在中国建立一个亚马逊分基地，你就不用通过美国进行邮寄，从中国就能把书尽快给你送到。
```


CDN内容分发网络，本质上是一种缓存，CDN管理着多个不同地理位置主机的资源，大多数CDN基于截获和重定向本地域名服务器（LDNS）的DNS请求来实现。用户想要访问一个静态资源，它不必向源服务器请求资源，而是向一个地理上更近的CDN服务器去请求。

**使用场景：**  
面向内部的网站，资源文件和业务代码比较耦合，script引用往往指向站内文件（相对地址）。这种方式的优点是发布简单、对服务器要求小。但是系统访问量增加后，为了减少服务器的压力，需要将服务器分离为资源服务器和应用服务器，这个时候各种静态资源的应用就需要改成绝对路径，指向对应的资源服务器。  
提升用户发出请求后资源的响应速度、分担服务器压力、节省空间和流量。

`<script src="http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>`



当服务器请求js的时候直接去最近的百度CDN服务器找，而不是当前自己的服务器

### CDN流程

CDN的加速策略和通常域名绑定，通过域名访问资源，首先是通过**DNS调度系统**查找离用户地理上最近的CDN节点（边缘服务器）的ip地址。通过IP地址访问资源，如果CDN服务器上没有缓存资源，则会到源站请求资源，并缓存到CDN节点上。

大致流程：  
用户向资源发出请求，首先会向本地DNS服务器（LDNS）发出DNS请求，本地域名服务在缓存中没有查询出结果后将递归查询，最终得到一个**CNAME记录**，这个CNAME最终会被解析到一台CDN专用的DNS服务器。  
用户向CDN的**全局负载均衡设备**发出请求（递归），CDN全局负载均衡设备根据用户的IP地址、请求的URL选择一台用户所属区域的**区域负载均衡设备**，告诉用户向该设备发送请求，区域负载均衡设备向用户返回一台缓存服务器的IP地址。最终的结果就是，全局负载均衡设备向用户返回了一台CDN缓存服务器的IP地址。  
如果该IP地址对应的服务器节点缓存了该资源，则会将数据直接返回给用户，请求结束。否则，该服务器节点将向源站发出对该资源的请求（当前缓存服务器依次向上一级服务器请求，直到请求达到源服务器）。获取资源后，结合用户（CDN服务购买者）自定义配置的缓存策略将资源缓存在该服务器节点。

总结：  
DNS服务器可以根据用户IP地址（如分析地理位置）将域名解析到合适的缓存服务器IP地址，实现用户就近访问，而使用CDN服务，其实就是**将域名解析的请求委托为CDN专用的DNS服务器**，实现内容加速查询。（CDN实现多种多样，以上只是以DNS为例）


工作流程：当用户访问已经加入CDN服务的网站时，首先通过DNS重定向技术确定最接近用户的最佳CDN 节点，同时将用户的请求指向该节点。当用户的请求到达指定节点时，CDN的服务器（节点上的高速缓存）负责将用户请求的内容提供给用户。具体流程为：用户 在自己的浏览器中输入要访问的网站的域名，浏览器向本地DNS请求对该域名的解析，本地DNS将请求发到网站的主DNS，主DNS根据一系列的策略确定当 时最适当的CDN节点，并将解析的结果（IP地址）发给用户，用户向给定的CDN节点请求相应网站的内容。  
  
CDN主要是针对静态内容的，对于动态的请求(提交表单等)还是需要访问源站，但是获得了HTTP response之后下载的那些resource文件可以通过CDN的节点快速传递，同时CDN节点和源站之间通过高速网络连接，速度很快。 即使是动态请求，也可以从这两方面加快速度  
  
CDN的核心技术就是其中的路由算法，怎么判断客户离哪个节点最近，有点类似于城市最短路径的算法， 同时还要根据网络流量负载等情况，保证网络处于最佳的运行状态。



```1.浏览器要将域名解析为 IP 地址，所以先向本地 DNS 发出请求。

2.本地 DNS 依次向根服务器、顶级域名服务器、二级域名服务器、三级域名服务器发出请求，得到全局负载均衡系统（GSLB）的 IP 地址。

3.本地 DNS 再向 GSLB 发出请求，GSLB 可以根据本地 DNS 的 IP 地址判断用户的位置，筛选出距离用户较近的本地负载均衡系统（SLB），并将该 SLB 的 IP 地址作为结果返回给本地 DNS。

4.本地 DNS 将 SLB 的 IP 地址发回给浏览器，浏览器向 SLB 发出请求。

5.SLB 根据浏览器请求的资源和地址，选出最优的缓存服务器发回给浏览器。

6.浏览器再重定向到缓存服务器。

7.如果缓存服务器有浏览器需要的资源，就将资源发回给浏览器。

8.如果没有，就向源服务器请求资源，再发给浏览器并缓存在本地。

```

**什么情况下会用到CNAME记录？**  
［如果需要将域名指向另一个域名，再由另一个域名提供ip地址，就需要添加CNAME记录］最常用到CNAME的情况包括：做CDN，做企业邮局

CDN服务之所以要用CNAME，最主要的原因是
1. 要根据用户所在位置选择并返回最优节点 IP。如果不用CNAME，A记录只能实现域名解析到IP，因此就无法实现CDN的加速效果。

2. cname主要存在的意义是在于 有的域名不属于你自己

你在使用cdn服务的时候，服务商提供给你的就是一个cname地址，如果服务商给你一个ip，假如哪天服务商想把ip地址换一个，很多人域名上对应的ip地址就要跟着变化，要让所有人都一起改完，完全没有办法做到的事情，换成cname就没事了，你用你的cdn，他改他的ip地址。唯一的坏处就是，第一次DNS解析域名的时候会多解析一次。总结来看，好处远大于坏处

## HTTP协议
http协议是超文本传输协议。它是基于TCP协议的应用层传输协议，即客户端和服务端进行数据传输的一种规则。该协议本身HTTP 是一种无状态的协议。


**应用层研究的是两个应用进程如何交换报文，本质上是进程通信，开发者是可以直接控制的。而应用层协议本质上是为了制定一个规则——如果进程B提供某些服务（如HTTP），那么进程A与进程B之间应该遵循什么样的报文交换规则和报文语法。**  
应用层不需要关心报文如何传输到对面，因为应用层可以直接使用传输层提供的报文传输服务，关注点也不再是两个端点之间的通信，而是两个进程之间的通信（如客户端进程和服务器进程）

HTTP超文本传输协议，如果一个服务器进程支持超文本传输服务（web服务器），那么客户端进程想要请求服务器进程的某些资源，则必须遵守HTTP协议进行报文交换。  
超文本是一种非线性组织文本的方式，它的内容含有大量超链接，就像一个指向其他资源的指针，可以指向一个动态资源或静态资源包括servlet、文字、图片、视频、音频资源等。其中html超文本标记语言，是一种标记语言，不需要编译便可以直接被浏览器识别并渲染，是超文本的一种格式。

WEB是一个基于CS架构的网络应用，web可以大致分为四部分组成：【1】应用层协议【2】web浏览器【3】web服务器【4】传输的文档格式标准（如html）

网站就是网页的集合，网页的本质是一个个html文件，其中web浏览器实现了http客户端，用于请求超文本（html），如果web服务器实现了HTTP协议，那么它便可以被看做HTTP服务器（用于提供超文本传输服务的服务器），用于存储超文本，每个超文本通过URL寻址（同一资源定位符）。Http协议定义了http客户端向http服务器请求超文本的方式已经服务器向客户端回传超文本的方式。


### 网址

url是统一资源定位符，也就是常说的网址，用于定位某种资源。（不一定非用于web）

> URL统一资源定位符（locator）是一个更具体的概念，它不但说明了资源是什么，还告诉你怎么获取。而统一资源标识符URI(identitor)是抽象的，**它可以定位出某一个资源，但是你不知道如何获取**。总结：URL是URI的子集，URI是抽象的，URL是具体的。


>假设所有的Html文档都有唯一的编号，记作html:xxxxx，xxxxx是一串数字，即Html文档的身份证号码，这个能唯一标识一个Html文档，那么这个号码就是一个URI。
>而**URL则通过描述是哪个主机上哪个路径上的文件来唯一确定一个资源**，也就是定位的方式来实现的URI。
>对于现在网址我更倾向于叫它URL，毕竟它提供了资源的位置信息，如果有一天网址通过号码来标识变成了[http://741236985.html](https://link.zhihu.com/?target=http%3A//741236985.html)，那感觉叫成URI更为合适，不过这样子的话还得想办法找到这个资源咯…


>URI 不一定非得是通过号码确定的。URI 是在「某一规则」下标识出一个资源的字符串，通过地址或者通过号码都是可行的规则，其中通过地址规则实现的 URI 可以被称作 URL ，URL 是 URI 的一种实现，所以URI 作为更宽泛的定义是包含了 URL 的，就像三角形包含等边三角形一样。

语法规则  
**Scheme : // host . domain : port/path/filename**  
schema定义因特网服务类型，如ftp 、 http 、 file等  
Host定义域主机（http的默认主机是www）  
Domain定义因特网域名  
Port定义主机上的端口号  
Path定义资源在服务器上的路径  
Filename定位资源的名称

> 把WEB看作一个特殊的文件系统，那么URL就是普通的文件路径，和用于本地定位文件的路径没有差异。而文件系统是虚拟的，schema可以看作对这个文件系统实现的标识。两个不同的进程进行通信，其实也简单看作两个文件之间的数据复制操作（把网卡设备看作一个文件），我们从网络获取某个资源，也无非就是从文件系统检出某些文件，复制到本机的文件系统中。



### 输入地址一敲
这里简单聊一聊个人对“地址栏一敲”这个问题的思路。首先，我觉得没必要一直深挖到物理层，最多答到网络层即可，其实答到传输层即可。  
我们可以从两方面答：**客户端行为（浏览器发送）**和**服务器行为（服务器接收）**。

地址栏一敲，那么浏览器肯定先对输入进行检查，并且生成一个正确的URL。地址栏拼接好之后，浏览器肯定接着准备生成请求报文发出去，但是这个时候有两个工作可能还没做，一个是域名解析，另外一个是传输层建立连接。

> 这里域名解析和建立间接是否展开说看个人吧，如果可以展开说说也无妨。

域名解析完成后，浏览器构造一个请求报文就可以委托为操作系统发出去了（socket相关系统调用），操作系统发送报文前则需要先和服务器端点三次握手建立连接，如果基于http协议通信那么这时就可以直接把上层呈递的报文看作字节流传输出去了，如果是基于https协议还需要接着进行安全层的握手，进行算法协商、身份鉴别、计算会话秘钥等操作（是否展开看个人）。

> 还能再展开，数据从用户缓冲发送到操作系统为socket维护的发送缓冲，DMA设备将发送缓冲的数据拷贝到网卡控制器的缓冲区（一个网卡对应一个ip地址），最终数据到达服务器的网卡控制器的缓冲区，DMA将数据拷贝到socket的接收缓冲区时（数据准备完毕）通知CPU。

数据成功发出后，服务器进程将收到浏览器的请求，一般服务器会为每个客户单独分配一个线程，并且将请求包装为request对象，服务器根据用户的请求从服务器路径中找到可以处理服务请求的处理器（controller），并将处理得到的结果封装为一个response对象返回（例如，这是一个get请求，最终会找到一个可以处理get请求的处理器，也有可能直接交给静态资源处理器）。最终服务器返回的结果会基于http响应的形式达到客户端，浏览器拿到响应报文后，解析响应头、渲染响应体。

> 本题十分开放，很多地方都可以展开，不过也可以不深度展开而展现广度，如根据网站的业务加入一些对CDN、cookie/session验证的想法。浏览器一方比较固定，但是服务器如何响应就可以相对开放一些了（服务器软件本身做了什么+处理器（controller）大概做了什么）。

> 二次补充：开放题的核心要素就是能够去分类讨论，请求的是静态资源还是动态资源？配置了缓存吗？这个请求是直接给源站还是给代理服务器？配置反向代理吗…只有不断分类讨论，才能让提问者看见你的广度，而且你的回答“链路”越长，越能够看见你的深度，没错，这种问题就是多叉树式展开。


### HTTP细节

#### 请求头和响应头
Http请求报文的通用格式：  
请求行/一行行的请求头/空行/请求体  
其中请求行由三部分组成：**请求方法+URL+版本** _如GET /ABC/A.jsp HTTP/1.1_

Http响应报文的通用格式：  
响应行/一行行的响应头/空行/响应体  
其中响应行由三部分组成：**版本+响应码+短语** _如 HTTP/1.1 200 OK_

#### 常见的头部字段
其中accept和accept-charset是客户端期望服务器返回的资源类型，以及内容的编码。可以是多个值，使用逗号分割。  
而**content-Type**就是服务器对以上两个头信息的回应——例如Content-Type:text/html;charset=utf8，代表当前响应体的内容基于utf8编码，如果浏览器想要正确的读取响应体的内容，那么它需要使用utf8解码。

> 编码就是将字符在某一映射表下（字符表），转换为字节。人看不懂字节，只能看懂字符，但是传输是基于字节的，因此传输前需要将字符编码为字节，而不同的编码方式，最终获得的字节总数也不一样，因此不止一种编码方式。而一方使用错误的解码方式去解编码则会导致乱码。

contentType指的都是MIME类型，指示资源所属类型。如果遇到浏览器收到未知类型会提示下载。

accept-encoding客户告诉服务器自己能够接收什么样的压缩格式，content-Encoding服务器告诉客户端，数据使用了什么压缩格式

accept-Language表示客户端期待的原因，而回应即content-Language  
content-Length表示响应体的长度，收到响应报文的一方可以通过该字段直接获取响应体的长度。

Host是http请求报文必须携带的头，否则服务器将会抛出400 bad request。http1.1允许一个服务器部署多个web网站（一个服务器软件可以托管多个不同域名的网站，host可以指定具体的虚拟主机），用来指定服务器的域名

connect字段用于客户端声明连接复用，http1.1之后默认长连接connect:keep-alive，而http1.1之前只有短连接connect:close


#### 长连接和短连接
http是不需要建立连接的，建立完报文就直接委托传输层发送出去了，这里的连接指的是TCP的连接。  
短连接下，一次会话等于http请求报文传输完毕、http响应报文传输完毕，之后这个连接将会被主动释放。也就是说每个连接都是和一次request/response绑定的。

> 请求一个包含十个图片标签的html，最终会创建11个短的TCP链接。而如果使用长连接，只需要创建一个长连接。

长连接下，一次会话可以包含多次request/response，也就是说TCP连接不会在一次request/response后立即被释放，而是会被复用。  
http1.1默认支持长连接，请求头中加入connect:keep-alive，同一个用户的多次请求将复用同一条连接。

长连接的优点，是在多次连续通信时，节省了创建和关闭连接的开销（每次创建连接至少占用一个RTT）。但是需要占用额外的内存去维护这个连接，一旦服务器积压了过多的连接数，可能会影响服务器的性能。

现代大部分浏览器都会打开若干个并行的TCP连接，使得响应的时间缩短，一定程度上缩减了短连接创建花费的时间。**短连接是独立的，而长连接是复用的**，这可能导致长连接更容易被攻击而造成**安全性**问题。连接复用还有可能造成“**上层数据报文黏连**”的问题（TCP不知道要传输的数据独立的还是、连续的，要传输的数据都看作流），而短连接和UDP则不会。

但是短连接有一个很大的问题：短连接可能造成服务器短时间内创建大量连接，而且可能造成服务器大量套接字处于**timeWait状态**，以至于可用socket被耗尽，内存压力也很大。

> 使用短连接报文发送一个请求，需要两个TCP分组往返时延，同时服务器需要等待2MSL才可以释放通信套接字占用的系统资源而且每一次新建一个连接，都有分配新的缓冲区（发送缓存、接收缓存）和各种变量。


#### 状态追踪
http是无状态的协议，服务器连续收到两个请求报文，它无法判断这两个请求报文是否来自同一个客户端。  
无状态使得服务器不需要维护额外的变量去记录客户端的信息，但是也需要额外的手段去进行状态跟踪。  
[cookie、session、token的深刻理解，请看这里](https://blog.csdn.net/qq_44793993/article/details/118096657)

##### cookie
HTTP是无状态的协议，同一个用户发送连续两个请求报文，服务器并不能通过读取所谓的上下文去得知这个用户是否是一个“老用户”，究其原因还是因为服务器没有为客户端保存任何可以标识身份的信息。  
而什么是cookie，它是一个HTTP的请求头，这个请求头在request中就是**cookie**，而在response中就是**set-Cookie**。当客户端（浏览器）访问某个服务器时，服务器根据已保存cookie的domain和path决定捎带哪些cookie信息，传送给服务器。而服务器拿到cookie集合一般会遍历这个每一个cookie，并且找到感兴趣的cookie进行身份验证——可以看作是**上下文加载**。而如果没有找到感兴趣的cookie，则大概有两种可能：这是个新的用户，或者用户的cookie被清除或已经过期了。那么就在response中addCookie，浏览器收到后保存这个cookie

> 上面的行为并不是某种标准，因为后端程序员编程的内容就包含上述部分。例如从HTTPServletRequest中拿到cookies然后遍历，以及向“将要发送给客户端”的空HTTPServletResponse中种cookie（addCookie，底层对应设置set-cookie）

说白了，cookie就是HTTP的一个header，它的值是以键值对的形式组织的，而且**只能是ASCII字符编码**，如果某些字符不能被ASCII编码（中文就不能被ASCII编码），那么可以考虑往response种下之前进行一次编码。

```java
                   //URL编码
                    str = URLEncoder.encode(str, "utf-8");
                    c.setValue(str);   
```

下次从request拿到浏览器传来的cookie时进行一次解码

```java
                    //URL解码
                     String str = c.getValue();
                     time = URLDecoder.decode(str, "utf-8");
```

> 既然str不能被ASCII编码，那么我就将它使用utf-8编码，编码后str便可以可以被作为ASCII字符形式保存

另外cookie的值如果是明文的，容易被盗取。那么同理可以再“种之前”和“拿到后”进行加密和解密。cookie是HTTP报文频繁携带的数据，因此还可以考虑进行相应的压缩和解压缩。

cookie的属性项最常见的除了name/value，还有过期时间expires和最长存活时长max-age，如果不指定以上两个属性，那么cookie默认就是**会话cookie**，一旦会话结束就会被销毁，浏览器不会将其持久化到本地。

path：当前cookie是在服务器的哪一个路径生成的，并不是客户端保存路径，而是服务器创建cookie设置的（程序员设置的），**当浏览器访问服务器某个路径时，根据path判断该不该携带这个cookie**。假如我把path设置为项目虚拟路径根路径（setPath(request.getContextPath())），那么你浏览器只有访问我这个项目，总是会带上该cookie  
domain：当前cookie是在哪一个域名下生成的，**如果设置一级域名相同，那么多个服务器之间cookie可以共享这个cookie**  
例如：SetDomain(“.baidu.com”),那么访问tieba.baidu.com时，这个cookie也会被拿到。

cookie一般存储的内容有限，毕竟cookie是存储在客户端的，不同的浏览器具有不同的标准（对cookie数量的限制、大小的限制等），使得服务器不好统一管理。

如何加强cookie的可靠性  
【1】防止在cookie存放敏感信息，即使存放也应该进行加密，如使用HTTPS  
【2】加防篡改验证码和登录随机验证码  
服务器为用户存放两个cookie，一个是用户信息，另一个是对用户信息加随机数等加密参数产生的验证码，当客户端传来cookie时服务器需要进行验证。  
【3】对cookie的name、value（或全部属性）进行加密处理  
【4】强制要求开启HTTPS连接，服务器传送cookie时设置secure为true，表示创建的cookie只能在HTTPS连接中被浏览器传递到服务器端进行会话验证，如果是HTTP连接则不会传递该信息。

另外，cookie开启httpOnly可以防止JS读取cookie，而开启secure则保证了只有HTTPS服务才会传递cookie。



**理解cookie的实质**

上面说了一大堆，你能看出我对cookie的理解是什么吗？  
cookie它就是一个header、一个技术、一个工具。它是HTTP标准的header，也就是实现了HTTP规范的服务器和浏览器公认的、用于实现状态化的header。（你也可以使用一个自定义header去实现同样的效果，但是毕竟不是规范，不是所有浏览器和服务器都能理解你的header）。  
所以，cookie是有状态的还是无状态的？它就是一种工具，有状态还是无状态程序员说了算。

在做登录功能时，抛开各种框架，就拿最底层的来说，我们需要注册一个filter或者拦截器，这个东西你可以看作是一个AOP切面，用户访问主页、个人页面等**需要上下文关系”的页面**，都需要验证身份，那么如果我直接去访问某个页面必然会被拦截，如果浏览器的cookie不满足要求，一般会被重定位到登录页面。

那怎么验证身份呢？  
【1】浏览器出示凭证，服务器不保存各种会话信息，验证浏览器出示凭证的合法性和真实性则放行，**从服务器的角度**，这种情况一般看作无状态实现的，一般称为token。  
【2】服务器保存会话信息，并且在浏览器上保存一个指向会话信息的唯一标识，浏览器出示唯一标识，服务器去查询。服务器保存了会话信息，因此这种实现被看作有状态的，一般称为session。

如果你拿到request递上来的cookies，检查没毛病后直接就放行，相当于验证了浏览器携带的token。如果cookie中存的是一个指针，而真正的会话信息保存在服务器，那么你就需要根据指针拿到对应的会话信息，即根据浏览器携带的sessionId拿到session。  
那我既给cookie存指针又存放数据呢？那我给cookie存数据还有再给服务器保存一份副本呢？都可以啊，你是程序员，而cookie就是一个工具，你爱怎么用怎么用——_**cookie本质上就是HTTP用于实现传递会话信息以实现有状态的一个HTTP标准header，本质上就是一个工具，真正其决定作用的是使用它的程序员**_。



##### session
很多面试题喜欢把cookie和session进行对比，其实我早就觉得有点不舒服了，因为这二者压根不是一个维度的东西啊。因为cookie算是一种实现HTTP有状态的标准header，而session更像一种实现有状态的思想——建立一种虚拟的会话。  
cookie是什么？客户端向服务器打招呼，然后把自己的cookie传过去，服务器一看它有我之前给他存的cookie，那我们应该认识，那我就替你恢复一下上下文吧——解析cookie，相当于恢复和当前用户的会话上下文。  
如果说cookie传递的是值，那么session传递是一个指针、一个索引。  
session可以看作是一种思想，实现方式有很多：基于cookie的、基于URL的、基于post表单的、会话信息保存在单机的、会话信息保存在中间件的…  
没有session之前，每次服务器想要“恢复上下文”，客户端必须通过cookie传递大量上下文信息（**极大浪费带宽，接收cookie需要更多流量和时间**），而且结构不灵活（KV、ASCII等），而基于cookie的session中，你cookie不需要存储过多信息或敏感信息，我session替你分担一下压力。  
服务器向客户端种下cookie时，额外种下了一个保存的sessionId的cookie，这个sessionId指向服务器的一个session对象，这个session对象一般由服务器软件的session集合集中管理。一个session就可以看作一个mapList，最重要的是，这是一个**更加抽象的会话对象**——它可以保持任何类型的会话信息，不论是字符串还是对象。

> Tomcat的Manager类有一个session集合的成员，Manager类将会管理所有session的声明周期，session过期则回收session对象，服务器关闭则将session对象序列化到服务器磁盘上，用户可以根据sessionId拿到对应的session对象

session对象是保存到服务器上的，过多的session对象给服务器的压力会很大，所以每个session对象不总是保存在服务器的内存中，通常会根据服务器的钝化策略持久化到磁盘上。（保证浏览器cookie的sessionId过来，能拿到session对象就没问题）

从上面的讨论来看，cookie和session其实不适合放在一起对比，session和cookie哪一个安全？你还不如问我cookie存放值和存放指针哪一个安全嘞  
cookie的一部分内容存放在服务器，传输的时候顶多看见一个sessionId，但是不论是cookie存放的值还是sessionId都有可能被盗取。而且如果真想要会话信息不被窃取，最好还是强制HTTP使用加密的HTTPS

> HTTPS对URL、header、body都会进行加密，而且可以保证完整性。

与其对比session和cookie，不如对比token和session  
token可以看作：客户端主动出示证件然后服务器放行，而session可以看作浏览器出示一个证件的号码，然后服务器去查询这个号码拿到会话信息才放行。（本质上，不管是token还是sessionId，都可以基于cookie传递，而且都是没有直接含义的字符串）

> token、session终究也是一种抽象的思想，不同的产品有不同的实现，谈论也都是抽象的，如果非要具体到某一个细节，应该指明哪一个产品


##### 使用场景
一般面试问cookie和session，你完全可以看作：使用值和传递指针。  
一般传递指针优先于传递值，随便想想就能想一些：  
【1】会话信息一般都不少，而且cookie是作为频繁传递的header，必然造成大量带宽浪费（传递cookie前可以考虑压缩内容）

> HTTP2 开启 头部压缩，对头部使用压缩算法进行压缩，而对于频繁传递且不变化的值user-agent、cookie等保存在头信息表，使用指针代替值进行传递

【2】cookie传递值，如果不进行加密肯定会被看到如name=abd&age=123，最好传递cookie前进行加密（或者使用HTTPS）  
【3】使用cookie保存值只能是ascii不灵活，如果我想要保存一个对象，还需要在传递前转换为cookie支持的格式，肯定没使用session灵活  
【4】另一方面，cookie是保存在客户端的，（这种情况下，服务器一般不保存会话信息）那么相当于服务器就失去对会话信息的控制权，为了方式不安全的cookie，服务器必然需要某种检查、校验机制（数字签名、时间戳等，基本上就是token的思想了）

当然了，cookie存值也是可以的，因为如果所有会话信息都保存在服务器，那么服务器内存也不够用啊。另一方面，如果服务器是**分布式部署**的，那么session对象将可能无法及时同步，导致一个持有sessionId的用户总是无法正常自动登录。

> 一种解决方案是让每个集群服务器拿到所有session、另一种方案是当存在session未命中则进行一次session同步（相当于第一种方案的延迟同步）、一种主流的方案是使用中间件如redis作为第三方session容器，每次从中间件去拿session。

一般不把所有会话信息（上下文信息）放入session，一般将cookie放不下的信息、敏感信息如密码、账户（一般都是抽象为一个相应的对象）等放入session，session由服务器软件的session容器集中保存（这里仅指代单机存储session的场景）。

**cookie一般放什么**？  
一些指针信息（如sessionId）、一些视频小网站没有登录业务但是也要保存用户的浏览记录，那么观看历史记录、搜索历史记录都可以保存到cookie中（可能就是一个名字，也可能是一条记录的id）、自定义设置（背景图片、个性栏等）、一些不重要的临时信息（例如浏览器型号、手机型号）。  
还可以实现一些小功能如拿到用户上一次访问时间、统计用户访问总数、拿到用户上一次登录的ip地址等等。（说白了就是读取上下文相关的功能）

> cookie存放的东西五花八门，而且不同的网站都有不同的存放需求，你可以随便打开个网页抓个包，看看cookie里面都有什么

至于token，它可以保存到URL传递，也可以保存到cookie或其他的头部，它是一个字符串，看作一个凭证，通常由用户标识符+时间戳+签名组成，服务器使用私钥对它签名并且保存在客户端，当客户端回传token时，服务器再次计算签名，如果没错则视为当前用户通过验证并放行，对于要求状态的页面，浏览器总是需要携带这个服务器签发的token。一般将基于token的验证看作无状态的，服务器不记录哪些用户已经登录，它的唯一职责就是**签发token**以及**验证token**，每个token都是独立的。  
token更适合用于权限管理的系统，如：一个网站的业务就是普通用户看普通内容，VIP看普通内容+VIP内容，那么服务器唯一要做的就是拦截普通用户和为氪金用户签发token，这里面不需要太多上下文信息去保存，只需要做好访问控制即可。（session也能做，但是如果业务就是单纯访问控制，还是token做比较好：服务器压力小、逻辑简单、安全性高一些…）

> JWT就是json格式的token，有具体的结构：header+payload+signature。其实本质上就是一个约定好格式的json字符串。

因此，一般说**HTTP实现有状态（保存和用户会话的上下文）**的具体方案，无非就是cookie+session或者cookie+token和cookie+session+token，当然取决于程序员如何使用cookie这个工具去实现会话状态。

##### cookie/session/token总结
cookie是一种工具，每次HTTP客户端与HTTP服务器再次建立连接时，无状态的HTTP服务为了加载上下文，客户端必须把上下文数据通过cookie传送给服务器。但是上下文数据通常很大，而且不同的服务器给客户端保存的上下文数据各种各样，那么将上下文保存在服务器一方，并且将相应指针保存给客户端，保存在服务器的上下文就是会话的抽象对象即session。而token的主要目的用于授权，服务器向客户端颁发令牌token，当客户端携带令牌访问某些需要权限的网页时，服务器检查token并决定是否放行，可以将token看作一种用于证明身份的上下文数据。

总结：HTTP协议是无状态的，加载状态信息的方式是传递上下文数据（会话信息），而cookie是其中一种方式（还可以通过改写URL或者隐藏表单提交）。而session是保存在服务器的会话信息，其中sessionId和token和普通的cookie数据一样，都是字符串，都可以看作上下文数据，只不过业务功能不同罢了。


##### CSRF 攻击
CSRF 跨域伪造请求攻击，攻击者利用浏览器的缺点——无法判断打开某个网页是否是某个人的真实行为

用户访问A网站并拿到了cookie，而当用户访问B网站时误点某一个连接，这个连接指向A网站，例如：用个人账户发布某些虚假信息，访问A网站时浏览器自动将cookie传送过去，于是请求得到响应。

攻击者的攻击原理就是骗得浏览器信任，导致浏览器cookie传递给服务器，加载了上下文后攻击者间接盗取了被攻击者的登录信息/会话信息。  
防御方案：  
【1】限制访问来源——拿到请求头referer，判断请求来源的合法性和可信度  
【2】URL中添加token——攻击者虽然能够劫持cookie，但是它并不知道cookie的内容。  
服务器随机产生token（如对cookie的内容进行摘要计算和加密或者是生成一个随机数）存入session或cookie。**前端发送请求时先拿到token，然后将token放入url**。服务器执行请求前验证token ，攻击者无法伪造token（也就是说请求中总是需要包含正确的token，攻击者只是劫持cookie，但是并不知道token，更无法从cookie中拿到token，而**防止了攻击者去人为伪造被攻击用户的请求**），因此可以防范csrf

总结：通过在cookie中放入csrf_token可以防止CSRF攻击（不一定是cookie，只要是前端能够能够读取这个token。为了让前端能够解析cookie，不设置HTTPOnly选项）。  
csrf_token经过加密后保存在cookie中，**同源页面**每次发送请求时都会在请求头或者参数中加入cookie中读取到的token。而CSRF攻击每次只能拿到一大堆cookie而不能解析其中的内容，更不可能拿到其中的token去构造合法的http请求。**token经过加密存储在cookie中，只有同源网页才能获取到即可以规避非法来源的请求**。

简而言之，想要获取目标网页，必须从当前网页拿到token上下文，而CSRF攻击者无法解析cookie中的token，没有上下文无法请求目标网页，最终的下场就是被拦截器拦截。


#### 响应码

响应码可以分为五类，以下将列举常见的响应码：  
【1】服务器收到请求，要求客户端继续执行  
100 continue

> 通常，浏览器发送post请求时，需要两个TCP连接，先发送header，然后收到接收方的一个100响应码，然后继续传送body

101 switching protocols  
【2】操作被成功处理  
200 OK  
【3】重定向  
301 永久重定向

> 应用场景，原来的域名弃用了，但是还想往新域名继续引流，某些小网站直呼内行。

302 临时重定向  
304 资源未修改（配合缓存使用）  
305 使用代理  
【4】客户端错误  
400 bad request  
401 未认证  
403 forbidden  
404 资源未找到  
408 超时  
【5】服务器错误  
500 内部错误  
501 服务器不支持该请求  
502 bad gateway 代理服务器收到源服务器无效响应

> 如果一个服务器被DDOS干崩了，可能就会显示这个。

505 服务器不支持请求的HTTP版本


#### 对get和put的思考

Get是一个**拉**请求，用于请求资源。而post是一个**推**请求，用于提交信息。  
Get是不安全，因为请求的参数将被拼接在URL（浏览器地址栏）中，可能会被窃取。而post提交的信息存于请求体中，用户不可见  
Get请求的url有长度现在，而post内容存于响应体，可以存储大量的内容。  
基本不同就是：语义、数据存放位置、数据容量、数据敏感性、安全性（安全性这一点很鸡肋，因为如果要求安全应该使用HTTPS，就算明文不写在URL上面，难道wireShark嗅探不出来body的内容吗）

请求方法之间的本质：  
get、post、delete等都可以用来传输信息，get请求可以在body传输数据，而post也可以在url中传输数据，但是这不符合HTTP的语义规范。因此他们只在**语义上产生区别**，而安全的保证需要使用HTTPs协议

#### HTTP版本
HTTP采用了长连接，也称为持续连接。持续连接又可以流水线形式和非流水线（pipeline）形式。要知道，HTTP是基于TCP进行报文传输的，TCP可以一次性发出多个分组来提升效率，但是非流水线的HTTP则是类似“停等”的，浏览器必须收到服务器的前一个响应才能接着发出下一个请求。这对应短连接是很正常的，但是对于长连接就会使得TCP连接处于空闲。由于长连接中，TCP连接是多个HTTP请求复用的，因此HTTP1.1是支持流水线（pipeline）的，可以减少整体的HTTP请求响应时间。  
但是HTTP1.1存在**队头阻塞**问题：多个HTTP请求可以在未收到前一个响应就全部发出去，但是服务器仍然按照顺序进行响应，顺序靠后的请求仍然需要等待靠前的请求被响应后才可以得到处理。（你想想，运输层全是清一色的字节流，你后发的请求先到达了在TCP看来就是提前来的，迟到的“空洞”不补上，那就等着吧）  
总之：http1.1相对http1.0，增加了对长连接的支持，以及支持流水线式的长连接。  
但是仍然具有缺点：头部信息不压缩，占用额外带宽。队头阻塞问题、请求无法定义优先级、服务器只能进行响应。以及明文传输的问题



后面请求到了,但处理不了

##### HTTP2

http2支持**压缩头部**。使用HPACK算法，在客户端和服务器同时维护一张头信息表，使用索引替代字段。传输效率提升  
http2不再使用文本格式，而是使用**二进制格式**，将头信息和数据体称为**头信息帧**和**数据帧**。  
	http2的数据包不是按顺序发送的，因此同一个连接中的若干个连续的数据包可能属于不同的响应。http2对同一个连接中的每个数据包都进行标记，每个请求/响应的所有数据包称为一个**数据流stream**。每个数据流都拥有独一无二的标记（且区分请求与响应数据流）。客户端还可以指定数据流的优先级，让服务器优先响应。（多个请求复用一个TCP连接，通过数据流标识某一个请求/响应）

**HTTP2可以并发的处理多个HTTP请求,这才是提高效率的关键,防止前面的请求处理时间长导致后面阻塞时间太久**

> http2在同一个tcp连接中发出了10个http请求，每个请求/响应所属的数据包看作一个流，而且具有编号标识它属于哪个请求或响应。http2是可以在一个连接**并发处理**多个请求或回应的，而且不用按照顺序意义对应（根据编号），多个HTTP请求/响应复用了一个TCP连接，而且是**非串行复用**的，降低了延迟，大幅提高了连接的利用率。

http2中，服务器是可以主动向客户端发送消息的，改善了传统的请求-应答模式，服务器可以提前将静态资源推送到客户端——在浏览器刚请求 HTML 的时候，就提前把可能会用到的 JS、CSS 文件等静态资源主动发给客户端，**减少延时的等待**。

> 队头阻塞问题的产生：应用层的处理顺序是request1-response1 request2-response2,此时即使request2提前到达服务器，服务器也需要先响应request1.HTTP2将request切分为若干帧，并且将一次request-response使用进行标记，并将其抽象为数据流。服务器并发处理数据流中的各种数据包。

队头阻塞在TCP层发生

##### HTTP3

http2多个数据流复用同一条连接，而且是无序的，但是在TCP看来，这就是一条普通的数据流，一旦发生丢包就会触发重传机制，这样**所有的HTTP请求都必须等待丢失的包被重传**。这是由TCP的特点决定的，http作为应用层协议无法干涉。

> 相当于http1.x的时候，http客户端发送三个包123，http服务器强制按照123去接收，而http2的时候，这三个包的发送顺序是不固定的，如果发送321，那么底层TCP不关心，它也是按照321对应的字节流去传输的，而不管最后哪一个包到达，http服务器都会进行响应。但是我们通过改变应用层协议解决了队头阻塞问题，而底层TCP仍然存在“空洞”报文无法向上呈递的问题。加入最终到达的是31缺少个2，那么空洞报文后面的分组及时到达也需要阻塞。

为了解决这个问题，HTTP3将传输层协议更换为了UDP，而且UDP不关心顺序和丢包，因此也不会出现队头阻塞和丢包重传问题。  
为了保留可靠性，使用了基于UDP的QUIC协议，实现了类似TCP的可靠性传输。  
QUIC有自己的一套机制可以保证传输的可靠性，当某个数据发送丢包时，之后阻塞这个流而不是影响其他流。  
QUIC实际上相当于：支持可靠传输UDP + TLS + HTTP/2 的多路复用协议。


## HTTPS

HTTP的头部信息是基于键值对的，比较容易理解。而且首部字段允许开发人员自定义添加，灵活性和扩展性也比较好。但是HTTP是基于明文传输的，存在安全性问题。  
HTTPS就是提供安全服务的HTTP，在应用层与传输层之间增加了一个安全层子层SSL/TLS，其中HTTPS的熟知端口号是443.**发送数据时，应用程序向将数据呈递给SSL套接字（安全套接字），对数据加密后才会交给TCP套接字发送出去。**

总结：HTTPS本质上是在传输层与应用层之间增加了一个安全子层的HTTP服务，可以保证数据传输加密，同时可以保证数据完整性。

### 密码体制
对称加密体制中，加密秘钥和解密秘钥使用相同的密码体制，如DES和AES。对称加密的算法是公开的，但是**秘钥本身不公开**（如算法是1和0交换，秘钥是第几位被交换了）。对称秘钥体系中，双方使用相同的秘钥，但是**秘钥分配问题**和**数字签名**没有公钥体系实现方便。

> 密码体系和加密解密的效率没有本质关系，加密解密效率取决于算法和选取秘钥的复杂度，越复杂越安全。我的理解是，目前流行的公钥加密算法开销普遍都比较大，所以频繁数据传输一般偏向于使用对称加密算法进行加密。

（对称秘钥使用秘钥分配中心KDC进行秘钥分配是高度安全的，但是会产生大量网络带宽）

非对称加密体制中（也称**公钥密码体制**），能够被公开的钥匙称为公钥，自己保存私钥。其中**公钥只能加密不能解密**，因此加密和解密需要两把钥匙，最常用的就是RSA算法。  
非对称加密的公钥是公开的，而且公钥只能加密，那么A和B通信，A使用B的公钥加密，B收到后使用私钥解密，反过来同理，这就解决了分配难题（压根不用分配，反正公钥大家都知道，就像qq号一样，你知道它的qq号就可以向他发送消息，它登录qq就能看见，你的qq也是公开的）

> 这里存在中间人攻击，因为你怎么知道这个qq是不是被掉包的qq？通信前我们需要实体鉴别，防范中间人攻击（这里的中间人攻击仅指协商阶段）。

**HTTPS采用的是对称加密和非对称加密的混合加密。**  
1. 通信前，采用**非对称**加密的方式协商出**会话秘钥**。  
2. 通信过程中，使用会话秘钥进行**对称**加密数据。

> 举个例子，你想告诉基友你们的暗号，但是教室人多眼杂，于是你们通过qq聊天去发送这条信息，你们俩聊天（不考虑服务器）后，就协商出了暗号，以后使用暗号在线下传输信息。

RSA有哪些缺陷？一旦私钥泄露，那么历史中的所有加密信息都会被破解。（注意：讨论要具体到某一种具体的算法，而不要上升到某一种体制）

补充：  
为什非对称慢，因为现在流行的算法中，**对称**加密大多以**位移和替换及一些位运算**为主，而**非对称**加密涉及**数论**运算。  
如DES算法，按64位进行分组，秘钥也是64位，每64位明文分别与秘钥进行多轮置换，最后生成64位密文。而例如RSA涉及**大数乘法、大数取模，还有一些涉及离散对数、圆锥曲线的算法**。  
公钥密码体制一般分为三类：大整数分解、圆锥曲线、离散对数



**RSA公钥和私钥是什么？**

首先来说，RSA是一种非对称加密算法，它是由三位数学家（Rivest、Shamir、Adleman）设计出来的。非对称加密是相对于对称加密而言的。对称加密算法是指加密解密使用的是同一个秘钥，而非对称加密是由两个密钥（公钥、私钥）来进行加密解密的，由此可见非对称加密安全性更高。

公钥顾名思义就是公开的密钥会发放给多个持有人，而私钥是私有密码往往只有一个持有人。

**公私钥特性**

-   公钥与私钥是成对出现的；
    
-   私钥文件中包含了公钥数据，所以可以基于私钥导出公钥；
    
-   密钥越长，越难破解，所以2048位密钥比1024位密钥要更安全；
    
-   公钥和私钥都是密钥，被公开的那个就是公钥，没有被公开的那个就是私钥。
    

**公钥和私钥都可用于加密和解密**

公钥和私钥都可以用于加解密操作，用公钥加密的数据只能由对应的私钥解密，反之亦然。虽说两者都可用于加密，但是不同场景使用不同的密钥来加密，规则如下：

1、私钥用于签名、公钥用于验签

签名和加密作用不同，签名并不是为了保密，而是为了保证这个签名是由特定的某个人签名的，而不是被其它人伪造的签名，所以私钥的私有性就适合用在签名用途上。

私钥签名后，只能由对应的公钥解密，公钥又是公开的（很多人可持有），所以这些人拿着公钥来解密，解密成功后就能判断出是持有私钥的人做的签名，验证了身份合法性。

2、公钥用于加密、私钥用于解密，这才能起到加密作用

因为公钥是公开的，很多人可以持有公钥。若用私钥加密，那所有持有公钥的人都可以进行解密，这是不安全的！

若用公钥加密，那只能由私钥解密，而私钥是私有不公开的，只能由特定的私钥持有人解密，保证的数据的安全性。


### 鉴别

鉴别主要可以被分为两个方面：报文没有被掉包、通信对方没有被冒充



#### 数字签名

公钥对称加密体系很容易实现数字签名，就是使用私钥对报文进行加密，公钥是公开的，因此外界任何人都能够鉴别这个签名，当然了，这个“报文”肯定不可能是通信的报文，使用一小段摘要作为数字签名即可，这个摘要信息就可以作为当前报文的“指纹”——**用私钥对摘要信息进行加密就形成了数字签名**。

数字签名能够干什么？  
【1】发送方不能够否认——因为只有发送方拥有私钥，所以只有发送方可以进行签名，如果私钥被盗了那就怪你自己了…  
【2】报文鉴别——接收方收到后，对其进行验证，能够知道这个报文确实来自发送方。

> 它产生自发送方，但是它的发送人一定是发送方吗？这有两种攻击方式：  
> 【1】这是一个旧报文，攻击者嗅探到之后“重新发送”这个报文，然后发送方就会给攻击者返回响应的响应报文，虽然这是密文。但是加入私钥被攻击者盗取了，那么这些密文很可能也会被立刻获取（rsa的缺陷）。  
> 对付这种重放攻击，可以采用不重数（不重复的随机数），这样使得即使攻击者无法使用过期的不重数。  
> [重放攻击-实例解析 - 想思己自录记 - 博客园 (cnblogs.com)](https://www.cnblogs.com/sharphui2018727-WYH/p/10068383.html)
> 上图中使用对称密钥的方法有很大的漏洞。例如：入侵者C可以从网上截获A发给B的报文，C并不需要破译这个报文，而是直接把这个由A加密的报文发送给B，使B误认为C就是A；然后B就向伪装成A的C发送许多本来应当发给A的报文。这就叫做重放攻击
> 
> **使用公钥密码体制时，可以对不重数进行签名鉴别**。  
> 【2】公钥掉包——属于中间人攻击，CA认证就是一种防御方式，后面会聊到。

【3】报文完整性——报文没有被中间人篡改过，可以基于报文鉴别码（MAC）实现。

数字签名是实现报文鉴别+实体鉴别的基础。

#### 报文鉴别

实现报文鉴别需求，我们如何生成一个数字签名？  
散列函数是一种简单的方式，因为散列函数的输入可以不做要求，而输出的长度是固定的且比较短小，密码学中使用的散列函数称为密码散列函数，特点：**两个不同的报文，经过散列函数计算后，他们不可能有相同的输出，而且散列函数不能后被结果反推输入（多对一映射）**。

例如一个报文X，我们计算它的散列函数输出为H（X），这个H（X）就可以看作当前报文的“指纹”。但是这个H（X）不能直接被附加在报文后面，因为攻击者有可能**将报文+摘要一块篡改**，因此这个摘要必须使用秘钥加密（可以但是不一定是私钥），加密得到的【H（X）】称为报文鉴别码MAC  
这个MAC被附加在明文后，加密后传输到对端，对端解密后拿到两个部分：明文和MAC，对端使用秘钥（假设对端已知MAC解密秘钥）对MAC进行解密得到H（X），然后使用协商好的鉴别算法（如MD5）对明文进行散列计算，然后比对两个H（X）是否一致，一致则证明了报文没有被篡改。

> 以上建立在MAC秘钥没有泄露的前提上

注意：**由于明文有可能是重复的（如重复的请求），因此为了防止重放攻击，可以使用不重数作为摘要进行生成MAC。（对不重数进行签名），SSL握手就是对不重数进行签名的**

> 对摘要片段、不重数而不是对整个报文进行签名，需要的计算量大大下降，因为报文鉴别是一个频繁的操作——传输的报文（加密前）包含两个部分，一个是明文，另一个是MAC（签名）

使用不重数，可以区分重复请求和新请求，类似TCP序号机制

#### 实体鉴别

实体鉴别通常和公钥的分配挂钩，需要有一个值得信赖的机构CA（认证中心），来将服务器的公钥和对应的实体（服务器）进行绑定——类似于天子诏书，见诏如见朕

报文鉴别是**对每一个收到的报文，都需要鉴别报文的发送者**，实体鉴别是在系统接入的全部持续时间内，对和自己通信的实体进行验证，**只需要一次即可（目的就是拿到信任的公钥）**

实体鉴别可以防止**中间人攻击——掉包公钥**

> 接收方一开始拿到就是被攻击者掉包的公钥，看似接收方与正在的接收方在通信，其中从一开始就是攻击者和接收方在通信，攻击者还会将接收方的回复（使用攻击者私钥解密后，再使用发送方的公钥加密）转发给发送方，使得发送方无法察觉中间人的存在

客户端先向服务器发送握手报文，服务器向客户端发送自己的数字证书（附上公钥）。其中数字证书是第三方权威机构CA认证完毕的，是公开的。认证中心CA将公钥于实体（网站）进行了绑定认证，客户端（浏览器）就可以使用CA公布的公钥验证服务器数字证书的真实性。

> 服务器将自己的公钥注册到CA，CA使用自己的私钥对服务器的公钥（将注册者有关的各种信息使用散列函数得到一个hash摘要值）进行数字签名（私钥计算）并颁布数字证书。客户端的操作系统中保存了CA的公钥（使用CA的公钥计算对数字签名进行计算就能得到服务器的公钥，比对完毕就可以确认真实性后就可以确认此公钥是真实的）

以上的前提是“受信任的证书一定安全”，事实上“流氓证书”也是一种攻击方式，通常通过操纵或破坏证书颁发机构来获取主要浏览器信任的“流氓”证书，但是这种方式就比较花费成本了。

总结：HTTPS可以保证消息的安全传输，主要包括**防窃听**（加密传输）、**防止篡改**（摘要算法/MAC报文鉴别码）、**防止中间人攻击**（通过CA验证数字证书获得正确公钥）、**防止重放攻击**（不重数）。

注意：HTTPS无法防止CSRF（跨站请求仿造）攻击，因为这和浏览器行为有关。也不能防止木马，这是客户端行为（私钥被盗取、保存的CA公钥列表也可能被修改）



客户端向服务端发起加密通信请求,这时客户端会生成一个不重数,并且把自己支持的算法,协议版本等信息发送给服务器.

服务器收到客户端请求后发出响应,响应内容包含服务器的数字证书,确认的加密算法(RSA),生成的不重数,确认的协商版本
这里涉及到一个实体鉴别的问题--实体鉴别可以防止中间人攻击,怎么解决这个问题?
方法是通过数字证书中的数字签名,数字签名是CA把持有者的公钥,用途,颁发者等信息打成成一个包,再对这些信息进行hash计算,得到一个hash值,CA再用自己的私钥将该hash值加密,生成数字签名

客户端收到数字证书后也会使用同样的hash算法获取hash值,再通过浏览器或操作系统的CA公钥对数字签名进行解密,若两个hash值相同则可信,否则不可信


### HTTPS握手流程

https整体可以分为**证书验证**阶段和**数据传输**阶段。  
【1】在三次握手之后，客户端向服务器发起加密通信请求，客户端生成一个随机数（不重数），并且把自己支持的算法、协议版本等信息发送给服务器。服务器收到客户端请求后发出响应，响应的内容包含服务器的数字证书、确认的加密算法（如RSA）、生成的随机数、确认的协议版本。

> SSL允许双方协商出一种对称加密算法（如AES）、一种非对称加密算法（如RSA）和一组MAC算法（如SHA-1），服务器将算法选择、数字证书和服务器不重数返回给客户

【2】客户端收到响应后，使用浏览器或操作系统存储的CA公钥验证服务器数字证书的真实性。  
以上两个握手后，双方协商完毕了协议（TLS）版本号、使用的算法，而且互相传递了各自生成的随机数，随机数是后序生成会话秘钥的条件。

> 注意，前两次不重数都是没有加密的（因为还没有协商出算法，也没有传递公钥），第三个不重数是经过加密的，也称为前主密钥，双方使用三个随机数最终可以计算出一个主密钥。

【3】客户端生成一个新的随机数（前主密钥 pre-master key），并且使用服务器的RSA公钥进行加密，并且发给服务器，服务器收到后使用私钥解密得到这个随机数。此时双方都持有了三个随机数，最终会生成一个会话秘钥（主密钥 master secret），用于对称加密。生成会话密钥完毕后，第三次握手中客户端还会表示“加密通信算法改为基于会话秘钥的对称加密”，并且表示客户端握手结束的通知

告诉服务端开始使⽤加密⽅式发送消息。
把之前所有发送的数据做个摘要，再⽤会话密钥（master secret）加密⼀下，让服务器做个验证，验证加密通信是否可⽤和之前握⼿信息是否有被中途篡改过

> 为了更加安全，通常会基于主密钥（将主密钥切片）生成**四个密钥**：两个**加密秘钥**用于加密数据、两个**MAC秘钥**用于验证数据完整性。（全双工，A发送到B和B发送到A使用不同的对称秘钥）

【4】服务器收到第三个随机数后也将计算出会话秘钥，同时向客户端发送最后的信息——使用会话秘钥通信、服务器握手结束的通知

> 最后的握手报文中，客户端还会发送所有握手报文的一个MAC（对所有报文进行报文鉴别），服务器也会发送所有握手报文的一个MAC。这可以防止**算法弱化攻击**（篡改协商算法列表），如果“通过鉴别，发现二者看见的存在不一致的情况”则终止连接。

总结：HTTPS的连接建立大致可以分为TCP三次握手建立连接与SSL/TLS四次握手安全建立阶段，其中SSL/TLS四次握手可以概况为：**协商加密算法**、**服务器鉴别**、**计算会话秘钥**和**安全数据传输**。（传输的过程中也伴随着完整性的校验）

建立安全的代价就是需要耗费更多的带宽去进行“安全握手”，也增加了网络时延和响应时间等问题，而且使用HTTPS协议是有偿的。








### SSL记录

SSL是HTTP与TCP之间的协议，那么它也是具有格式的。  
SSL将数据流分割为记录，**对每个记录附加一个MAC用于完整性检查**

SSL记录有以下五部分组成：  
【1】类型  
【2】版本  
【3】长度  
【4】数据  
【5】MAC  
其中前个字段是明文，长度字段是为了从TCP字节流中分离出SSL记录。其中后两个字段使用加密秘钥加密。类型字段指明这是一个握手报文还是传输报文，也可以用于关闭SSL连接

> 类型字段可以防止截断攻击——构造FIN报文提前终止连接。类型字段指出该记录是否用于终止SSL连接

### 重放

重放攻击属于中间人攻击的一种，中间人截获报文并进行重新发送，接收方误以为中间人为实际通信者

公钥加密体制则使用不重数处理（**握手阶段**）重放攻击，使用不重复使用的随机数（全局不重数），接收方只接收一次不重数，重放报文将被看作无效报文。  
HTTPs可以防范中间人对报文的篡改（基于报文鉴别码），但是**无法阻止报文被截获**。

防止重放攻击的关键就是**保证报文的唯一性**

**解决方案**：

**时间戳+随机串——保证一个时间窗口内生成的随机串不重复**（使用时间窗口节省生成不重复字符串的成本）

假设时间窗口60s，然后使用redis记录出现过的随机串，这个串的过期时间就设置为60s。而且时间戳和随机串必须被私钥加密（作为签名的一部分），如果外界修改时间戳和随机串都会导致签名验证失败，这60s内，如果出现了重复的请求就直接忽略（同一用户刷新一次页面算作不同请求，因为随机串会重新生成）

> 以上可以看作HTTP的重放攻击解决方案，https可以避免重放攻击，数据传输时，通过SSL会对记录数进行计数，并且作为MAC的一部分。

SSL可以防止**分组级别**和**连接级别**的重放：  
【1】虽然TCP具有序号机制，但是SSL记录向HTTP传递的过程中，有可能出现：攻击者增删记录、调整记录顺序的情况。SSL维护了计数器，并且对每个发出SSL记录都进行计数（相当于SSL层的序号机制），这个序号并不是记录的一项，而是被包含在MAC中，作为报文鉴别的一个项目存在。  
【2】数据传输过程中不要求不重数，因为一旦握手阶段校验不通过，那么**连接级别的重放攻击**就无法进行。

> 攻击者通过嗅探整个HTTPS会话的报文，这些报文都是通过验证的，因此攻击者可以尝试发起连接重放攻击

SSL基于**不重数**解决连接重放攻击（和之前讨论的一样），SSL握手报文中使用的是不重数，而且每个HTTPS会话使用的不重数都是不同的，SSL计算会话秘钥也是参考不重数的，因此这使得不同HTTPS会话的会话秘钥都是不同的。这使得，即使一个HTTPS会话的所有报文被攻击者监听到，这个HTTPS会话的报文也不会被服务器再次响应了，避免了连接重放攻击

总结：**SSL中使用不重数防止连接重放攻击（和握手阶段的分组重放），序号用于防止分组级别的重放攻击（传输阶段）**。




## 简述cookie
HTTP 协议本身是无状态的，为了使其能处理更加复杂的逻辑，HTTP/1.1 引入 Cookie 来保存状态信息。

Cookie是由服务端产生的，再发送给客户端保存，当客户端再次访问的时候，服务器可根据cookie辨识客户端是哪个，以此可以做个性化推送，免账号密码登录等等。

## 简述session
session用于标记特定客户端信息，存在在服务器的一个文件里。一般客户端带Cookie对服务器进行访问，可通过cookie中的session id从整个session中查询到服务器记录的关于客户端的信息。



## 转发和重定向的区别

转发是服务器行为。服务器直接向目标地址访问URL,将相应内容读取之后发给浏览器，用户浏览器地址栏URL不变，转发页面和转发到的页面可以共享request里面的数据。

重定向是利用服务器返回的状态码来实现的，如果服务器返回301或者302，浏览器收到新的消息后自动跳转到新的网址重新请求资源。用户的地址栏url会发生改变，而且不能共享数据。

## 简述http1.0

规定了请求头和请求尾，响应头和响应尾（get post）

每一个请求都是一个单独的连接，做不到连接的复用

## 简述http1.1的改进

HTTP1.1默认开启长连接，在一个TCP连接上可以传送多个HTTP请求和响应。使用 TCP 长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。

支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。

服务端无法主动push

## 简述HTTP短连接与长连接区别

HTTP中的长连接短连接指HTTP底层TCP的连接。

短连接：客户端与服务器进行一次HTTP连接操作，就进行一次TCP连接，连接结束TCP关闭连接。

长连接：如果HTTP头部带有参数keep-alive，即开启长连接网页完成打开后，底层用于传输数据的TCP连接不会直接关闭，会根据服务器设置的保持时间保持连接，保持时间过后连接关闭。

## 简述http2.0的改进

提出多路复用。多路复用前，文件时串行传输的，请求a文件，b文件只能等待，并且连接数过多。引入多路复用，a文件b文件可以同时传输。

引入了二进制数据帧。其中帧对数据进行顺序标识，有了序列id，服务器就可以进行并行传输数据。

## http与https的区别

http所有传输的内容都是明文，并且客户端和服务器端都无法验证对方的身份。https具有安全性的ssl加密传输协议，加密采用对称加密， https协议需要到ca申请证书，一般免费证书很少，需要交费。

## 简述TLS/SSL, HTTP, HTTPS的关系

SSL全称为Secure Sockets Layer即安全套接层，其继任为TLSTransport Layer Security传输层安全协议，均用于在传输层为数据通讯提供安全支持。

可以将HTTPS协议简单理解为HTTP协议＋TLS/SSL

## https的连接过程

1.  浏览器将支持的加密算法信息发给服务器
    
2.  服务器选择一套浏览器支持的加密算法，以证书的形式回发给浏览器
    
3.  客户端(SSL/TLS)解析证书验证证书合法性，生成对称加密的密钥，我们将该密钥称之为client key，即客户端密钥，用服务器的公钥对客户端密钥进行非对称加密。
    
4.  客户端会发起HTTPS中的第二个HTTP请求，将加密之后的客户端对称密钥发送给服务器
    
5.  服务器接收到客户端发来的密文之后，会用自己的私钥对其进行非对称解密，解密之后的明文就是客户端密钥，然后用客户端密钥对数据进行对称加密，这样数据就变成了密文。
    
6.  服务器将加密后的密文发送给客户端
    
7.  客户端收到服务器发送来的密文，用客户端密钥对其进行对称解密，得到服务器发送的数据。这样HTTPS中的第二个HTTP请求结束，整个HTTPS传输完成
    

## Get与Post区别

Get：指定资源请求数据，刷新无害，Get请求的数据会附加到URL中，传输数据的大小受到url的限制。

Post：向指定资源提交要被处理的数据。刷新会使数据会被重复提交。post在发送数据前会先将请求头发送给服务器进行确认，然后才真正发送数据。

## Get方法参数有大小限制吗

一般HTTP协议里并不限制参数大小限制。但一般由于get请求是直接附加到地址栏里面的，由于浏览器地址栏有长度限制，因此使GET请求在浏览器实现层面上看会有长度限制。

## 了解REST API吗

REST API全称为表述性状态转移（Representational State Transfer，REST）即利用HTTP中get、post、put、delete以及其他的HTTP方法构成REST中数据资源的增删改查操作：

-   Create ：POST
    
-   Read ：GET
    
-   Update ：PUT/PATCH
    
-   Delete：DELETE
    

## 浏览器中输入一个网址后，具体发生了什么

1.  进行DNS解析操作，根据DNS解析的结果查到服务器IP地址
    
2.  通过ip寻址和arp，找到服务器，并利用三次握手建立TCP连接
    
3.  浏览器生成HTTP报文，发送HTTP请求，等待服务器响应
    
4.  服务器处理请求，并返回给浏览器
    
5.  根据HTTP是否开启长连接，进行TCP的挥手过程
    
6.  浏览器根据收到的静态资源进行页面渲染