# 进程和线程
>[从操作系统的角度来看，什么是线程与进程(超级详细) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/452969421)
>[万字长文带你还原进程和线程 - 程序员cxuan - 博客园 (cnblogs.com)](https://www.cnblogs.com/cxuanBlog/p/12302848.html)


进程的切换，实质上就是被中断运行进程与待运行进程的上下文切换。从主观上来理解。只分为两步：  
**1.切换新的页表(把虚拟地址空间对应的部分，映射到物理地址上**)，然后使用新的虚拟地址空间  
2.切换内核栈，加入新的内容(PCB控制块，资源相关)，硬件上下文切换(CPU 寄存器和程序计数器的内容)

因为线程共享进程的虚拟地址空间，所以切换的时候没有第一步的过程((TCB))
>进程调用系统调用，陷入内核，在内核中执行代码也需要调用函数，这时函数调用所用到的栈就是内核栈

进程上下文的切换会扰乱处理器的缓存机制。简单的说，一旦去切换上下文，处理器中所有已经缓存的内存地址和TLB一瞬间都作废了。

进程调度，切换进程上下文，包括分配的内存，数据段，堆栈段等  
线程调度，切换线程上下文，主要切换堆栈，以及各寄存器（同个进程里的线程 堆栈不同）

协程，（轻量级线程） 每个协程都自带一个栈，协程就是一个函数和这个函数运行时数据的栈


历史：
最开始，没有进程和线程的概念，一个程序就是一个任务，CPU按照顺序执行用户输入的任务。
出现的问题就是：程序是顺序执行的，一个任务正在执行的过程中，另一个任务必须等待。
这就导致了两个问题：
	1. 如果存在某些紧急任务需要执行，就必须等待前面的任务执行，
    2. 当前执行的任务可能在某些环节，CPU并不参与计算（如等待IO设备就绪），但是CPU仍被该任务占用。

为了解决这两类问题，引入了进程，引入进程后的改变：
	原来的程序是静态的，而且一次只能执行一个程序，而且不能被中断，也不能被抢占。一个任务只有两种状态：没执行，执行完了。
	而现在引入进程的主要作用是什么——**引入了一个新的状态“正在执行的程序”**。有了这个状态，我们可以指明哪个程序在执行的过程中遇到了中断，哪些程序执行到一半CPU被抢占了。说白了，引入进程本质上是为了保存程序运行途中的上下文。
	“上下文”是一个比较抽象的概念，它指的是一组软件或硬件的状态，比如当前某个寄存器保存的值、当前程序计数器指向了那个指令等，例如游戏打到一半存个档，当前的血槽、当前的金币数目都是上下文的一部分。上下文可以直接看作一组寄存器的状态和一组变量的值。（或者理解为硬件软件状态在某一刻的快照）
	能够保存存档，我就能玩玩其他存档了，于是你可以“并发”玩多个存档
	狭义上的并发就是这样产生的，**并发的基础就是上下文能够被保存与恢复。** 我们可以将进程想象为一长串**顺序**的指令流，当指令流执行完毕后，进程就退出来了，那么我们可以将这一串指令流来切开，即将这一长串指令流切为一片一片的指令片，只要CPU按照顺序执行这些指令片，也能得到和一直执行进程一样的结果。

引入进程后，由于进程切换代价大，我们由引入了线程，为什么：
	进程切换知识点
	引入线程之后，本质上是为了实现多线程，减少进程的切换。如果默认创建一个进程，那么它仅有一个执行流，我们可以称之为主线程。之前谈论的都可以看作单线程进程，进程之间的切换其实本质上是两个线程的跨进程切换。
	如果一个程序内部想要实现并发，那么程序内部能够包含多个执行流，这个执行流就是线程。如果将多个单线程进程的程序改造成多线程单进程的程序，那么执行流之间的切换，时间成本将大大降低，CPU就可以更多的执行程序本身的代码，而不是执行流切换的内核代码。

总结：线程是对程序执行流的抽象，引入线程是为了实现程序内执行流之间的并发，进而减少执行流切换的代价，同时进一步提升CPU利用率。进程侧重程序之间的并发，而线程侧重于程序内部的并发


之前说过，不管进程切换还是线程切换，本质上切换的是任务执行流。进程至少包含一个执行流，同时它管理了各种资源如内存地址空间、文件描述符等程序所拥有的资源。这个执行流就是一个线程，而线程的实现主要分为系统线程和用户线程。
系统线程之间由操作系统管理(os能够“看见”它，因为os转为为它定义了特定的结构体)。而os“看不见”用户线程。
系统线程的切换、销毁、创建等全部由os负责，但是如果系统线程过多，对操作系统来说也是不小的压力，因为系统线程的保存占用系统内存，**系统线程的调度也是需要经过“陷入内核”的 ，这个“陷入内核”将导致上下文切换，使得一部分时间不得不用来执行内核的代码。**
为了解决上下文切换这个问题，引入了协程：
协程的引用，其实就是通过线程的**分时复用**实现基于一对多或多对多的线程模型。
其中一个系统线程可以与多个用户线程产生映射，用户线程的切换在用户态进行，不需要上下文切换，CPU利用率进一步提高。
>再次使用上面的例子，CPU使用的会议软件有多个对话框， 每个框是一个系统线程，CPU可以看见每一个框。引入协程，一个框不再是一个用户了，而是若干个用户，每个用户都是一个用户线程。（这下子，CPU切换框的几秒钟摸鱼时间都没了，CPU快被榨干了！）
这也存在一个问题，当其中一个用户线程（协程）发起系统调用或其他阻塞操作，系统线程对应的一组用户线程（协程）都将被阻塞。因此协程一般不进行阻塞调用，一般将协程和异步IO结合使用。（框里面一个用户办点事，然后CPU开始框中服务另一个用户，前一个用户办完事后主动通知CPU我办完了，咱们继续吧）
黑皮书里提了一个解决方案：如果某个协程进行阻塞调用，就拿到一个新的系统线程，然后将其他任务迁移过去，调用返回后读取结果

总之：常规的线程通常指的是操作系统可以直接控制的系统线程，而协程是一种用户线程，一个线程与多个协程可以通过分时复用进行映射，协程的引入通过减少线程切换的次数来进一步提升CPU利用率。
但是，协程毕竟是用户级层面的，而系统线程委托给操作系统管理，操作系统直接管理CPU硬件，它将把线程映射到多核CPU的每一个核心上，实现线程（执行流）的并行执行。而协程则无法利用这一点，因为操作系统只能看见与协程关联的系统进程而已


## 区别
我们聊一聊进程与线程的区别，进程是对运行中程序的抽象，而线程是对程序执行流的抽象。进程的引入是为了保存运行到一半的程序的状态，转而可以执行别的程序，以此来实现程序之间的并发。在任务调度和执行方法，一个进程可以看作是一条执行流。而线程的引入同理，也是为了保存运行到一半的某条执行流，转而可以执行另外的一条执行流，以此来实现执行流之间的并行。
其实线程可以粗略地看作进程的一个子集，广义上的进程可以看作内存空间、CPU、文件描述符等资源的分配的基本单位。而狭义上的进程可以看作一个仅有一个主线程的工作单元，因此线程也称为轻量级线程。引入线程概念之前，进程即使资源分配的单位，又是程序调度和执行的基本单位。而引入线程后，“执行流”这个抽象的东西就被概念化为了“线程”，因此线程称为了描述任务执行的基本单位。
>一个程序如果能够被执行下去，那么地址空间必不可少，程序中可能涉及打开文件、申请变量等操作，而且程序想要执行也必须占用CPU这个资源。如何理解资源分配的基本单位？内存条这个硬件被操作系统管理，操作系统将内存地址分配出去是以进程为单位分配的，但是一般分配的都是逻辑地址（虚拟地址），当对应空间需要被访问时才会映射为物理地址。而CPU也是一次分配给一个进程的，CPU的作用可以粗略理解为：一行行扫描代码，然后转换为CPU指令集中的汇编指令，产生结果…执行一个范围内的代码就是一个执行流，因此线程是程序执行的基本单位。


## 资源共享
进程和线程在操作系统中都对应一个具体的结构去保存各自的上下文。  
进程中至少有一个线程（执行流），如果忽略这个主线程，那么进程仍需要保存各种和资源有关的信息，如各种**全局变量、申请到的文件描述符、静态变量、局部变量、内存地址空间（包括堆内存、栈内存、数据段、代码段和一些共享内存）**，而这些资源也都是被线程共享的。  
而线程的上下文主要是和执行有关的。如：**两个堆栈指针（stack point）、程序计数器（PC）、状态寄存字（PSW）、通用寄存器**。也包含了一些状态信息如线程优先级、pid等，以上这些线程上下文都是线程私有的。

## 系统态与用户态
计算机操作系统运行着两类程序：用户程序与系统程序。这两个程序没什么区别，编译都是一对汇编语言，CPU去执行。但是为了防止应用程序破坏操作系统，两类程序会分别运行在不同的状态，而CPU通过程序状态字PSW来实现状态的切换。  
现代操作系统将CPU指令集划分为特权指令和非特权指令，其中非特权指令只能完成有限的任务，如访问自己的内存、操作数入栈出栈、逻辑运算等。一旦需要使用到特权指令，如获打开一个文件、创建一个进程、访问内核空间等就必须采用**系统调用**，委托操作系统来完成。  
一旦使用了系统调用，就会产生陷入内核，并将CPU上下文保存在内核栈，然后转而执行相应在指令（在当前用户的系统栈）。（不发生线程切换）。线程切换的过程中，发生陷入内核，其实就是使用了系统调用，委托操作系统执行线程调度程序（在被切换线程的内核栈执行操作系统内核提供的代码）

> 用户进程的代码保存在用户空间的代码段，而内核代码则保存在内核空间，每个进程/线程（这里指的都是CPU执行流）都拥有两个栈指针，内核栈和用户栈，在用户栈中执行用户程序的普通调用，而在内核栈执行的是系统调用。  
> 如果用户想要创建一个进程，那么应该使用系统调用fork委托操作系统创建，操作系统创建完毕后返回用户一个子进程的标识符，操作系统创建这个子进程的过程，进程/线程是被“中断”的，当它恢复后便得到了这个标识符。因此系统调用导致陷入内核保存的上下文——即保存在内核栈的信息也可以称作**中断上下文**。

进程、线程、中断上下文本质上都属于CPU上下文，粒度依次降低

用户态与系统态进行切换，会保存CPU上下文，如果没有发生线程切换，那么这个上下文指的就是中断上下文。如果是进程内线程的切换那么这个上下文是线程上下文，如果是进程间的线程切换，那么这个上下文是进程上下文。粒度越大，上下文包含的内容越多，


## 系统栈与用户栈
操作系统为每个进程都分配了栈空间（划定一片空间，线程可以在其中进一步划分自己的领域），而每个线程都拥有独立的栈内存，而每当一个函数（方法）被调用，栈空间中就会创建一个栈帧，如果方法嵌套调用层数太多，可能会因为超过进程所分配的栈空间而导致“栈溢出错误”。  
栈帧中保存了传递给函数的参数、返回值局部变量、程序计数器副本以及一些函数的元信息。

用户栈的作用前面提到了，即负责用户程序的调用执行。而系统栈除了负责系统调用的执行，也负责保存CPU上下文。  
这里的CPU上下文主要就是几个之前提到过的寄存器的值：PC/PSW/通用寄存器/堆栈指针

> 用户态下的CPU、用户态下的进程/线程指的是：CPU正在执行的某一个进程的代码，同时CPU的PSW指向用户态。且用户态下的CPU在用户栈中执行指令

用户态转换为内核态的三种方式：系统调用、函数调用出现异常、外围设备的中断信号。其中只有系统调用是进程主动要求中断（说具体点，CPU执行到了涉及系统调用的代码）

> 内核栈不止一个，但是也并不是每个用户进程都各指向一个，具体个数与具体操作系统有关


## 上下文切换
因为线程共享进程的一切资源，最主要是内存地址空间。使得线程切换时不会发生页表的切换，仅仅需要保存与加载线程上下文即可。

> CPU是执行代码用的，CPU在任意时刻在某一进程——这里的意思是：读取一个进程上下文的数据后，就开始在为该进程分配的内存空间中执行计算，如果需要创建对象，就在进程的堆内存中创建，如果需要执行方法就在进程的栈内存中创建栈帧，进程基本上只是为CPU提供了初始数据、代码、和存放中间数据的内存空间等资源。CPU执行某段代码所产生的上下文可以使用线程上下文来描述——因此进程、线程上下文本质上都是CPU上下文，或者说进程和线程这两个结构就是**用于存储/描述不同粒度的CPU上下文的**。  
> 执行代码产生的临时数据都可以看作CPU上下文，而**CPU执行中断后会把此时的上下文数据保存到进程的内核栈，当程序恢复后会从内核栈去读取线程上下文**。因此当程序正在用户栈执行时，内核栈都是空的。而一个没有持有CPU的**就绪进程/线程**，它的**上下文数据保存在内核栈**。

### 具体的过程

CPU执行A线程中的代码时（其实就是执行一个个方法，用户态），如果A线程发生线程切换，CPU将陷入内核（CPU的程序状态字寄存器修改状态，进入内核态），然后CPU将堆栈指针从用户栈指向内核栈，并将线程上下文保存的内核栈后，开始在内核栈中执行线程切换的内核代码（主要是读取线程B的上下文，当线程切换相关的代码/方法全部执行完毕，相应的栈帧全部出栈，此时内核栈就剩下一个上下文数据了），切换程序执行文本后，此时CPU上下文已经被载入新的上下文数据（读取线程B上下文，栈帧出栈，然后堆栈指针指向线程B的用户栈，同时将PSW拨回用户态），因此CPU开始执行另一些代码了。

### 线程切换代价对比

进程切换和线程切换都需要涉及一次CPU上下文（中断上下文）的切换，CPU的堆栈指针会指向内核栈，然后将包括程序计数器、通用寄存器、程序状态字等硬件上下文保存在内核栈中。  
进程切换还需要额外做的就是切换虚拟地址空间，通过切换页表指针来实现，这个过程需要访问内存，重新读取页表的地址。  
同时，页表切换意味着CPU的高速缓存要刷新，主要是页表条目的高速缓存TLB会被刷新（清空），CPU进行地址转换时必须通过访问内存。

> CPU在一个时钟周期能够执行多条指令，而访问一次内存耗费的时间占用多个时钟周期，这使得CPU利用率下降。而现代内存地址映射一般采用多级页表，从内存中加载页表项的代价相对会更大。






## 通信
[[OS] Linux进程、线程通信方式总结 - Strawberry丶 - 博客园 (cnblogs.com)](https://www.cnblogs.com/lca1826/p/6810532.html)


进程通信和线程通信**本质上都是程序之间的两个执行流的通信**。  
通信无处不在，通过cmd窗口执行命令、连接数据库、使用浏览器都是通信。

进程通信核心关注点，是将两个内存地址独立的进程可以互相发送消息。  
大致的思路：使用一个中间的载体，一个进程放入一个进程取，或者进程能够通过端口+ip地址定位到另一个进程，约定某一种格式的信息，像写信一样交流、还有一种思路是内存映射。

由于同一进程下线程是共享内存空间的，也就是说“线程没有自己的内存空间”或者“线程可以访问进程的地址空间”，亦或者“线程可以轻易地访问对方的地址空间”。  
因此线程通信非常简单，它们天生共享内存。因此线程通信的关注点在于如何“安全地”访问内存——线程安全问题。

进程如果想要达成线程的效果，需要使用共享内存映射或者共享内存文件映射。  
因此，**线程关注点在于“安全访问”，而进程关注点在于“信息传递”。**



共享内存模型：共享内存，消息队列
消息传递模型：管道，信号，还有就是Socket，用于不同主机之间的消息传递

匿名管道，有名管道，信号量数组，共享内存，消息队列和socket

> 这里简单说一下通信方式。进程之间可以
> 【1】采用管道通信，属于内核中的缓存（是否阻塞取决于具体实现和策略），但是通信效率低下（访问需要涉及系统调用，而且一般是阻塞调用），不适合频繁交换数据  ，但是简单，一般执行简单的指令可以使用  
> 【2】消息队列，进程以格式化的消息（类似报文）为单位，将数据封装到消息中，并且通过OS的原语进行消息传递。  
> 通信不及时、而且消息体的大小有限制，通信的过程中需要把消息在内核缓存和用户缓存中进行数据拷贝。因此也不适合大数据传输以及频繁的通信  
> 【3】内存映射  
> 进程可以通过内存映射实现共享内存，达到和线程类似的效果（指内存），之后应用程序（CPU）可以像访问普通内存一样访问共享内存（不用切换PSW）  
> 【4】线程和基于共享内存通信的进程都具有“访问安全问题”。因此需要使用锁或者信号量进行解决不安全问题。  
> 操作系统层面可以使用P/V原语实现进程的同步或互斥。一些编程语言提供了锁和信号量的API可以直接使用。（这里同步与互斥的对象都是执行流，没必要区分太开）  
> 【5】信号是一种异步的通信机制，如Ctrl+C和kill命令。  
> 【6】同主机或跨主机进程还可以使用socket通信。  
> 【7】锁。锁本质上是一个标志，是否已经上锁？owner是谁？锁也可以分为共享锁、互斥锁、自旋锁、无锁等。上锁（标志被置位）一般是基于CAS指令实现的（关中断、testAndSet也可以实现）。  
> **有锁**：资源访问前先CAS修改锁变量。  
> **无锁**：CAS直接修改资源，如果修改失败则认为竞争失败，如何重试取决具体逻辑  
> 线程层面，锁一般是**进程内存地址中的一个变量**，而进程层面（跨进程线程），锁一般是两个进程**共同指向的一个打开的文件**  
> 【8】文件映射。是共享内存的实现方式之一。先将磁盘物理块部分或者全部映射到内存物理块上，然后将这个内存物理块映射到若干个进程的内存地址空间（虚拟内存页）上。（内存映射后，内存物理块保存的二进制内存最终会保存到磁盘的物理块上）  
> 【9】文件。如果说线程共享是进程内存地址下某个共享变量，那么进程就可以共享共同指向的某个文件。

```markdown
1. 管道通信：管道是一种两个进程间进行**单向通信**的机制，因为管道传递数据的单向性，管道又称之为半双工管道

匿名管道特点：

1）数据只能由一个进程流向另一个进程，如果要进行双工通信，则需要建立两个管道（一个读管道，一个写管道）

2）匿名管道只能用于父子进程或者兄弟进程间的通信，有名管道则不受这个限制

3）管道缓冲区的大小是有限制的

4）管道传输的是无格式的字节流，需要传输双方事先约定传输格式

5）管道也是一种文件，但这个文件只存在于内存中

值得注意的是，**匿名管道是存在于内核中的，而有名管道则是以FIFO的文件形式存在于文件系统中**，这样即使是不具有亲缘关系的进程，只有可以访问文件系统，都可以进行通信


有名管道的特点：

1）可以使互不相关的两个进程实现彼此通信

2）该命名管道可以通过路径名来指出，并且在文件系统中是可见的，在建立了有名管道之后，两个进程就可以把它当作普通文件一样进行读写操作，使用方便

3）数据严格遵循FIFO先进先出原则


2. 消息队列：消息队列是有消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。


和命名管道相比，消息队列的优势在于：

1）消息队列也可以独立于发送和接收进程而存在，从而消除了在同步命名管道的打开和关闭时可能产生的困难

2）可以同时发通过发送消息来避免命名管道的同步和阻塞问题，而不需要进程自己提供同步方法

3)接收程序可以通过消息类型有选择的接收数据，而不是像命名管道那样只能默认接收


3. **共享内存**
定义：允许两个不相关的进程访问同一个逻辑内存

1.共享内存是两个正在运行的进程之间传递和共享数据的一种非常有效的方式

2.不同进程之间共享的内存通常安排在同一段物理内存中

3.进程可以将同一段共享内存链接到自己的地址空间中

4.共享内存没有提供同步机制！所以通过需要其他的机制来进行同步

4. 信号量：信号量是一个计数器，可以用来控制多个进程对共享资源的访问

共享内存是进程间最快的通信方式，但是共享内存的同步问题共享内存无法解决，可以采用信号量解决共享内存的同步问题

**在进程访问临界资源之前，需要测试信号量，如果为正数，则信号量-1并且进程可以进入临界区，若为非正数，则进程挂起放入等待队列，直至有进程退出临界区，释放资源并+1信号量，此时唤醒等待队列的进程。**

样例程序：信号量+共享内存，共享内存解决进程间的通信，而信号量解决共享内存的同步问题

5. 消息队列
消息队列其实就是内核中维护的一个队列，提供了一个从一个进程向另一个进程发送一块（有类型）数据的方法；

每个数据块都被认为只有一个类型，接收者进程接收数据的数据块可以由不同的类型值；

特性：每个数据块都由一个特定的类型，接收方可以根据这些类型来有选择地加收数据；
```
![[Pasted image 20220323202807.png]]



### 进程隔离

程隔离是为保护操作系统中进程互不干扰而设计的一组不同硬件和软件的技术。这个技术是为了避免进程A写入进程B的情况发生。 进程的隔离实现，使用了虚拟地址空间。进程A的虚拟地址和进程B的虚拟地址不同，这样就防止进程A将数据信息写入进程B。

**虚拟地址空间**

每个进程只能访问自己虚拟地址空间中的数据，无法访问别的进程中的数据，通过这种方法实现了进程间的地址隔离。

针对 Linux 操作系统，将最高的1G字节（从虚拟地址 0xC0000000 到 0xFFFFFFFF ）供内核使用，称为**内核空间**，而较低的 3G 字节（从虚拟地址 0x00000000 到0xBFFFFFFF），供各个进程使用，称为**用户空间**。每个进程都可以通过系统调用进入到内核。其中在 Linux 系统中，**进程的用户空间是独立的，而内核空间是共有的，进程切换时，用户空间切换，内核空间不变**。

**创建虚拟地址空间目的是为了解决进程地址空间隔离的问题。但程序要想执行，必须运行在真实的内存上，所以，必须在虚拟地址与物理地址间建立一种映射关系**。这样，通过映射机制，当程序访问虚拟地址空间上的某个地址值时，就相当于访问了物理地址空间中的另一个值。人们想到了一种分段、分页的方法，它的思想是在虚拟地址空间和物理地址空间之间做一一**映射**。这种思想理解起来并不难，操作系统保证不同进程的地址空间被映射到物理地址空间中不同的区域上，这样每个进程最终访问到的物理地址空间都是彼此分开的。通过这种方式，就实现了进程间的地址隔离。

### IPC通信原理
每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核,在内核中开辟一块缓冲区,进程1把数据从用户空间拷到内核缓冲区,进程2再从内核缓冲区把数据读走,内核提供的这种机制称为进程间通信机制。通常的做法是消息发送方将要发送的数据存放在内存缓存区中，通过系统调用进入内核态。然后内核程序在内核空间分配内存，开辟一块内核缓存区，内核空间调用 _**copy_from_user()**_ 函数将数据从用户空间的内存缓存区拷贝到内核空间的内核缓存区中。同样的，接收方进程在接收数据时在自己的用户空间开辟一块内存缓存区，然后内核程序调用 _**copy_to_user()**_ 函数将数据从内核缓存区拷贝到接收进程的用户空间内存缓存区。这样数据发送方进程和数据接收方进程就完成了一次数据传输，我们称完成了一次进程间通信

[(66条消息) 【操作系统】进程间通信_Ant_Davis的博客-CSDN博客](https://blog.csdn.net/flyersboy/article/details/117574342?spm=1001.2014.3001.5502)

#### 管道
无名管道：

特点：
1. 它是半双工的（即数据只能在一个方向上流动），具有固定的读端和写端。

2. 它只能用于具有亲缘关系的进程之间的通信（也是父子进程或者兄弟进程之间）。

3. 它可以看成是一种特殊的文件，对于它的读写也可以使用普通的read、write 等函数。但是它不是普通的文件，并不属于其他任何文件系统，并且只存在于内存中。


当一个管道建立时，调用pipe函数 在内核中开辟一块缓冲区用于通信，它会创建两个文件描述符：`fd[0]`为读而打开，`fd[1]`为写而打开

要关闭管道只需将这两个文件描述符关闭即可。

单个进程中的管道几乎没有任何用处。所以，通常调用 pipe 的进程接着调用 fork，这样就创建了父进程与子进程之间的 IPC 通道

若要数据流从父进程流向子进程，则关闭父进程的读端（`fd[0]`）与子进程的写端（`fd[1]`）；反之，则可以使数据流从子进程流向父进程。


#### 命名管道
1.  FIFO可以在无关的进程之间交换数据，与无名管道不同。

2.  FIFO有路径名与之相关联，它以一种特殊设备文件形式存在于文件系统中。


FIFO的通信方式类似于在进程中使用文件来传输数据，只不过FIFO类型文件同时具有管道的特性。在数据读出时，FIFO管道中同时清除数据，并且“先进先出”


#### 消息队列
消息队列，顾名思义，想必看到的公共资源可能就是一种队列。这种队列满足数据结构里队列的特点：先进先出。消息队列提供了一个进程向另一个进程发送一块数据快的通信方法，注意，**是以块为基本单位，前面的管道是以字节流为基本单位**。**每个数据块都被认为是有类型的。接收者进程接受数据块可以有不同的类型值**，比如可以是结构体型。每个消息 队列的最大长度是上限的(MSGMAX)，每个消息队列的总的字节数也是有上限的(MSGMNB),系统的消息队列总数也是有上线的(MSGMNI)，

一个消息队列由一个标识符（即队列ID）来标识。每个消息队列都有一个队列头，用结构struct msg_queue来描述。队列头中包含了该消息队列的大量信息，包括消息队列键值、用户ID、组ID、消息队列中消息数目等等，甚至记录了最近对消息队列读写进程的ID。读者可以访问这些信息，也可以设置其中的某些信息。

特点：
 1. 消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识.
 2. 消息队列允许一个或多个进程向它写入与读取消息.
 3. 管道和命名管道都是通信数据都是先进先出的原则。
 4. 消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比FIFO更有优势。

消息队列通信过程中，存在⽤户态与内核态之间的数据拷⻉开销，因为进程写⼊数据到内核中的消息队列时，会发⽣从⽤户态拷⻉数据到内核态的过程，同理另⼀进程读取内核中的消息数据时，会发⽣从内核态拷⻉数据到⽤户态的过程。


#### 信号量
**信号量（semaphore）与已经介绍过的 IPC 结构不同，它是一个计数器。信号量用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。**

特点：
1.  信号量用于进程间同步，若要在进程间传递数据需要结合共享内存。
 
2.  信号量基于操作系统的 PV 操作，程序对信号量的操作都是原子操作。
 
3.  每次对信号量的 PV 操作不仅限于对信号量值加 1 或减 1，而且可以加减任意正整数。
 
4.  支持信号量组。



#### 共享内存
**共享内存（Shared Memory）**，指两个或多个进程共享一个给定的存储区。

特点：
1.  共享内存是最快的一种 IPC，因为进程是直接对内存进行存取。
 
2.  因为多个进程可以同时操作，所以需要进行同步。
 
3.  信号量+共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。

当一段共享内存被创建以后，它并不能被任何进程访问。必须使用shmat函数连接该共享内存到当前进程的地址空间，连接成功后把共享内存区对象映射到调用进程的地址空间，随后可像本地空间一样访问。

shmdt函数是用来断开shmat建立的连接的。注意，这并不是从系统中删除该共享内存，只是当前进程不能再访问该共享内存而已

#### 信号
**信号**是一种事件通知机制，当接收到该信号的进程会执行相应的操作。



特点：
1.  由硬件产生，如从键盘输入Ctrl+C可以终止当前进程
2.  由其他进程发送，例如，在shell进程下，使用命令kill  -信号值 PID
3.  异常，当进程异常时发送信号



#### 套接字
**socket**，即套接字是一种通信机制，凭借这种机制，客户/服务器（即要进行通信的进程）系统的开发工作既可以在本地单机上进行，也可以跨网络进行。也就是说它可以让不在同一台计算机但通过网络连接计算机上的进程进行通信。也因为这样，套接字明确地将客户端和服务器区分开来。


因特网提供了两种通信机制：流（stream）和数据报（datagram），因而套接字的类型也就分为流套接字和数据报套接字。这里主要讲流套接字。

流套接字由类型SOCK_STREAM指定，它们是在AF_INET域中通过TCP/IP连接实现，同时也是AF_UNIX中常用的套接字类型。流套接字提供的是一个有序、可靠、双向字节流的连接，因此发送的数据可以确保不会丢失、重复或乱序到达，而且它还有一定的出错后重新发送的机制。

与流套接字相对的是由类型SOCK_DGRAM指定的数据报套接字，它不需要建立连接和维持一个连接，它们在AF_INET中通常是通过UDP/IP协议实现的。它对可以发送的数据的长度有限制，数据报作为一个单独的网络消息被传输,它可能会丢失、复制或错乱到达，UDP不是一个可靠的协议，但是它的速度比较高，因为它并一需要总是要建立和维持一个连接。

## 同步
进程：临界区，信号量，事件对象

同步的基础是通信，从通信中找到能设置状态变化并且其他进程能感知到
只要都共享的读取某个东西，就可以通过修改这个东西的状态实现同步

为禁止两个进程同时进入临界区，同步机制应遵循以下准则：
1. 空闲让进
2. 忙则等待
3. 有限等待
4. 让权等待

## 多进程与多线程，协程
多进程并发程序设计性能不一定差，但是通信和切换都需要借助系统调用，实现代价大，而且并发粒度有限。  
其中，每个进程是相互独立的，每个进程都代表一个独立的程序，其中一个进程崩溃并不会影响主进程的正常运行，而且各进程可以分别运行在不同的主机上，实现分布式部署。线程（这里指用户级别的线程）的崩溃可能影响到整个程序（进程）的稳定性，如果发起系统调用，可能会导致当前进程陷入阻塞。

> 操作系统可以控制进程，那么一个进程中肯定存在一个系统级别的执行流（线程），受到影响的一定是用户可以看见，且“使用了用户线程API被阻塞”的系统线程，虽然是用户线程出问题，但是操作系统只看到了“调用被阻塞”的某个系统线程/执行流。

另一方面，同一时刻，一个CPU总是正在执行一个进程的代码。因此如果想要提升性能，那么可以通过扩充CPU的数量来直接提升性能。而线程（这里指系统级别的线程）的并行度可以通过增加CPU和核心数量来提升。

与多进程相比，线程能够提升的总性能总是收到当前进程的限制，因为它的内存地址空间总是不能高于进程申请的总地址空间，而且并行度总是受限于某个CPU的核心数量

总结：多线程程序天生共享内存，通信方便。创建、切换、销毁代价更小。但是性能提升有瓶颈。（是否稳定需要具体分析，和具体的线程模型、线程的实现等有关）。  
多进程粒度大、代价大之外，但是性能扩展比较容易实现，而且具有稳定性（健壮性）


那和多线程比，协程最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。

第二大优势就是**不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突**，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。

### 线程共享进程哪些资源
**线程共享的环境包括：**进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。

**进程拥有这许多共性的同时，还拥有自己的个性。有了这些个性，线程才能实现并发性。这些个性包括：**  
1.线程ID  
每个线程都有自己的线程ID，这个ID在本进程中是唯一的。进程用此来标识线程。  
2.寄存器组的值  
由于线程间是并发运行的，每个线程有自己不同的运行线索，当从一个线程切换到另一个线程上时，必须将原有的线程的寄存器集合的状态保存，以便将来该线程在被重新切换到时能得以恢复。  
3.线程的栈  
栈是保证线程独立运行所必须的。线程函数可以调用函数，而被调用函数中又是可以层层嵌套的，所以线程必须拥有自己的函数堆栈，使得函数调用可以正常执行，不受其他线程的影响。  
4.错误返回码  
由于同一个进程中有很多个线程在同时运行，可能某个线程进行系统调用后设置了errno值，而在该线程还没有处理这个错误，另外一个线程就在此时被调度器投入运行，这样错误值就有可能被修改。所以，不同的线程应该拥有自己的错误返回码变量。  
5.线程的信号屏蔽码  
由于每个线程所感兴趣的信号不同，所以线程的信号屏蔽码应该由线程自己管理。但所有的线程都 共享同样的信号处理器。  
6.线程的优先级  
由于线程需要像进程那样能够被调度，那么就必须要有可供调度使用的参数，这个参数就是线程的优先级。


```markdown
**1.进程和线程分别是什么？**
说道这个问题，我们需要首先说下操作系统，操作系统将硬件管理起来，提供统一管理接口供用户调用。操作系统提供一个平台，进程则在平台上完成相应的操作(比如听音乐，看电影，编辑word文档等等)。在现代的操作系统中，操作系统支持多进程，即现代操作系统支持多个进程*同时*运行。进程提供了运行的环境，具体的逻辑任务由线程来完成。

我们来系统总结下：**_进程提供一个各种资源的容器，定义了一个地址空间作为基本的执行环境；线程是一个指令执行序列，可以直接访问进程中的资源。每个进程中至少有一个线程，线程在任一时刻必属于某个进程。_**


**2.前面说现代操作系统支持多个进程同时运行，那么是如何实现同时运行呢？**

这里的同时运行，需要区分CPU多核和单核的情况。

在单核的情况下，因为只有一个CPU，所以只能是在某一个时候只有一个进程在运行，操作系统采用了“分时”的办法。

首先，我们可以将进程想象为一长串**顺序**的指令流，当指令流执行完毕后，进程就退出来了，那么我们可以将这一串指令流来切开，即将这一长串指令流切为一片一片的指令片，只要CPU按照顺序执行这些指令片，也能得到和一直执行进程的结果。

当有多个进程的时候，我们将每个进程都切为对应的片，每隔一定时间来执行某个进程的片，之后在切换到另一个进程的片，这样就达到了“同时运行”的效果。只要时间片是很短暂的，小于人的感知时间(大约100ms到200ms)，就可以让人感觉不到进程在切换。

多核的情况和单核的情况相似，也是采用“分时”的方法。只不过是多核有多个处理器。这里可以把每个核想象为一个个单核就可以了。

到这里，大家就知道了如何达到多进程并行的效果，那么这**需要操作系统提供什么呢？**

很显然，操作系统要想完成分时的效果，需要做如下的事情：
> 1.  维护一个全局的进程表，记录当前哪些进程正在被执行。      
> 2.  将时间分为适当的片段。  
> 3.  在进程间实施切换，即保存上一个进程的执行环境信息，恢复下一个进程的执行环境。


**3.进程在操作系统中是怎么来的和怎么没的呢？**

在操作系统启动的时候，需要先把进程的运行环境全部创建起来，为进程的运行打好基础。创建一个进程即为进程建立基本的执行环境，然后将其加入到系统的全局进程表中，这样进程就能获得相应的资源来运行。

而进程的退出则是通知操作系统将其由全局进程表中去除，之后销毁此进程所有的资源。一般，系统会有检测的功能，当发现某个进程不正常的时候，操作系统可以将这个进程*杀掉*,从而释放对应的资源。


**4.既然进程就能达到并行的目的，那么为什么还需要线程呢？**

总地说来，是因为线程比进程成本更低。因为创建一个进程需要为其分配各种资源，而线程只需要使用进程事先分配好的即可；进程之间的通信成本也会较高，而线程则是在一个进程的环境下，通信起来会很方便。其实，我们可以把线程看做是一个“轻量级”的进程。


**5.线程的创建和切换是什么样子的？**

线程的创建其实和进程类似，也是建立起来线程的执行环境，比如分配线程所需要的数据结构和调用栈，完成这些数据结构的初始化操作。虽然这些操作比较于进程是很轻量级的，但是频繁创建和销毁也会是不可忽视的消耗。所以当需要频繁创建和销毁线程的时候，可以考虑线程池式的方式来解决消耗过大的问题。

进程中的多个线程执行的时候，是乱序的，即在某个时刻执行哪个线程，是不确定的，所以多线程的问题会比较难以调试。对于线程来说，是有用户态和内核态的区别，当进行线程切换的时候，如果线程是用户态，是需要切换为内核态的，这种切换也是需要耗费资源，不过目前随着硬件的发展，这部分切换的成本正在减少，所以，这部分的开销是可以接受的，大部分情况下，我们可以忽略线程的切换。



**6.线程的调度算法**

在这里我们先需要来了解下如何评价算法。调度算法的准则大约有如下的方面：

> 1.  公平性，在选择下一个运行的线程时，要考虑同等地位的线程必须有相同的机会来获取处理器执行权。  
>     
> 2.  CPU有效利用，即只要有进程和线程在等待，CPU就不能空闲。

对于不同类型的操作系统会对算法有不同的需求，例如：实时操作系统对于响应时间有最低的要求。

调度算法可以分为非抢占式和抢占式，在非抢占式系统中，一个线程一旦被选择在处理器上执行，就一直运行，直到阻塞或自愿放弃或退出。我们可以看到，这类算法如果一个线程陷入长时间的处理中时，系统就无法切换其它的线程，容易造成其它线程的饿死。而抢占式系统中，一个线程被选中后，允许运行的时间长度有最大的限制，当到达这个时间后，就被迫放弃执行权，由系统来决定下一个执行的线程。
```

## 进程和线程思考

### 多线程性能一定由于多进程吗
这里的进程都死单进程单线程，答案是不一定的，若是进程个数等于cpu个数，并且都是cpu密集型的，那么进程不用切换（进程切换开销之大较之线程主要在于页表的切换和TLB的失效），性能并不会比一个进程开多个线程差。

若是涉及到磁盘IO，那么多线程还是有明显优势的





# 内存管理
>[(62条消息) 现代操作系统（三）_前来打酱油的的博客-CSDN博客](https://blog.csdn.net/u011955067/article/details/80332007)

共享内存，就是允许两个不相关的进程访问同一个逻辑内存，然后进行通信

1.数据的共享进程间的数据直接访问内存，是最快的进程通信方式，但并未提供同步机制，通常需要用其他的机制来同步，例如信号量
2.共享内存代码申请的适合，尽量是一页大小4k的倍数申请，否则容易产生内存碎片
这里用一张图来说明：
两个不同的虚拟地址通过页表映射到物理空间的同一区域，它们所指向的这块区域即共享内存。




## 前提的内存知识

内存是什么？说白了就是一长串字节数组。编程的时候难免申请一段内存空间，有了内存空间才能存放数据、存放指令代码。  
早期不存在操作系统提供的存储器抽象，每一个程序直接访问物理内存，即从0到某个上限的地址集合，也称为物理地址空间。

**当一个程序被执行后，如果程序中的指令想要被执行，它必须被从磁盘加载/复制到内存中，因为机器语言不能够将磁盘地址作为参数，只能将内存地址作为参数。**  
程序中的指令最终虚拟机解释为CPU可以看懂的汇编指令，而指针或值类型最终也会被转换为一个具体的地址（符号引用转直接引用），CPU对汇编指令的执行总是从若干个地址中取出数据，计算后生成一个新地址。CPU拿到的地址是一个逻辑地址，在内存条上是找不到对应位置的，所以它必须通过某些硬件（**内存管理单元MMU**）将这个逻辑地址转换内存条上对应的物理地址（**地址的映射**），然后去物理内存地址取出数据。  
MMU通常是CPU芯片的一部分，负责执行将虚拟内存地址映射为物理内存地址，通常包含着用于高速缓存映射对的寄存器TLB。

> 通常说的占用内存资源，指的是这个资源已经加载到内存中去了，它可以对应上一个物理内存地址。而程序编译后，数据仅对应一个逻辑内存地址，运行之前它是存放在磁盘中的（点击运行后，它的一个副本载入内存，并且绑定物理内存地址），占用外存资源、不占用内存资源。  
> 读取一个文件，需要启动IO将它调入内存。关闭时，如果没有进行修改或者是一个只读文件，直接释放内存资源。否则需要启动IO使磁盘中的文件副本同步后是否内存资源。


因此如果需要访问内存中存放的内容，则必须将已有的逻辑内存地址，转换为物理内存地址。一个程序，代码本身的存放需要占用地址空间，数据本身也要占用地址空间（如int i= 4需要占用四字节的地址范围），因此程序执行时总是需要占用一部分内存。

将程序用到的内存区分为逻辑内存和物理内存，防止程序被编译后就完全被限定死“只能在某个物理地址”，这也是**内存复用**的基础。把物理内存直接暴露给进程的两个后果：不利于实现内存复用（虚拟内存）、不安全。而地址空间本质上是对物理内存的一种抽象。

程序经过编译后，一般都对应一个虚拟/逻辑地址，既然都说是虚拟的了，那肯定管够（当然了，也不是随便分配的，必须限定在CPU的寻址范围之内的，如果2位的CPU，它撑死能访问四个内存地址00 01 10 11，分配个1111的内存地址，CPU的寄存器也放不下这个四位的地址值），这些逻辑地址的集合就是逻辑地址空间。而物理地址空间就是实际可用的内存地址空间。程序如果能够运行，最终一定是运行在物理内存上的，逻辑地址可以看作是一种逻辑视图。  
地址空间是一个进程可以用于内存寻址的一套地址集合，每个进程都有一个独立的地址空间。使用 **基址寄存器（start）** 和 **界限寄存器（len）** 可以实现地址空间的抽象，前者存储程序的物理地址，而后者存储程序的长度。通过以上两个寄存器，可以实现程序的**重定位**和**保护**，从而保证了多个应用程序可以同时存在内存，并且不互相影响。

当一个程序运行时，程序的起始物理地址装载到基址寄存器，程序的长度装载到界限寄存器  
每次一个进程访问内存，CPU硬件会在把地址发送到内存总线前，自动把基址值加到进程发出的地址值上；同时，它检查程序提供的地址是否大于等于界限寄存器的值


> 程序源代码**编译**时就进行地址映射，那么程序的位置运行期间被定死了，如果需要变动地址，必须重新编译程序。  
> 地址绑定可以推迟到**加载**阶段，编译后仅仅为更指令生成一个逻辑地址，程序加载到内存时（此时也可以称为进程），一次性为为其分配其所需要的物理地址，然后这个程序便获取了所需的内存资源，可以看作就绪的进程。如果它需要变更地址，那么需要重新加载该程序（为其重新分配物理地址空间）  
> 大多数计算机操作系统将地址绑定推迟到程序**执行**时，程序运行时，并不需要将所有逻辑内存进行物理内存的映射，只有当程序片段被运行时才进行内存映射（边执行边映射），这时实现虚拟内存的基础


## 虚拟地址空间分配算法
**使用链表的存储管理**
链表中每个结点都包含以下域：空闲区或者进程的指示标志、起始地址、长度和指向下一个结点的指针
首次适配算法：存储管理器沿着段链表进行搜索，直到找到一个足够大的空闲区，然后分成一部分供进程使用，另一部分形成新的空闲区。速度很快，尽可能减少搜索链表节点
下次适配算法：类似首次适配算法，不同点在于每次找到合适的空闲区时都记录当前的位置，以便下次寻找空闲区时从上次结束的地方开始搜索，而不用从头开始搜索。性能略低于首次适配算法
最佳适配算法：搜索整个链表，找出能够容纳进程的最小的空闲区。试图找出一个最接近实际需要的空闲区，以最好的匹配请求和可用空闲区。性能比首次适配算法要慢，而且会浪费更多的内存（因为会产生大量无用的小空闲区）。如果进程和空闲区使用不同的链表，则可以按照大小对空闲区链表排序，以便提高最佳适配算法的速度
快速适配算法：他为那些常用大小的空闲区维护单独的链表。寻找一个指定大小的空闲区十分快速，但是和所有按大小排序的方案一样，有一个共同缺点——在一个进程终止或者被换出时，寻找它相邻块并查看是否可合并的过程都是耗时的。如果不进行合并，内存将很快分裂出大量进程无法使用的小空闲区



以上谈论的都是连续内存分配，运行在内存中的多个程序，全部被装入内存中，那么如果一个程序达到无法装入内存呢？另一方面，许多程序并不是所有数据代码都需要被一直使用，这部分将是对内存资源的浪费。即使存在**以进程为单位**进行换入换出的交换技术，也并不能很好解决上面的问题。那么就需要引入虚拟内存技术了，而虚拟内存技术的基础是离散内存分配。


离散的内存分配也是实现虚拟内存的基础。连续内存分配方式最大的不足就是存在外部碎片，而离散内存分配可以解决这个问题。

分段和分页就是离散内存分配的一种方式，不过他们常被组合使用。

> 虚拟内存是一种容量扩充技术，而虚拟地址空间和逻辑地址空间都是一个意思，是经过程序运算/编译器编译得到的地址空间，最终需要转换/映射为物理地址空间


## 分段和分页
分页是什么，一个字符串转换为字符数组，这个数组可以看作一个内存空间，那么每一单元就可以看作一个页面，是一维结构的，而且数组单元没有逻辑可言（java字符是utf-16编码的，那么一个emoji表情就可能占用两个单元）。  
分段是什么，二维数组，arr[0]是数据段，arr[1]是代码段…每一个段都是一个独立的地址空间，可以理解为把一个总的地址空间分为许多小段，各小段都是独立的地址空间。

一维的虚拟内存可以看作一个段，虚拟地址从1到最大地址，每个单元都是地址空间紧密相连。而分段的最大特点是，每个段都是独立的，运行期间可以动态改变（堆栈段和数据段），并且不会影响到其他的段。  
单独看分段，它属于一种**连续内存分配策略**，不存在“缺页中断”，而如果分页内存分配涉及函数调用或对象分配导致的内存空间变化，将会引起缺页中断。  
如果每一个过程/函数都位于一个独立的段，并且起始地址都是从0开始的，那么把单独编译好的过程链接起来的操作就可以大大简化。分段有助于进程之间的数据共享，而分页中的数据共享起始是通过模拟分段而实现的，并且可以对不同的段进行差异化的保护。

**分段的出现，是为了使程序和数据可以被划分为逻辑上独立的内存空间，并且更加容易实现共享与保护。而分页的出现，是为了将一个程序的内存分成若干小份，为每个程序提供虚拟地址空间，装入必要部分而不是一次性装入全部物理内存块，从而支持虚拟内存**（好将多个程序全部装入内存）

> 分段先于分页，分段存在的问题是：外部碎片明显，且段大小不一，大的段中往往有些内存不总被访问却占用物理内存（内部碎片），同时换入换出内存时占用大量IO时间。分页解决了外部碎片和IO对换效率低的缺点，仅仅存在少量的部分碎片。其实分段有很多痛点都源于连续内存分配的特点。因此后面有采用段页式内存分配的OS，但是分页更加主流。

分段和分页都是为了更好的利用内存资源。  
其中分页服务于操作系统的管理需求，将逻辑地址空间划分为大小相同的若干逻辑页面，同时将物理地址空间划分为大小相同且大小也与页面大小相同的若干个物理块。将页面和块编上号，一个进程可以被划分为多个页面，分配内存时将页面与物理块进行地址映射即可  
。  
分段服务于程序员的编程和使用需求，将逻辑地址根据逻辑关系划分若干个大小不等的、逻辑地址连续的逻辑段，而物理地址通常是不连续的，如堆段、栈段、共享段、数据段、代码段等，编译器对源程序进行编译时，会分析程序并自动构造段，加载程序会加载段并为之分配段号。（创建对象就划入堆段，元信息就划入共享段等）

_偏正式的回答：_  
**对内存进行分页和分段，目的都是为了更好的利用内存资源。分页服务于系统，为了更好的管理内存（分配、回收），而分段服务于用户，为了更好的根据开发逻辑关系，去使用内存**。  
现代操作系统通常将一个进程的地址空间分为大小不等、逻辑地址连续的若干逻辑段，然后对逻辑空间再进行分页，将逻辑地址空间分为若干大小相等的页面，而将物理地址空间分为若干大小相等的物理块，其中物理块大小等于页面大小，当程序执行时将用到的页面映射到一个物理块上，实现内存的离散分配。

_对比：_  
分页会造成内部碎片，因为进程地址空间的最后一个页面可能无法被完全利用（用不完），而分段会造成外部碎片，当申请不到足够的连续内存的时候会出现报错（堆内存溢出），（有内存但用不了）。

> 假设堆内存3M，而目前堆中最大还可以连续内存是1M，如果这时申请一个1.5M的对象就会OOM

**分页是信息的物理单位，而分段是信息的逻辑单位。因此分页的大小是收到内存硬件和操作系统软件的因素约束，而分段的对象取决于用户编写的程序。**  
分页是一维的，是系统行为（页号|页内地址）而分段是二维的，是用户行为（段号|段内地址+段的逻辑名字）  
**分段更易实现信息共享和保护，因为一个共享代码区可以涉及多个页面，分页实现相对困难。**



## 页表
**在不使用虚拟内存的情况下，系统直接将虚拟地址送入内存总线，读写操作使用具有相同虚拟地址的物理地址（不存在所谓缺页中断，程序运行时虚拟地址以及完成映射），而使用虚拟内存时，虚拟内存由CPU交给MMU得到内存物理地址，再送入存储器。**

操作系统维护的页面非常多，不可能全部存入寄存器，因此使用一个常驻内存的页表（页面映射表）去管理。

> cpu维护一个保存页表基址的寄存器，而地址映射交给硬件内存管理单元MMU完成，系统为每个进程维护一个页表。每当CPU发出一个虚拟地址，MMU翻译出一个物理地址（物理块号）并且添加到缓存（TLB）中，CPU先去内存中查页表，根据查出的物理块号在根据页内偏移量拼出物理内存地址，再去对应的物理内存访问真实数据

CPU访问读取某个内存中的数据，通常需要先访问内存中的页表，查询页表项中页号对应的物理块号，根据**物理块号+偏移量**得到目标物理地址。然后再次访问内存中的目标物理地址。  
至少需要两次访存。如果涉及多级页表将增加更多访存次数。

> 页表占用过多内存，主要有两种解决办法  
> 【1】分割它：多级页表，时间换空间，将占用连续内存空间的大页表分割成可以离散存储的小页表。但是总的占用空间仍然没变  
> 【2】请求调页策略：部分页表项驻留外存，直到真正用到时再调入内存。  
> 两种方式可以结合使用。

另一种针对页层级增多的解决方案：**倒排页表**。  
【1】线性倒排  
每个物理页框对应一个页表项（页框-页号），虽然节省了大量空间，但是内存装换变得困难，而且每一次内存访问操作都需要执行一次“整个倒排页表的检索”。解决方法仍然是使用TLB  
【2】散列倒排  
使用散列表进行优化，建立一张散列表（虚拟页号-倒排页表索引号即框号），先根据页号（多对一）拿到框号，然后再根据框号拿到页表项对应的页号（一对多）。如果散列表的槽数和物理块数一样多，那么哈希冲突概率将会减小

解决页表项占用内存过多的问题，往往会引入新的问题：如更长的查询时间（使用TLB缓解）、难以实现共享内存

> 页表必须覆盖整个虚拟内存空间，不分级的情况下页表占用大量内存，而如果考虑分级，可以仅仅创建一级页表保证页表覆盖所有内存空间，而二级页表可以在“真正用到时”再进行懒调入相应页表项，同理，页表分级越多，**粒度分的越细占用内存越少**。（每一级页表负责若干域位的转换，页表经过打散，高级页表可以不常驻内存中）

大多数CPU在一个时钟周期内可以执行多条指令，而访问内存通常要占用CPU多个时钟周期，为了弥补速度差，引入**快表TLB**。TLB一般存放在CPU内部的高速缓存寄存器中，CPU寻址时首先会查询快表，查询时间很快，可以忽略不计，如果没有命中才会查询内存中的页表。如果命中则可以得到目标物理地址。快表只包含少量的页表条目，如果慢了会根据某个策略淘汰一些条目。  
（对TLB的管理和失效处理完全由MMU硬件实现）

> 操作系统为每个进程都维护了一个页表，因此一旦切换进程，则页表地址指针（**页表基址寄存器PTBR的内容**）也会跟着切换，TLB会被刷新（其实发生任意CPU上下文切换，从内核栈切回来时就可能被抢占导致进程切换）。CPU将不得不查询内存中的页面，这降低了CPU利用率。  
> **>问进程和线程切换代价对比，TLB刷新的代价很大，这点很重要！**  
> 有的TLB会在条目中包含**地址空间标识符**，会唯一标识一个进程，如果进程切换会导致TLB未命中，不会刷新TLB。

分页太小，可以更好的利用内存，但是每个分页在页表中都对应一个页表项，而页表通常是常驻内存的（占用物理块），因此可用内存减少。而且页表项太多，检索的时间也上升的，还有一方面，页面小了，存储的数据也就少了，一段时间内，换入换出的页面数量也要增加，IO次数增加，效率下降。  
而分页太大，虽然使得页面变小，但是页内碎片会增大。  
页面大小通常为2的幂。


## 理解虚拟内存
虚拟内存，虚拟？什么是虚拟的。理解虚拟之前，先谈谈真实内存。真实内存就是能用到的，说白了就是一个内存条可以反映的内存，其实就是之前一直提到的物理内存空间。  
虚拟内存=主存内存+辅存内存。  
虚拟内存并不是从物理上扩充内存，而是从**逻辑上扩充内存**。它给用户一个假象：我电脑内存很大，可以运行整个GTA5（说不定只是装入了一个菜单程序）。它也给程序（进程）一个假象：整个地址空间很大，而且我独占了，而且是连续的，不用给别的程序让路，可以放心运行程序（其实只有极少部分与物理地址进行映射，另外一部分并没有被装入）。  
说白了，你以为当前电脑中此时同时运行着A B C，其实可能内存中仅包含A 5%、B10%和C 2%的代码和数据。

**虚拟内存的基本思想**：  
每个程序都有自己独立的地址空间，这些空间被分成若干相等的页面，并不是一个程序的所有页面全部装入内存，程序才可以运行。每当有程序的一部分代码需要被使用时（虚拟地址被使用），硬件（MMU）必须执行一个虚拟地址到物理地址的映射。如果虚拟地址对应的页面不在内存（一个页面就对应一个虚拟地址的集合，即虚拟地址空间），则OS必须负责将页面调入内存，并装入一个物理块，并且需要重新执行引发缺页异常的指令。

虚拟内存使得整个地址空间可以使用相对较小的单元（按需），映射到物理内存，而不是为代码段和物理段分别进行重定位。

_**偏正式的回答：**_  
虚拟内存管理让每个进程认为自己独占了整个地址空间，其实这个地址空间是**主存和磁盘地址空间的抽象**，目的是逻辑上扩充内存容量，逻辑容量等于主存容量与磁盘容量之和。同时，让每个进程拥有一致的虚拟内存空间简化了内存的管理，不同进程的同一个虚拟地址可以被内存管理单元（MMU）映射到同一个物理地址上。而且保证了进程之间不会被互相干扰，无需考虑内存冲突的问题。  
_回答细节不重要，一定要体现“逻辑上扩容内存”、“内存+外存”、“进程逻辑上独占整个内存空间”、“虚拟地址被硬件映射到独立的物理地址上，不会发生冲突（除非设计共享信息）”_



## 简述一下如何实现
虚拟存储器中，进程的内存映射是推迟到运行时的，也就是说，一个程序三行代码，只有第一行代码经过地址映射了，执行到第二行时就执行不到了。CPU拿到虚拟地址，然后委托MMU芯片进行地址转换，当MMU查询页表时发现 **存在位是false**，页面没有驻留内存，这时会发出缺页中断（缺页异常 page fault），CPU陷入内核转去处理中断，最终缺页从外存调入内存。物理块不够用则会执行页面置换策略，将低优先级页面换出内存，并将物理块与目标页面进行映射（其实就是修改页表）。此处仅是简单说明，后面会详细叙述过程。

虚拟内存管理使得进程以为自己独占了整个内存空间，可以放心执行，它以为自己的页面都是可以被映射到物理内存上的，其他的进程也是这么想的，**“页多块少”**。而实际上，块总是被映射到被使用的页面，当进程执行时地址转换得不到满足，发出page fault，OS为它调页后恢复上下文，进程好像什么都没发生，继续向下执行。

当然了，上面只是便于理解，肯定不能就这么说，问你简单描述一下如何实现，还没必要答得特别具体，其实就是想引导你说出“**缺页中断**”、“**请求调页**”和“**页面置换”**，如果能扯一下局部性原理就更好了。后面提问者和有可能就会问你“发生缺页中断会发生什么”

> 局部性原理（规律）是程序执行时反映出的一组规律，主要体现在空间和时间两方面  
> 时间方面，如果某行代码被执行后，那么不久后有可能再次被执行（如循环操作）  
> 空间方面，如果某个地址被访问，那么它附近的地址有可能会在不久后被访问（顺序执行）

_**偏正式回答：**_  
根据局部性原理，应用程序运行之前，没必要将全部数据装入内存，只需要装入必要的页面即可，进程运行前，采用预调页策略调入必要的页面，运行时基于请求调页策略将需要的页面从外存调入内存。通过缺页中断引起操作系统干预。

> 程序是静态的，是存入外存的，在外存是有外存地址（物理块）与之映射的，装入内存说白了，就是将外存的副本读入内存，与主存的物理地址映射上（将页面放入主存的某个地址块）。当发生置换或者正在关闭程序时，将脏页更新到外存（磁盘）。

> 随着内存的动态分配，运行堆向上增长，随着子程序的不断调用，运行栈向下增长。**只有在堆和栈增长时才需要物理块，触发缺页异常**，中间部分可以存放共享对象（动态链接库）

**当缺页中断发生时，操作系统必须通过读硬件寄存器来确定哪一个虚拟地址造成缺页中断，并且根据虚拟地址拿到页表项，然后从页表项拿到页面在磁盘中的地址。找到合适的页框存入从磁盘调入的页面。最后，还需要回退程序计数器PC，使程序计数器指向引起缺页中断的指令，重新执行指令。**


## 后备存储
当进程被换出时，页表不需要驻留内存，但是当进程运行时，它必须在内存中，保证覆盖内存的地址空间。操作系统还需要在**磁盘交换区**中分配空间，以便在一个进程换出时，在磁盘上有放置此进程的空间，操作系统会将有关页表和磁盘交换区的信息保存在进程表中。

【1】对静态交换区分页  
磁盘上的交换区与进程虚拟地址空间一样大，每个页面都有固定的位置，当页面从内存换出时则写入到对应的位置。

当写回一个页面时，需要计算其在交换区的地址：**虚拟内存空间中的页面偏移量 + 交换区的起始地址**  
因为数据和堆栈都是动态变化的，因此通常需要为数据、堆栈等保留额外的交换区空间。  
内存中的页面通常在磁盘上具有副本，并且当页面换出内存时被更新同步。  
【2】动态备份页面  
另一种方案是不进行提前分配，而是当页面换出时为其分配磁盘空间，并在换入时回收磁盘空间。  
页面在磁盘上没有固定的地址，当页面换出内存时，需要**及时选择一个空的磁盘页面，并且更新磁盘映射**（每个虚拟页面都会映射到一个磁盘地址空间，这个磁盘映射的哈希表保存在内存中）。  
内存中的页面在磁盘上没有副本，每一次的映射都是动态创建，并随时可能被释放的。

由于不能总是保证可以分配到交换分区，当没有磁盘分区可以时，可以利用文件系统的文件。  
每个进程的程序都来自文件系统中的某个文件，这个**文件就可以作为对换区**。当页面换出内存时可以直接丢弃，需要换入内存时直接从文件中读入即可，共享库利用此方式工作。


## 缺页中断处理过程
什么是缺页中断？请求分页系统中，通过查询页表发现页面没有驻留内存，则触发缺页中断，引起操作系统将页面调入内存的行为。  
缺页中断就是：**虚拟地址没有和物理地址产生映射关系时，通知操作系统调入缺失页面的信号**

缺页中断和一般中断比有一些特点：一条**指令执行期间**可能产生**多次**缺页中断、**指令执行期间**可以产生和处理缺页中断、缺页中断恢复后会**再次执行原指令**（访问页表），而不是向下执行。

> add(1,2,3)中，1,2,3可能在不同页面中，当发生缺页中断，add需要重新执行

_**OS处理缺页中断：**_  
当程序进行地址映射时，MMU查询页表发现页表项存在位为0，则发出缺页中断。CPU响应中断信号，保存上下文后转入执行进程内核态。分析中断原因后转入具体的中断处理程序。如果内存中没有足够的物理块，则根据页面置换程序选出某个页面，如果页表项的修改为1，则将页面刷新到外存，并将物理块释放。当有足够的内存资源时，则启动磁盘IO，根据目标页面的页表项查出页面所在的外存地址，将副本调入内存。IO完成后，操作系统修改页表项存在位为1，并且写入物理块号。还需要刷新TLB（全刷新还是局部刷新看具体OS实现）。恢复上下文，重新执行引起缺页中断的命令（会再次查询页表）。（快表刷新了，没有命中，再次查询页表）

> 虚拟存储管理的两种调页策略：  
> 为了防止进程启动后产生大量缺页，会执行预调页策略。但是预调页可能存在调页失效的问题，可以借助工作集来优化  
> 请求调页策略发生在程序执行时，所需页面没有驻留内存的情况。

系统抖动是频繁缺页中断的表现，如果抛开操作系统层面，服务器抖动通常是由于**内存不够用，（运行的程序）进程（线程）太多**。过多的缺页导致很多进程等待磁盘IO将页面调入的内存而主动放弃CPU，CPU利用率很低。可以考虑主动暂停一部分进程的运行（暂时从内存中取得一些进程）或者限制进程的创建

> 进程的虚拟地址空间不是每个地址都有物理地址与之对应的，当地址第一次被使用后，会触发缺页中断，创建进程、线程都会涉及内存的使用。（广义上的创建进程其实是创建单线程进程）

> 如果考虑OS，则全局置换算法会导致抖动，因为一个进程缺页会抢占别人的物理块，导致别人页缺页，缺页太多导致排队过长，磁盘利用率提升，CPU利用率下降。而CPU利用率下降，可能导致OS错误的创建更多进程。  
> 可以采用局部置换算法。也可以在CPU调度中使用工作集算法，将程序的全部**工作集——任意一个时刻T，都会存在一个集合，包含所有最近K次内存访问过的页面**全部装入内存（进程在某段时间内实际所要访问页面的集合），预调页策略也一定程度上依赖工作集。

## 共享
并不是所有页面都适合共享，对于只读页面如程序文件适合共享，而数据页面也不适合共享。  
**写时复制**思想（CopyOnWrite cow）可以避免最初的请求调页需要。系统调用fork()便具有写时复制的思想.  
当fork（）创建了一个子进程时，子进程共享父进程的地址空间，并将共享的页面标记为共享页面，一旦子进程进行写入操作时（触发只读保护，引发缺页异常陷阱），则发出缺页中断，为子进程创建共享页面的副本。

两个进程还可以将同一物理块映射到各自的虚拟地址空间，来作为**共享内存**。但是需要进行线程安全问题避免。  
还有一种和虚拟内存有关的进程通信是**内存映射文件**，将多个通信进程的虚拟内存页面映射到同一物理地址块，而物理块又与磁盘物理块进行了映射，当关闭文件时，所有的内存映射数据都会同步写入磁盘，并从虚拟内存中删除。  
内存映射文件使得访问磁盘文件像访问常规内存一样简单。内存映射文件是实现共享内存的一种方式。

思想：将一个文件映射到某些进行虚拟地址空间的一部分，仅当访问页面时才会以页为单位的读，磁盘文件则被当做后备存储。当进程退出或显式解除映射的时候，所有被改动的页面会被写回到磁盘文件中。

相当于**把一个文件当做一个内存中的大字符数组**来访问。

如果一个程序两次启动，大多数操作系统会自动共享所有的代码页面，而在内存中只保存一份代码页面副本。  
一个更加通用的技术是**动态链接库**——当一个动态链接库被装载和使用时，整个库并不是被一次性读入内存的，而是按需，以页面为单位进行装载的，**没有被调用的函数不会被装载到内存中**。

编译动态链接库时，通常只使用相对偏移量，而避免使用绝对地址——**位置无关代码**

## 背诵
```markdown

### 为什么要内存管理？
	当一个程序被执行后，如果程序中的指令想要被执行，它必须被从磁盘加载/复制到内存中，因为机器语言不能够将磁盘地址作为参数，只能将内存地址作为参数,为了更好的利用内存资源需要进行内存管理

### 分段与分页
	//分段的最大特点是，每个段都是独立的，运行期间可以动态改变（堆栈段和数据段），并且不会影响到其他的段。  
	单独看分段，它属于一种**连续内存分配策略**，不存在“缺页中断”，而如果分页内存分配涉及函数调用或对象分配导致的内存空间变化，将会引起缺页中断。
	分段的出现，是为了使程序和数据可以被划分为逻辑上独立的内存空间，并且更加容易实现共享与保护。而分页的出现，是为了将一个程序的内存分成若干小份，为每个程序提供虚拟地址空间，装入必要部分而不是一次性装入全部物理内存块，从而支持虚拟内存
	分段存在的问题是：外部碎片明显，且段大小不一，大的段中往往有些内存不总被访问却占用物理内存（内部碎片），同时换入换出内存时占用大量IO时间。分页解决了外部碎片和IO对换效率低的缺点，仅仅存在少量的部分碎片

回答：
	**对内存进行分页和分段，目的都是为了更好的利用内存资源。分页服务于系统，为了更好的管理内存（分配、回收），而分段服务于用户，为了更好的根据开发逻辑关系，去使用内存**。  
现代操作系统通常将一个进程的地址空间分为大小不等、逻辑地址连续的若干逻辑段，然后对逻辑空间再进行分页，将逻辑地址空间分为若干大小相等的页面，而将物理地址空间分为若干大小相等的物理块，其中物理块大小等于页面大小，当程序执行时将用到的页面映射到一个物理块上，实现内存的离散分配。

对比：
分页会造成内部碎片，因为进程地址空间的最后一个页面可能无法被完全利用（用不完），而分段会造成外部碎片，当申请不到足够的连续内存的时候会出现报错（堆内存溢出），（有内存但用不了）。

**分页是信息的物理单位，而分段是信息的逻辑单位。因此分页的大小是收到内存硬件和操作系统软件的因素约束，而分段的对象取决于用户编写的程序。**  
分页是一维的，是系统行为（页号|页内地址）而分段是二维的，是用户行为（段号|段内地址+段的逻辑名字）  
**分段更易实现信息共享和保护，因为一个共享代码区可以涉及多个页面，分页实现相对困难。**


### 页表
//系统为每个进程维护一个页表。每当CPU发出一个虚拟地址，MMU翻译出一个物理地址（物理块号）并且添加到缓存（TLB）中，CPU先去内存中查页表，根据查出的物理块号在根据页内偏移量拼出物理内存地址，再去对应的物理内存访问真实数据

//CPU寻址时首先会查询快表，查询时间很快，可以忽略不计，如果没有命中才会查询内存中的页表。如果命中则可以得到目标物理地址


①CPU给出逻辑地址，由MMU算得页号、页内偏移量，将页号与快表中的所有页号进行比较。

②如果找到匹配的页号，说明要访问的页表项在快表中有副本，则直接从中取出该页对应的内存块号，再将内存块号与页内偏移量拼接形成物理地址，最后，访问该物理地址对应的内存单元。因此，若快表命中，则访问某个逻辑地址仅需一次访存即可。

③如果没有找到匹配的页号，则需要访问内存中的页表，找到对应页表项，得到页面存放的内存块号，再将内存块号与页内偏移量拼接形成物理地址，最后，访问该物理地址对应的内存单元。因此,若快表未命中，则访问某个逻辑地址需要两次访存(注意:在找到页表项后，应同时将其存入快表,以便后面可能的再次访问。但若快表已满，则必须按照-定的算法对旧的页表项进行替换)


### 虚拟内存
虚拟内存=主存内存+辅存内存。  
虚拟内存并不是从物理上扩充内存，而是从**逻辑上扩充内存**。

虚拟内存管理让每个进程认为自己独占了整个地址空间，其实这个地址空间是**主存和磁盘地址空间的抽象**，目的是逻辑上扩充内存容量，逻辑容量等于主存容量与磁盘容量之和。同时，让每个进程拥有一致的虚拟内存空间简化了内存的管理，不同进程的同一个虚拟地址可以被内存管理单元（MMU）映射到同一个物理地址上。而且保证了进程之间不会被互相干扰，无需考虑内存冲突的问题。  
_回答细节不重要，一定要体现“逻辑上扩容内存”、“内存+外存”、“进程逻辑上独占整个内存空间”、“虚拟地址被硬件映射到独立的物理地址上，不会发生冲突（除非设计共享信息）”_

#### 简述一下如何实现
虚拟存储器中，进程的内存映射是推迟到运行时的，也就是说，一个程序三行代码，只有第一行代码经过地址映射了，执行到第二行时就执行不到了。CPU拿到虚拟地址，然后委托MMU芯片进行地址转换，当MMU查询页表时发现 **存在位是false**，页面没有驻留内存，这时会发出缺页中断（缺页异常 page fault），CPU陷入内核转去处理中断，最终缺页从外存调入内存。物理块不够用则会执行页面置换策略，将低优先级页面换出内存，并将物理块与目标页面进行映射（其实就是修改页表）

根据局部性原理，应用程序运行之前，没必要将全部数据装入内存，只需要装入必要的页面即可，**进程运行前，采用预调页策略**调入必要的页面，**运行时基于请求调页策略**将需要的页面从外存调入内存。通过缺页中断引起操作系统干预。

**当缺页中断发生时，操作系统必须通过读硬件寄存器来确定哪一个虚拟地址造成缺页中断，并且根据虚拟地址拿到页表项，然后从页表项拿到页面在磁盘中的地址。找到合适的页框存入从磁盘调入的页面。最后，还需要回退程序计数器PC，使程序计数器指向引起缺页中断的指令，重新执行指令。**

**虚拟存储管理的两种调页策略**：  
1.为了防止进程启动后产生大量缺页，会执行预调页策略。但是预调页可能存在调页失效的问题，//可以借助工作集来优化
	工作集：给一个进程分配的物理页框的集合就是这个进程的驻留集
		   Ⅰ.分配给进程的存储量越小，驻留在主存中的进程数越多，从而提高处理机的时间利用效率
		   Ⅱ.若一个进程在主存中的页数过少，尽管有局部性原理，但页错误率仍然会相对较高
		   Ⅲ.若页数过多，由于局部性原理，给特定进程分配更多的主存空间对该进程的错误率没有明显的影响
	基于以上因素，采用三种策略：
		   Ⅰ.固定分配局部置换，为每个进程分配一定数目的物理块，运行期间不会改变
		   Ⅱ.可变分配全局置换，每个进程分配一定数目的物理块，操作系统自身也保持一个空闲物理块队列，当某进程发生缺页时，系统从空闲物理块队列取出一个物理块分配给该进程
		   Ⅲ.为进程分配一定数目的物理块，若进程发生缺页，只允许该进程在内存的页面中选出一页换出，因此不会影响其他进程的运行。若频繁缺页，则操作系统再为其分配一定数目的物理块，直至缺页率趋于适当程度。反之，若该进程缺页率特别低，则适当减少分配给该进程的物理块
2.请求调页策略发生在程序执行时，所需页面没有驻留内存的情况。

#### OS处理缺页中断
什么是缺页中断？请求分页系统中，通过查询页表发现页面没有驻留内存，则触发缺页中断，引起操作系统将页面调入内存的行为。  
缺页中断就是：**虚拟地址没有和物理地址产生映射关系时，通知操作系统调入缺失页面的信号**

缺页中断和一般中断比有一些特点：一条**指令执行期间**可能产生**多次**缺页中断、**指令执行期间**可以产生和处理缺页中断、缺页中断恢复后会**再次执行原指令**（访问页表），而不是向下执行。

当程序进行地址映射时，MMU查询页表发现页表项存在位为0，则发出缺页中断。CPU响应中断信号，保存上下文后转入执行进程内核态。分析中断原因后转入具体的中断处理程序。如果内存中没有足够的物理块，则根据页面置换程序选出某个页面，如果页表项的修改为1，则将页面刷新到外存，并将物理块释放。当由足够的内存资源时，则启动磁盘IO，根据目标页面的页表项查出页面所在的外存地址，将副本调入内存。IO完成后，操作系统修改页表项存在位为1，并且写入物理块号。还需要刷新TLB（全刷新还是局部刷新看具体OS实现）。恢复上下文，重新执行引起缺页中断的命令（会再次查询页表）。（快表刷新了，没有命中，再次查询页表）


### 抖动
刚刚换出的页面马上又要换入主存，刚刚换入的页面马上又要换出主存。若一个进程在换页上用的时间多于执行时间，则这个进程就在抖动

原因：某个进程频繁访问的页面数目高于可用的物理页数目。还有就是不合适的页面置换算法

系统抖动是频繁缺页中断的表现，如果抛开操作系统层面，服务器抖动通常是由于**内存不够用，（运行的程序）进程（线程）太多**。过多的缺页导致很多进程等待磁盘IO将页面调入的内存而主动放弃CPU，CPU利用率很低。可以考虑主动暂停一部分进程的运行（暂时从内存中取得一些进程）或者限制进程的创建

如果考虑OS，则全局置换算法会导致抖动，因为一个进程缺页会抢占别人的物理块，导致别人页缺页，缺页太多导致排队过长，磁盘利用率提升，CPU利用率下降。而CPU利用率下降，可能导致OS错误的创建更多进程。  
> 可以采用局部置换算法。也可以在CPU调度中使用工作集算法，将程序的全部**工作集——任意一个时刻T，都会存在一个集合，包含所有最近K次内存访问过的页面**全部装入内存（进程在某段时间内实际所要访问页面的集合），预调页策略也一定程度上依赖工作集。

### 工作集
某段时间间隔内，进程要访问的页面的集合。基于局部性原理，可用最近访问过的页面来确定工作集

工作集模型原理：让操作系统跟踪每个进程的工作集，并为进程分配大于其工作集的物理块，落在工作集内的页面需要调入驻留集中，而落在工作集外的页面可从驻留集中换出。若还有空闲物理块，可以再调一个进程到内存中增加多道程序数。**若所以进程的工作集之和超过了可用物理块的总数，则操作系统会暂停一个进程，将其页面调出并将其物理块分配给其他进程，防止出现抖动现象**


### 共享
并不是所有页面都适合共享，对于只读页面如程序文件适合共享，而数据页面也不适合共享。  
**写时复制**思想（CopyOnWrite cow）可以避免最初的请求调页需要。系统调用fork()便具有写时复制的思想.  
当fork（）创建了一个子进程时，子进程共享父进程的地址空间，并将共享的页面标记为共享页面，一旦子进程进行写入操作时（触发只读保护，引发缺页异常陷阱），则发出缺页中断，为子进程创建共享页面的副本。

两个进程还可以将同一物理块映射到各自的虚拟地址空间，来作为**共享内存**。但是需要进行线程安全问题避免。  
还有一种和虚拟内存有关的进程通信是**内存映射文件**，将多个通信进程的虚拟内存页面映射到同一物理地址块，而物理块又与磁盘物理块进行了映射，当关闭文件时，所有的内存映射数据都会同步写入磁盘，并从虚拟内存中删除。  
内存映射文件使得访问磁盘文件像访问常规内存一样简单。内存映射文件是实现共享内存的一种方式。

思想：将一个文件映射到某些进行虚拟地址空间的一部分，仅当访问页面时才会以页为单位的读，磁盘文件则被当做后备存储。当进程退出或显式解除映射的时候，所有被改动的页面会被写回到磁盘文件中。

相当于**把一个文件当做一个内存中的大字符数组**来访问。

如果一个程序两次启动，大多数操作系统会自动共享所有的代码页面，而在内存中只保存一份代码页面副本。  
一个更加通用的技术是**动态链接库**——当一个动态链接库被装载和使用时，整个库并不是被一次性读入内存的，而是按需，以页面为单位进行装载的，**没有被调用的函数不会被装载到内存中**。

编译动态链接库时，通常只使用相对偏移量，而避免使用绝对地址——**位置无关代码**
```




```markdown

首先,内存就是一长串字节数组,编程的时候难免要申请一段内存空间来存放数据和指令代码

将用户源程序变为可在内存中执行的程序,通常需要经过以下三个步骤:
1.编译
2.链接
3.装入
  装入又分为3类:
  1.绝对装入:程序中的逻辑地址与实际内存地址完全相同
  2.可重定位装入:一个作业装入内存时,必须分配其要求的全部内存空间,如果没有足够的内存,就不能装入该作业,此外,作业一旦进入内存后,在整个运行期间都不能在内存中移动,也不能再申请内存空间
  3.动态运行时装入(动态重定位):装入内存后的所有地址均为相对地址((需要重定位寄存器支持)).其特点是可以将程序分配到不连续的存储区中,在程序运行前只装入部分代码就可运行,在运行期间根据需要动态申请分配内存,可以向用户提供一个比存储空间大得多的地址地址空间

  
早期每一个程序直接访问物理内存,即0到某个上限的地址集合,这也被称为物理地址空间

**如果程序中的指令想要被执行，它必须被从磁盘加载/复制到内存中，因为机器语言不能够将磁盘地址作为参数，只能将内存地址作为参数。**  

cpu看到的数据地址是一个逻辑地址,通过内存管理单元MMU将该逻辑地址转换为物理地址,再去取数据
MMU通常是CPU芯片的一部分，负责执行将虚拟内存地址映射为物理内存地址，通常包含着用于高速缓存映射对的寄存器TLB。

为什么要用虚拟内存?
1.实现内存复用,提高内存利用率





**对内存进行分页和分段，目的都是为了更好的利用内存资源。分页服务于系统，为了更好的管理内存（分配、回收），而分段服务于用户，为了更好的根据开发逻辑关系，去使用内存**。  
现代操作系统通常将一个进程的地址空间分为大小不等、逻辑地址连续的若干逻辑段，然后对逻辑空间再进行分页，将逻辑地址空间分为若干大小相等的页面，而将物理地址空间分为若干大小相等的物理块，其中物理块大小等于页面大小，当程序执行时将用到的页面映射到一个物理块上，实现内存的离散分配。

_对比：_  
分页会造成内部碎片，因为进程地址空间的最后一个页面可能无法被完全利用（用不完），而分段会造成外部碎片，当申请不到足够的连续内存的时候会出现报错（堆内存溢出），（有内存但用不了）。

假设堆内存3M，而目前堆中最大还可以连续内存是1M，如果这时申请一个1.5M的对象就会OOM

分页是信息的物理单位，而分段是信息的逻辑单位。因此分页的大小是收到内存硬件和操作系统软件的因素约束，而分段的对象取决于用户编写的程序。  
分页是一维的，是系统行为（页号|页内地址）而分段是二维的，是用户行为（段号|段内地址+段的逻辑名字）  
分段更易实现信息共享和保护，而一个共享代码区可以涉及多个页面，分页实现相对困难。




虚拟内存管理让每个进程任务自己独占了整个地址空间，其实这个地址空间是**主存和磁盘地址空间的抽象**，目的是逻辑上扩充内存容量，逻辑容量等于主存容量与磁盘容量之和。同时，让每个进程拥有一致的虚拟内存空间简化了内存的管理，不同进程的同一个虚拟地址可以被内存管理单元（MMU）映射到同一个物理地址上。而且保证了进程之间不会被互相干扰，无需考虑内存冲突的问题。  
_回答细节不重要，一定要体现“逻辑上扩容内存”、“内存+外存”、“进程逻辑上独占整个内存空间”、“虚拟地址被硬件映射到独立的物理地址上，不会发生冲突（除非设计共享信息）”_

```


## 页面置换算法
最佳页面置换算法
在缺页中断发生时，有些页面在内存中，其中有一个页面（包含紧接着的下一条指令的那个页面）将很快被访问，其他页面也可能在10、100、1000条指令后才会被访问，每个页面都可以用在该页面首次被访问前所要执行的指令数作为标记。
最优页面置换算法规定应该置换标记最大的页面

最近未使用页面置换算法（Not Recently Used）
随机的从类编号最小的非空类中挑选一个页面淘汰。优点在于易于理解和能够有效被实现，虽然性能不是最好的。
使用R位和W位：当启动进程时，它的所有的页面的两个位都被设置为0，R位被定期的（比如每次时钟中断时）清零，以区分最近没有被访问的页面和被访问的页面

先进先出页面置换算法
由操作系统维护一个所有当前在内存中的页面的链表，最新进入的页面放在表尾，最早进入的页面放在表头。当发生缺页中断时，淘汰表头的页面并把新调入的页面放在表尾

第二次机会页面置换算法
对先进先出算法进行改进，检查最老页面的R位。如果R位是0，那么这个页面既老又没有被使用，可以立刻置换掉；如果是1，就将R位清0，并把该页面放在链表的尾端，修改它的装入时间就像刚装入的一样，然后继续搜索
第二次机会算法就是寻找一个最近的时钟间隔内没有被访问过的页面

时钟页面置换算法
一个更好的办法是把所有的页面都保存在一个类似钟面的环形链表中，一个表针指向最老的页面。当发生缺页中断时，算法首先检查表针指向的页面，如果它的R位是0就淘汰该页面，并把新的页面插入这个位置；如果R位是1就清除R位并把表针前移一个位置。如此重复。

最近最少使用页面置换算法（Least Recently Used）
在缺页中断发生时，置换未使用时间最长的页面

用软件模拟LRU
一种可能的方案是NFU（Not Frequently Used，最不常用）算法。该算法将每个页面与一个软件计数器相关联，计数器的初值为0。每次时钟中断时，由操作系统扫描内存中所有的页面，将每个页面的R位加到它的计数器上。这个计数器大体上跟踪了每个页面被访问的频繁程度。发生缺页中断时，则置换计数器值最小的页面。该算法的主要问题在于从来不忘记任何事情

优化：使用老化算法。首先在R位被加进之前先将计数器右移一位；其次。将R位加到计数器最左端的位而不是最右端的位。发生缺页中断时，将置换计数器值最小的页面。

工作集页面置换算法
请求调页策略：页面是在需要时被调入的，而不是预先装入
局部性原理：在进程运行的任何阶段，它都只访问较少的一部分页面
一个进程正在使用的页面的集合称为它的工作集。如果整个工作集都被装入到了内存中，那么进程在运行到下一阶段之前，不会产生很多缺页中断。
若每执行几条指令就发生了一次缺页中断，那么就称这个程序发生了颠簸。
跟踪进程的工作集，以确保在让进程运行以前，它的工作集就已经在内存中。该方法称为工作集模型，其目的在于大大减少缺页中断率。在进程运行前就预先装入其工作集页面也称为预先调页。工作集是随着时间变化的


[(63条消息) 页面调度算法模拟_欠扁的小篮子的博客-CSDN博客](https://blog.csdn.net/u013805360/article/details/50571859?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1.pc_relevant_paycolumn_v3&spm=1001.2101.3001.4242.2&utm_relevant_index=4)
先进先出，最佳置换，最近最久未使用算法，Clock算法，改进Clock算法

先进先出不用说

最佳置换：未来最慢被访问到的页面淘汰

最近最久未使用算法：选择过去一段时间里最久未被使用的页面

Clock算法：当某一页首次装入内存中时，则将该页框的使用位设置为1；当该页随后被访问到时（在访问产生缺页中断之后），它的使用位也会被设置为1。当请求页面不在内存中时，查找内存中的页面，每当遇到一个使用位为1的页框时，就将该位重新置为0；如果在这个过程开始时，缓冲区中所有页框的使用位均为0时，则选择遇到的第一个页框置换；如果所有页框的使用位均为1时，则指针在缓冲区中完整地循环一周，把所有使用位都置为0，再次循环遍历，置换第一个遇到的使用位为0的页面。

改进Clock算法：在将一个页面换出时，如果该页已被修改过，便须将它重新写到磁盘上；但如果该页未被修改过，则不必将它拷回磁盘。同时满足这两条件的页面作为首先淘汰的页。由访问位A和修改位M可以组合成下面四种类型的页面：

1.（A=0，M=0）：表示该页最近既未被访问、又未被修改，是最佳淘汰页。 

2. （A=0，M=1）：表示该页最近未被访问，但已被修改，并不是很好的淘汰页。 

3. （A=1，M=0）：最近已被访问，但未被修改，该页有可能再被访问。 

4. （A=1，M=1）：最近已被访问且被修改，该页有可能再被访问.

在进行页面置换时，其执行过程可分成以下三次遍历：

（1）从指针所指示的当前位置开始，扫描循环队列，寻找A=0且M=0的第一类页面，将所遇到的第一个页面作为所选中的淘汰页。在第一次扫描期间不改变访问位A。   

（2）如果第一步失败，即查找一周后未遇到第一类页面，则开始第二轮扫描，寻找A=0且M=1的第二类页面，将所遇到的第一个这类页面作为淘汰页。在第二轮扫描期间，将所有经过的页面的访问位置0。   

（3）如果第二步也失败，即未找到第二类页面，则将指针返回到开始的位置，并将所有的访问位复0。然后，重复第一步，如果仍失败，必要时再重复第二步，此时就一定能够找到被淘汰的页。

# 死锁

## 理解死锁
死锁是并发环境下的一种僵持状态。  
一组线程中，两部分线程**互相等待**只有对方才能引起的事件，陷入僵持导致谁都不能够返回。

死锁，这个锁可以是操作系统层面的，也可以是应用层面的，可以是自旋锁，也可以是阻塞锁，我们主要关注它的行为。死锁中的僵持状态，指的就是一个线程上锁失败，**执行失败策略——自旋或阻塞**，前者是占用CPU的，后者是不占用CPU的。但是它们由于某种原因，**无限执行失败策略**而不能返回。

引发死锁的四个条件（牢记）：  
**【1】互斥  
【2】请求与保持  
【3】不可剥夺  
【4】循环等待**  
死锁必须是一个互斥锁，非共享的，这个锁要么owner是A要么是B，非owner想要获得锁就必须等待或询问锁的状态。而且这个锁不能被抢占剥夺，只能被持有者主动释放。而一个线程保持一个互斥锁并请求另外一个互斥锁，同时另一个线程也执行类似的操作，那么便满足上述四个条件，将造成死锁。（同一线程连续执行不可重入代码块也会造成死锁）


## 写一个死锁

Java中的死锁通常聚焦多线程场景下的死锁，指的是多个线程之间，由于自身持有对方所需要的资源不释放，而等待对方先释放自身所需要的资源，而造成的一种永久阻塞的状态

```
    static class A implements Runnable{
        @Override
        public void run() {
            synchronized ("A"){
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                synchronized ("B"){
                }
            }
        }
    }
    static class B implements Runnable{
        @Override
        public void run() {
            synchronized ("B"){
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                synchronized ("A"){
                }
            }
        }
    }

    
```

synchronized底层基于互斥锁实现，满足互斥条件。  
开启两个线程，并且他们的任务需要申请两个锁，其中A任务先申请A锁再申请B锁。而B任务先申请B锁再申请A锁，并且申请第二个锁的时候阻塞一段时间保证两个线程申请第二个锁前已经获取第一个锁完毕，当获取第二个锁时阻塞，达成请求与保持。等待对方释放锁并挂起，谁都不能唤醒谁，达成循环等待。锁只能由owner释放，满足不可剥夺条件。

## 检测死锁

死锁最直接的外部表现就是**调用不能返回**（或是**请求总是超时**，无法正常执行）。调用不能够返回，那么当前运行中的线程则不能退出，线程申请的栈内存、以及使用到的对象不能被释放，将会造成**内存泄露**。  
如果该锁是应用层实现的自旋锁，那么还表现为CPU占用率上升（本质上是非正常的死循环）

【1】首先在命令行，使用jps拿到java进程的进程号。通过jps命令列出本机内所有java进程的pid。  
【2】拿到进程号就可以使用jdk提供的工具了。使用jstack可以打印目标java进程的线程堆栈信息，并且发现死锁的线程。  
【3】也可以使用jdk提供了可视化工具JConsole或者VisualVM

通常使用jstack定位获取线程栈来定位死锁，定位相互之间的依赖关系，进而找到死锁。如果程序发生死锁，只能重启，修复程序本身

死锁的预防的直接副作用就是，降低设备使用率以及系统吞吐量。  
java中可以通过Lock接口提供的非阻塞锁、超时锁、可响应中断锁来预防死锁。

> 银行家算法是批处理时代避免死锁的技术，资源需求量总是在程序运行前就是已知的。而现代操作系统中以交互性程序为主，事前无法确定资源需求量，需要更加复杂的解决方案。

## 哲学家进餐问题

[哲学家进餐问题](https://leetcode-cn.com/problems/the-dining-philosophers/)  
五个哲学家，面前只有五个筷子，哲学家能够进餐的前提是拿到左右两个筷子。**当五个哲学家同时左手持有五个叉子等待右边的叉子时会发生死锁**，问题的难点在于如何避免死锁

哲学家问题的本质是如何避免死锁，如果五个哲学家全部左手/右手有筷子，而空等另一只筷子，就会陷入线程死锁

## 串行化

```java
 private Semaphore mutex =new Semaphore(1);	//这里的二元信号量作为互斥量使用

    public void wantsToEat(int philosopher,
                           Runnable pickLeftFork,
                           Runnable pickRightFork,
                           Runnable eat,
                           Runnable putLeftFork,
                           Runnable putRightFork) throws InterruptedException {
        mutex.acquire();
        pickLeftFork.run();
        pickRightFork.run();
        eat.run();
        putLeftFork.run();
        putRightFork.run();
        mutex.release();
    }

    
```

这种方案相当于把哲学家进程串行化了，使得同一时刻仅能使得一个哲学家可以进餐，即使存在多余的筷子，申请筷子的其余哲学家也阻塞在mutex中。

这里提供三种主流的解决方案：

## 最多允许四位哲学家申请左边筷子

最多只允许4位哲学家同时去拿左边的筷子，最终能够保证至少有一位哲学家可以进餐，用餐过后释放出两个筷子

```java
private ReentrantLock[] locks = new ReentrantLock[5];
private Semaphore limit = new Semaphore(4);

    
```

最多允许四个哲学家去拿**左边**的筷子，因此我们不使用互斥量，而是使用值为4的信号量，当信号量值大于0时表示放行，小于等于0时表示禁止通行，就像红绿灯一样。  
其中大小为5的锁数组分别对应5个筷子，之前的串行化中，5个筷子都是属于owner的，不需要特地标志  
而现在5个筷子可能同时被不同的哲学家申请，因此需要是互斥的。

```
int leftFork = philosopher;
int rightFork = (philosopher+1)%5;

limit.acquire();
//至少有一个哲学家两个筷子可以同时得到
locks[leftFork].lock();
locks[rightFork].lock();

eat.run();

locks[leftFork].unlock();
locks[rightFork].unlock();

limit.release();

    
```

申请信号量的代码可以返回，只是说明当前哲学家**抢到了申请左边筷子的机会**（最终会有一个哲学家不能申请左边的筷子），但是不一定能申请到右边的筷子。（四个哲学家从五个筷子中抢两个，且仅当左边筷子申请成功后，才能申请右边的筷子）  
某一时刻，四位哲学家拿到了左手边的筷子，一位哲学家被阻塞到limit.acquire().**而四位哲学家中一定有一位能够抢到剩下一个筷子**

## 必须同时拿到两双筷子

【2】仅仅当同时拿到左右两个筷子的时候，才能够允许拿取筷子

```
    private ReentrantLock[] locks = new ReentrantLock[5];
    //用于互斥访问临界资源:同一时间只能有一个人完成“一次拿起两个叉子”的动作
    private Semaphore mutex =new Semaphore(1);

    
```

互斥量用于限定拿筷子的动作，为的是**保证“同时拿起两个筷子”这个动作不能够被打断**。

```
        mutex.acquire();//一次拿起两个叉子，是一个原子操作，必须进行同步

        locks[philosopher].lock();//左手
        locks[(philosopher+1)%5].lock();//右手
        pickLeftFork.run();
        pickRightFork.run();

        mutex.release();//同时拿到筷子完毕，这个动作可以由另一个哲学家执行，不影响当前哲学家进餐

        eat.run();
        putLeftFork.run();
        putRightFork.run();
        //放下筷子
        locks[philosopher].unlock();
        locks[(philosopher+1)%5].unlock();

    
```

## 根据编号执行不同动作

**规定奇数编号的哲学家先拿起左手边的筷子，再去拿右手边的筷子。而偶数编号的哲学家正好相反**。即1和2中，有一位将得到1号筷子，3和4中争取3号筷子，之后再去竞争偶数号筷子，最终总有一个哲学家会获得两个筷子进餐。相当于**将竞争的范围缩小到相邻哲学家之间**，而不是所有哲学家共同异步竞争。

```
	   int left =philosopher;
       int right=(philosopher+1)%5;

       //奇数哲学家先拿左边的筷子,偶数哲学家先拿右边筷子
        if(left%2==1){
            semaphores[left].acquire();
            semaphores[right].acquire();
        }else {
            semaphores[right].acquire();
            semaphores[left].acquire();
        }
        pickLeftFork.run();
        pickRightFork.run();

        eat.run();

        putLeftFork.run();
        putRightFork.run();

        semaphores[left].release();
        semaphores[right].release();

    
```

上面使用semaphore数组的成员为semaphore（1）等价于reentrantLock，实现的都是互斥量的效果。


## PV操作的情况
>[(61条消息) PV操作的三种情况_四季风-天平的博客-CSDN博客](https://blog.csdn.net/u012987386/article/details/72511569)

1. 把信号量视为一个加锁标志位，实现对一个共享变量的互斥访问。
2. 把信号量视为是某种类型的共享资源的剩余个数，实现对一类共享资源的访问。
3. 把信号量作为进程间的同步工具