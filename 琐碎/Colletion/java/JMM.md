# 线程安全问题出现的根本原因
程序最开始是静态的、原子的、顺序执行的，虽然CPU利用率不高（CPU总是能有时间偷懒），但是程序员需要操心的问题不多。为了提升效率，陆续引入进程和线程的概念，并且实现了相应的数据结构。程序变成了动态的、并发的、异步的、执行也不再是原子的了。CPU利用率上去了，程序员要操心的事情多了…

CPU缓存的作用主要是为了解决CPU运算速度与内存读写速度不匹配的矛盾

线程安全问题的根源可以被归纳为三点：原子性问题、可见性问题和有序性问题。

## 原子性
由于存在线程和进程切换，我们无法做到一气呵成的执行某段程序，因为总是存在无法避免的中断产生。这倒没什么，因为如果它们的数据如果是私有的，这种切换不会造成什么问题，肯定不会存在“一会儿聊QQ一会儿聊微信，然后QQ发出的消息跑到微信上了”，**真正导致线程安全问题的是：多个线程/进程操作的不是它们私有的数据，而是共享数据！！！如果一个线程修改共享数据不是原子的，那么其他线程就会读到“改到一半”的数据——不是原子性，就意味着访问共享变量的操作不能被称之为一个事务，可能会产生读写冲突（脏读）、写写冲突（修改丢失）等**

## 可见性
**CPU访问内存需要的时钟周期，和CPU执行一条指令的时钟周期相比实在是太慢了，引入高速缓存就是为了弥补这种速度差，要求CPU减少直接访存的次数。
这虽然提升了CPU读取数据的效率，但是引入了可见性问题，每个核心都有自己的缓存（L1/L2 ）**，因为多核处理器时代中，每个核心具有独立的缓存，这导致核心之间缓存同一共享变量的值可能是不一致的。

## 有序性
编译器和处理器都会对不同层次的指令进行重排序，来加速指令执行效率（如一次性执行所有的读操作后，再统一执行一次写操作），重排序优化的原则只能保证不影响单线程的执行结果，但是可能使多线程程序执行结果出现问题。


# Java内存模型
因为不同的平台具有不同的CPU、缓存、操作系统等，JVM为了实现跨平台性，制定了统一规范，这其中就包含内存模型JMM。
>JVM本质上是一组规范，是一堆接口，而不同的平台去实现这组规范，最终达成不同平台但功能一致的特点

JMM定义了线程如何访问共享变量的规则：
所有的共享变量存储在主内存中，而每个线程都具有独立的工作内存，各线程只能自己的工作内存操作变量，而且线程之间不能直接访问对方的工作内存，工作内存存放的主内存的副本，线程之间如果需要传递变量值，必须依靠主内存。
（不是操作共享变量，那就不存在线程安全问题了，没有讨论必要）
>JMM就是对CPU-高速缓存-主存关系的抽象。JMM不管你的CPU和内存条是AMD还是Intel，它提供一个统一的视图，你用java写程序，就不要考虑操作系统和CPU那一套东西了，就看我给你提供内存模型JMM就可以了。

总结：JMM屏蔽了底层内存模型的差异与细节，使得java程序在各个不同底层软件和硬件平台上，都可以达到一致的内存访问效果。


## JMM中的原子/可见/有序问题
既然JMM让我们“眼里只有它”，那么我们就来分析一下包上一层外衣之后，java程序存在哪些线程安全问题，依然是之前的三条：
【1】线程切换，访问共享变量的一组操作无法被看作一个事务
【2】由于线程只和自己的工作内存打交道，而且无法直接访问其他线程的工作内存，因此存在数据不一致问题
【3】编译器会对程序进行重排序优化，而且仅保证as-if-serial

>这里先说个结论：synchronized、Lock基于AQS的实现类具有原子、可见、有序性，而volatile具有可见、有序性，而且它只能做到“无状态”的单条语句读写的原子性（如double d =2.2，但是涉及对个变量的读写如i=i+1无法保证）


### as-if-serial原则
CPU重排序可以在保证程序线性执行结果准确性的前提，对性能进行优化，而JMM则将这项保证一项JMM规则提供给java程序员——**保证java程序单线程下总是能得到正确的结果，但是多线程就无法保证了**。（对于未正确同步的程序，JMM只提供**最小的安全性保证**：读取的值可能会出错，但是不是无中生有的，程序员可以简单推断出多线程的异步执行如何导致错误结果产生。）


### happens-before规则

happens-before是JMM的核心，之所以设计happens-before，主要出于以下两个方面的因素考虑的：1）程序员的角度，JMM内存模型需要易于理解、易于编程；2）编译器和处理器的角度，编译器和处理器希望内存模型对其束缚越少越好，这样就可以根据自己的处理规则进行优化。但是这两个方面其实是相互矛盾的，因为JMM易于编程和理解就意味着对编译器和处理器的束缚就越多。


happens-before定义
基于上面的考虑，设计JMM时采用了一种折中的选择——JMM将需要禁止的重排序分为两类（因为编译器和处理器的优化大部分是重排序，所以JMM的处理的关键也就是重排序了）：

会改变程序执行结果的重排序
不会改变程序执行结果的重排序
对应这两种情况，JMM采用了不同的策略：

对于会改变程序执行结果的重排序，JMM要求编译器和处理器必须禁止这种重排序
对于不会改变程序执行结果的重排序，JMM对编译器和和处理器不做任何要求（自然，编译器和处理器可以其进行重排序）

所以JMM的设计基于这样一种原则：先保证正确性，在考虑执行效率问题。

说了这么多，与happens-before原则有什么关系呢？从上面可以看到JMM实际上可以看做是操作之间的约束模型，这种约束模型的实现就是我们要提到的happens-before了。happens-before**用来指定两个操作之间的执行顺序**，这两个操作可以在一个线程之内也可以在不同的线程中，所以这种对操作顺序的关系的界定可以为程序员提供内存可见性的保证。具体happen-before的定义如下：
>1）如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前
  2）两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。

```markdown
《JSR-133:Java Memory Model and Thread Specification》对happens-before关系的定义如下：

1）如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。注意：这一点仅仅是JMM对程序员的保证

2）两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）。
```

上面第二句话的意思就是说，如果重排序之后的执行结果与按照原来那种happens-before关系执行的结果一致，那么JMM允许编译器和处理器进行这种重排序。所以可以认为：**只要不改变程序的执行结果，编译器和处理器可以随意优化**。

**联系之前提到的as-if-serial语义（保证单线程内的程序执行结果不会改变），现在提到的happens-before则保证正确同步的多线程的执行程序的执行结果不会发生改变。**

```markdown
A happens-before B并不代表A一定先于B发生。只是保证如果有共享内存，A改变后B立即可见。i=1:j=2;同一个线程中的这两条指令根据程序顺序原则:i=1 happens-before j=2意思就是说:如果i=1先执行(如果两字很重要)，那么执行j=2时就能看到i已经改变了。但是由于这两条指令相互没有任何影响，可以进行重排序，所以不一定先执行i=1这条指令。即hapens-before与操作的执行顺序完全是两码事，只不过对于:i=2;j=i+i; **因为程序判定两条指令之间有依赖关系从而不会重排序而已**

```


程序顺序规则：一个线程中的每个操作，happens-before于随后该线程中的任意后续操作
监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的获取
volatile变量规则：对一个volatile域的写，happens-before于对这个变量的读
传递性：如果A happens-before B，B happens-before C，那么A happens-before C
start规则：如果线程A执行线程B的start方法，那么线程A的ThreadB.start()happens-before于线程B的任意操作
join规则：如果线程A执行线程B的join方法，那么线程B的任意操作happens-before于线程A从TreadB.join()方法成功返回。
1.  对线程interrup()方法的调用， happens-before 于被中断线程的代码检测到中断事件的发生
2.  一个对象的初始化完成（构造函数执行结束）happens-before finalize()方法的开始

看看start规则，假设这样一种情况，如果线程A在执行线程B的start方法之前修改了一些共享变量的值，那么当线程B执行start方法的时候，会去读取这些修改的共享变量的值（上面规则就是这么规定的），这就意味着线程A对共享变量的修改对线程B可见

下面看看join规则是怎么回事。join方法的本义是等待当前执行的线程终止。假设在线程B终止之前，修改了一些共享变量（完全可能啊），线程A从线程B的join方法成功返回后，就会读取这些修改的共享变量。这样也保证了线程B对共享变量的修改对线程A是可见的。







CPU设计者并不会保证程序的线程安全，但是提供了一些内存屏障给程序员，程序员可以使用内存屏障约束CPU某些行为——告诉CPU，哪些指令、哪些位置不需要进行优化。
内存屏障可以用于解决可见性和有序性问题，编译器会根据用户的程序，在适当的位置插入内存屏障，程序员不需要了解复杂的内存屏障指令，JMM通过happens-before原则告诉程序员“JMM为哪些地方可以做出线程安全保障”

>程序员根据happens-before原则指导，使用volatile、lock、start()等，编译器会在指令的周围插入内存屏障，最终内存屏障在底层会被翻译为附带lock前缀的汇编指令，来保证相应的同步语义。
说白了，底层很复杂，JMM屏蔽了底层是如何实现同步语义的，向上提供一组规则，程序员在规则指导下正确地使用相应的关键字和方法，来达到编写正确同步的多线程程序的目的

happens-before提供一组规则，如果A操作在B操作之前发生（即使两个操作来自不同的线程），那么执行操作B时，A操作中做出的改变B操作都是可以看见的，而且底层不会对这两个操作进行重排序——A对B可见。可以看作**JMM就重排序和可见性对程序员提供的保证**






### 理解同步和互斥(这就是为什么明明存在线程调度,但synchronized锁住的代码块逻辑上却是原子的)
同步就是步调一致，我在缓存区中修改了变量值，你也应该把你那一份和我的数据保证一种，线程同步就是让两个线程保持的信息一致。
如果一个现在正在执行某一组同步操作，那么这执行中途就不能被其他线程打断，如果线程被打断，另一线程读到“脏数据即不同步的数据”，那么两个线程同时执行完毕同步块的内容，最终两个线程的数据将是不一致的即不同步的。
因此访问同步代码时，线程需要互斥地访问，同一时间只能有一个线程正在执行同步块的内容。

>讨论java中的同步块，那么这个切换仅限于java线程，因为java层次无法阻止操作系统层次的线程进行切换和调度，有上层进行了同步，即使操作系统层面发送线程切换，被切换到的线程没有获取到锁于是进入就绪队列，仍然不影响上层同步块的语义。




# 理解volatile
volatile易变的。从语义上理解，volatile对某个变量声明为“易变的”，每个线程总是从主存中拿到最新的数值而不是从读取工作缓存中的，而每个线程修改完这个变量后也将不会保存这个易变的变量，而是写入主存。达到的效果就是——volatile修饰的变量，总是在线程之间可见

## volatile的使用
volatile是java提供的一种**轻量级的线程通信方案**，可以代替锁保证线程对变量的可见性，可以配合CAS指令实现乐观锁或悲观锁。AQS框架底层就是基于读写volatile实现可见性语义的。
>volatile读写的内存语义是从JSR-133(JDK5)加强之后才具有的，volatile读写与锁的获取/释放具有相同的内存语义

## volatile的特性
volatile两大特性：
【1】保证变量的内存可见性
【2】禁止重排序

内存语义：
【1】当写一个volatile变量时，JMM会将线程保存在本地内存的副本刷新入主内存，并使其他线程的变量副本无效化。
【2】当读一个volatile变量时，从主存中读取最新值。

注意：
volatile不是线程安全的，因为只能保证可见和有序，无法保证原子性。
volatile可以保证类似i=1读写的原子性，但是不能保证“存在关联关系”的原子性如i=i+1、a=b+c
>对于任意单个volatile变量的读/写具有原子性，即使是64位的long和double

# volatile原理

## MESI缓存一致性协议
因为有高速缓存的存在，所以就导致各个处理器可能对一个变量会在自己的高速缓存里有自己的副本，这样一个处理器修改了变量值，别的处理器是看不到的，所以就是为了这个问题引入了缓存一致性协议（MESI协议）

### MESI协议介绍
MESI协议规定：对一个共享变量的读操作可以是多个处理器并发执行的，但是如果是对一个共享变量的写操作，只有一个cpu可以执行，其实也会通过排他锁的机制保证就一个处理器能写  

MESI协议规定了一组消息，各个cpu在操作内存数据的时候，都会往总线发送消息，而且各个cpu还会不停的从总线嗅探最新的消息，通过这个总线的消息传递来保证各个cpu的协作  

上面的特性保证了可见性和有序性问题.(但是排他锁的机制降低了性能)

在MESI协议中，每个Cache line有4个状态，可用2个bit表示，它们分别是：
```markdown
（1）invalid：无效的，标记为I，这个意思就是当前cache entry无效，里面的数据不能使用
 
（2）shared：共享的，标记为S，这个意思是当前cache entry有效，而且里面的数据在各个处理器中都有各自的副本，但是这些副本的值跟主内存的值是一样的，各个处理器就是并发的在读而已
 
（3）exclusive：独占的，标记为E，这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中。
 
（4）modified：修改过的，标记为M，只能有一个处理器对共享数据更新，所以只有更新数据的处理器的cache到内存中，才是exclusive状态，表明当前线程更新了这个数据，这个副本的数据跟主内存是不一样的

```


CPU缓冲区为了保证数据一致性，遵循MESI缓存一致性协议，某个CPU更新数据时，若数据状态为S，那么需要invalidate消息到总线，尝试让其他的处理器的该数据对应的cache line状态变为I .得到其他CPU确认信号后才会进行写缓冲区操作.
其他的cpu会从总线嗅探到invalidate消息，若缓存中存在对应的cache line，就将其状态设为I,然后返回invalidate ack消息到总线
**修改数据的cpu要修改的数据对应的 cache line状态设为 E，这样在独占期间别的cpu发出 invalidate消息时，该cpu是不会返回 invalidate ack消息的，这就保证了有序性**
当cpu修改完这条数据后，将其状态设置为M，或是将数据强制写回主内存中。
然后其他cpu此时这条数据的状态就都是I了，如果需要读的话，就要重新发送read消息，从主内存（或其他处理器）来加载


**旧版本的MESI存在的串行化问题**
可能存在性能问题.上面的机制相当于串行执行写数据了.如果每次写数据的时候都要发送invalidate消息等待所有处理器返回ack，然后获取独占锁后才能写数据，那可能就会导致性能很差了，因为这个对共享变量的写操作，实际上在硬件级别变成串行的了
**如何解决呢? 硬件层面引入了写缓冲器和无效队列**

写缓冲器：将数据先写入写缓冲器，同时发送 invalidate消息，就可以去干别的事，不会阻塞在这里，收到其他cpu的ack消息后，再将数据取出并写入本地缓存。
（引入写缓冲器后，一个写指令发出后，放入缓冲器后就直接往下执行了，也就是说写操作不是立即生效的）
查询数据的时候，会先从写缓冲器里查，因为有可能刚修改的值在这里，然后才会从高速缓存里查，这个就是存储转发,同样提升了效率

无效队列：引入无效队列,其他处理器在接收到了invalidate消息之后，不需要立马过期本地缓存，直接把消息放入无效队列，就返回ack给那个写处理器了，这就进一步加速了性能，然后之后从无效队列里取出来消息，过期本地缓存即可

【1】对于收到的所有invalidate请求，必须立即返回确认
【2】invalidate并不会真正执行，而是放入失效队列，在方便的时候才回去执行
【3】处理器不会发送任何消息给所处理的缓存条目，直到处理invalidate请求
（相当于使用一个队列，临时存储invalidate请求，收到请求后立刻回复，之后CPU再异步处理这些请求，失效队列的引入导致线程读到**“本应该失效却还没有失效”的脏数据**）


**上面的写缓冲器和无效队列中的数据不能立马刷回高速缓存.会导致可见性问题和有序性问题.如何解决呢?**

有序性例子：
>第一个a=1是Store，第二个是b=c是Load。但是可能处理器对store操作先写入了写缓冲器，此时这个写操作相当于没执行，然后就执行了第二行代码，第二行代码的b是局部变量，那这个操作等于是读取a的值，是load操作  
这就导致好像第二行代码的load先执行了，第一行代码的store后执行  
第一个store操作写到写缓冲器里去了，导致其他的线程是读不到的，看不到的，好像是第一个写操作没执行一样；

写缓冲器和无效队列导致的原因：
- 写数据不一定立马写入自己的高速缓存（或者主内存），是因为可能写入了写缓冲器；  
- 读数据不一定立马从别人的高速缓存（或者主内存）刷新最新值过来，invalidate消息在无效队列里面,有可能获得的值还是高速缓存和主内存的旧值.

可见性问题也是一样的，写入写缓冲器之后，没刷入高速缓存，导致别人读不到；读数据的时候，可能invalidate消息在无效队列里，导致没法立马感知到过期的缓存，立马加载最新的数据


写缓存器的引入使得指令看起来是乱序执行的——写缓冲器和本地缓存行的数据是不一致的。

另一方面，机器指令本身也会被处理器重排序，因为CPU无法确定多线程环境下哪些变量具有相关性（**只能保证单线程情况下，重排序不会影响最终结果**），但是CPU设计者提供了内存屏障供程序员规范CPU行为。

>内存屏障是CPU设计者为程序员提供的一组方法，可以约束CPU的行为，不同的CPU具有不同的内存屏障，相当于为程序员提供相应工具，将保证线程安全的责任交给程序员。  
程序员通过使用内存屏障，告诉CPU哪些部分不应该被（重排序）优化，底层就是通过临时禁用失效队列、写缓冲器等实现的。



### lock前缀
volatile、CAS（synchronized底层也是基于CAS上锁的）被编译为汇编指令后（即时编译器），都会在相应指令前增加一个lock前缀，lock前缀正是这些关键字实现有序性和可见性的基础。
LOCK前缀在多核处理器中引发两件事
【1】让当前处理器缓存行的数据回写入主存
【2】其他核心维护该变量相应的缓存行过期/无效，下次取需要从主存中获取

CPU为了提升效率，通过增加高速缓存来缓解读写内存造成的（CPU计算和访存之间的）速度差，而告诉缓存的最小单位是缓存行，因此CPU读数据都是一块块读的，多核处理机中，每个核心都有自己独立的缓存（L1/L2），各个核心通过总线连接在一起，并且嗅探总线上信号，为了保证各个核心缓存行中的数据都是一致的，有两种解决思路（这也是lock前缀执行上的，两种实现方式）。
【1】锁总线
执行指令期间，核心发出Lock信号，总线仲裁机构该核心独占总线，而其他核心必须等待，代价很大，非主流方案。
【2】锁缓存，而且缓存之间需要遵守一个缓存一致性协议
各个CPU核心都是通过总线连接在一起的，每个核心都维护自己缓存的状态，一旦某个核心修改了自己缓存的内容，就会通过总线向其他核心发出信号，其他核心根据MESI协议修改相应的缓存行状态。

Lock前缀的汇编指令会强制写入主存，也可以避免前后指令的CPU重排序，并且及时让其他核心中的相应缓存行失效（从而利用MESI达到符合预期的效果）。
非lock前缀的汇编指令执行写操作时，可能不会立刻生效，因为存在写缓存区，lock前缀的指令在功能上可以等价内存屏障，让写操作立刻生效（或者说jvm插入内存屏障，平台通过CPU指令实现对应效果）。

总结：为什么volatile、synchronized、CAS等能保证可见性、有序性，因为它们共同的底层实现lock前缀，满足了MESI缓存一致性协议的触发条件，才使得变量具有缓存一致性。而普通的读写涉及各种优化，如写缓存、失效延迟处理等导致MESI条件无法触发，进而产生一系列数据不一致的问题。


#### 特点
这里我们讨论API层面的CAS。因为CAS可以分为CAS修改锁变量和CAS乐观锁，我们这里讨论CAS乐观锁，而这里乐观锁指代CAS自旋乐观锁。
在竞争不激烈的情况下，CAS可以提高系统的吞吐量——说白了，就是在一段时间内，让CPU多执行用户代码，少执行操作系统代码如（系统调用、切换上下文等）。如果竞争特别激烈，或者同步代码执行时间特别长，那么就使用自旋CAS乐观锁就是白白浪费CPU资源——虽然CPU一直在执行用户代码，但是执行循环啥也不干，还不如把CPU让给别人呢——这种情况不如主动申请阻塞、转让CPU给其他线程。
>当多个核心针对同一内存地址指向CAS指令时，其实他们是在试图修改每个核心自己维护的缓存行，假如两个核心同时同时对同一内存地址执行CAS指令，则他们都会尝试向其他核心发出invalidate，仲裁获胜的核心将先一步发出invalid，失败者则需要对自己的缓存行invalidate，读取胜利者修改后的内存值，CAS指令执行失败。
>因此**锁并没有消失，只是转嫁到了环总线上的总线仲裁协议上，而多核同时针对一个地址CAS会导致对应的缓存行频繁失效，降低性能，因此CAS不能滥用**

另外，CAS指令提供的总是**一个变量的内存地址**，也就是说乐观锁只能CAS修改某一个变量的值——不如独占锁变量，想怎么修改就怎么修改来的爽快啊。
>也可以将多个变量包装为一个对象（结构体），通过JUC的atomicReference来实现。当然了，肯定还是加锁更方便



### ABA
ABA问题：
首先，CAS指令的三个参数实际上都是内存地址，比较两个内存地址的值，然后考虑要不要把第一个内存地址的值修改为第三个内存地址的值，既然涉及到寻址，那么两次寻址之间必然具有时间间隔，我们只能保证CAS指令执行是原子的，在CPU寻址过程中（三个地址的寻址过程），源地址上的值从A变成B，再变成A是可能的。

造成以上问题的主要原因，是因为我们使用CAS时的逻辑就是:**值相同，就交换**。如果我们的业务禁止ABA问题，我们完全可以将CAS的逻辑更改为：**值+时间戳或版本号 相同，则交换。**

ABA解决思路就是：CAS输入不但考虑值本身，还附带具有标识意义的字段。例如JUC的atomicStampedReference（额外维护了一个时间戳）、mysql可以维护一个version字段（mybatis plus 提供了乐观锁功能，本质上就是维护额外版本号）

_总结：  
造成ABA问题的不是CAS指令本身，因为它只是一个原子指令，出现ABA问题不是执行CAS的时候，而是CPU为CAS指令加载值的过程中。_

```markdown
既然有MESI协议可以是保证cache一致性，为什么还需要volatile来保证可见性（内存屏障）？
有人回答，MESI协议是需要触发的，真的是这样吗？

volatile使用内存屏障来解决write buffer和invalidate queue带来的问题
首先volatile不单单能够保证可见性，还能保证有序性。此外，缓存一致性只作用于缓存，即L1、L2、L3 cache，不能作用于寄存器。

那是因为cpu写缓冲器不可见，以及无效队列的原因，你可以去了接下相关内容，cpu等待无效，和更新主存是写到写缓冲器的，至于执行时机是需要等到收到其他核心无效队列执行结果才会写入的，cpu把执行写入缓存指令，和无效队列中指令的时机交给开发者。volatite通过读写内存屏障保证了那一块过程的一致性




你好博主，有个缓存一致性的问题想向您请教一下。比如这样一个场景：有一个变量a=0；有两个线程同时在多核环境下去执行a++去更改这个变量。 按照缓存一致性是不是应该是这样一个过程：两个cpu都把a读到缓存，此时a是一个S的状态，现在两个cpu都要去修改a，但是只能有一个cpu（假设是cpu1）把a修改为M状态，另一个会变为I（失效状态），当cpu2再要去执行a++的时候，会去主存读取a(因为缓存中的a已经是失效状态)，这就会先触发cpu1缓存中的M状态的a（此时a=1）写回主存，然后cpu2才会读取到主存中a=1到缓存，这时两个cpu缓存中的a都变成S状态，然后cpu2再去做修改。---------过程如果是这样的话，最后输出的a一定是2了（测试结果并不是-。-）。我理解的这个过程什么地方有问题啊？


缓存一致性和原子性无关。a++操作由三个步骤组成：读a，加1，写a，如果两个线程同时执行a++，那么可能的顺序是：线程1执行读a，线程1执行加1，线程2执行读a，线程2执行加1，线程1写a，线程2写a，结果a=1。上述过程中，缓存一致性始终生效，但由于a++的非原子性，导致线程不安全。
MESI协议只是保证每个缓存中使用的共享变量的副本是一致的，而不会保证程序运行的原子性和内存可见性，你这种情况是需要考虑程序并发运行过程中的原子性和可见性问题，比如使用synchronized、lock、volatile等
```