# 线程安全问题出现的根本原因
程序最开始是静态的、原子的、顺序执行的，虽然CPU利用率不高（CPU总是能有时间偷懒），但是程序员需要操心的问题不多。为了提升效率，陆续引入进程和线程的概念，并且实现了相应的数据结构。程序变成了动态的、并发的、异步的、执行也不再是原子的了。CPU利用率上去了，程序员要操心的事情多了…

CPU缓存的作用主要是为了解决CPU运算速度与内存读写速度不匹配的矛盾

线程安全问题的根源可以被归纳为三点：原子性问题、可见性问题和有序性问题。

## 原子性
由于存在线程和进程切换，我们无法做到一气呵成的执行某段程序，因为总是存在无法避免的中断产生。这倒没什么，因为如果它们的数据如果是私有的，这种切换不会造成什么问题，肯定不会存在“一会儿聊QQ一会儿聊微信，然后QQ发出的消息跑到微信上了”，**真正导致线程安全问题的是：多个线程/进程操作的不是它们私有的数据，而是共享数据！！！如果一个线程修改共享数据不是原子的，那么其他线程就会读到“改到一半”的数据——不是原子性，就意味着访问共享变量的操作不能被称之为一个事务，可能会产生读写冲突（脏读）、写写冲突（修改丢失）等**

## 可见性
**CPU访问内存需要的时钟周期，和CPU执行一条指令的时钟周期相比实在是太慢了，引入高速缓存就是为了弥补这种速度差，要求CPU减少直接访存的次数。
这虽然提升了CPU读取数据的效率，但是引入了可见性问题，每个核心都有自己的缓存（L1/L2 ）**，因为多核处理器时代中，每个核心具有独立的缓存，这导致核心之间缓存同一共享变量的值可能是不一致的。

## 有序性
编译器和处理器都会对不同层次的指令进行重排序，来加速指令执行效率（如一次性执行所有的读操作后，再统一执行一次写操作），重排序优化的原则只能保证不影响单线程的执行结果，但是可能使多线程程序执行结果出现问题。


# Java内存模型
因为不同的平台具有不同的CPU、缓存、操作系统等，JVM为了实现跨平台性，制定了统一规范，这其中就包含内存模型JMM。
>JVM本质上是一组规范，是一堆接口，而不同的平台去实现这组规范，最终达成不同平台但功能一致的特点

JMM定义了线程如何访问共享变量的规则：
所有的共享变量存储在主内存中，而每个线程都具有独立的工作内存，各线程只能自己的工作内存操作变量，而且线程之间不能直接访问对方的工作内存，工作内存存放的主内存的副本，线程之间如果需要传递变量值，必须依靠主内存。
（不是操作共享变量，那就不存在线程安全问题了，没有讨论必要）
>JMM就是对CPU-高速缓存-主存关系的抽象。JMM不管你的CPU和内存条是AMD还是Intel，它提供一个统一的视图，你用java写程序，就不要考虑操作系统和CPU那一套东西了，就看我给你提供内存模型JMM就可以了。

总结：JMM屏蔽了底层内存模型的差异与细节，使得java程序在各个不同底层软件和硬件平台上，都可以达到一致的内存访问效果。


## JMM中的原子/可见/有序问题
既然JMM让我们“眼里只有它”，那么我们就来分析一下包上一层外衣之后，java程序存在哪些线程安全问题，依然是之前的三条：
【1】线程切换，访问共享变量的一组操作无法被看作一个事务
【2】由于线程只和自己的工作内存打交道，而且无法直接访问其他线程的工作内存，因此存在数据不一致问题
【3】编译器会对程序进行重排序优化，而且仅保证as-if-serial

>这里先说个结论：synchronized、Lock基于AQS的实现类具有原子、可见、有序性，而volatile具有可见、有序性，而且它只能做到“无状态”的单条语句读写的原子性（如double d =2.2，但是涉及对个变量的读写如i=i+1无法保证）


### as-if-serial原则
CPU重排序可以在保证程序线性执行结果准确性的前提，对性能进行优化，而JMM则将这项保证一项JMM规则提供给java程序员——**保证java程序单线程下总是能得到正确的结果，但是多线程就无法保证了**。（对于未正确同步的程序，JMM只提供**最小的安全性保证**：读取的值可能会出错，但是不是无中生有的，程序员可以简单推断出多线程的异步执行如何导致错误结果产生。）


### happens-before规则

happens-before是JMM的核心，之所以设计happens-before，主要出于以下两个方面的因素考虑的：1）程序员的角度，JMM内存模型需要易于理解、易于编程；2）编译器和处理器的角度，编译器和处理器希望内存模型对其束缚越少越好，这样就可以根据自己的处理规则进行优化。但是这两个方面其实是相互矛盾的，因为JMM易于编程和理解就意味着对编译器和处理器的束缚就越多。


happens-before定义
基于上面的考虑，设计JMM时采用了一种折中的选择——JMM将需要禁止的重排序分为两类（因为编译器和处理器的优化大部分是重排序，所以JMM的处理的关键也就是重排序了）：

会改变程序执行结果的重排序
不会改变程序执行结果的重排序
对应这两种情况，JMM采用了不同的策略：

对于会改变程序执行结果的重排序，JMM要求编译器和处理器必须禁止这种重排序
对于不会改变程序执行结果的重排序，JMM对编译器和和处理器不做任何要求（自然，编译器和处理器可以其进行重排序）

所以JMM的设计基于这样一种原则：先保证正确性，在考虑执行效率问题。

说了这么多，与happens-before原则有什么关系呢？从上面可以看到JMM实际上可以看做是操作之间的约束模型，这种约束模型的实现就是我们要提到的happens-before了。happens-before**用来指定两个操作之间的执行顺序**，这两个操作可以在一个线程之内也可以在不同的线程中，所以这种对操作顺序的关系的界定可以为程序员提供内存可见性的保证。具体happen-before的定义如下：
>1）如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前
  2）两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。

```markdown
《JSR-133:Java Memory Model and Thread Specification》对happens-before关系的定义如下：

1）如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。注意：这一点仅仅是JMM对程序员的保证

2）两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）。
```

上面第二句话的意思就是说，如果重排序之后的执行结果与按照原来那种happens-before关系执行的结果一致，那么JMM允许编译器和处理器进行这种重排序。所以可以认为：**只要不改变程序的执行结果，编译器和处理器可以随意优化**。

**联系之前提到的as-if-serial语义（保证单线程内的程序执行结果不会改变），现在提到的happens-before则保证正确同步的多线程的执行程序的执行结果不会发生改变。**

```markdown
A happens-before B并不代表A一定先于B发生。只是保证如果有共享内存，A改变后B立即可见。i=1:j=2;同一个线程中的这两条指令根据程序顺序原则:i=1 happens-before j=2意思就是说:如果i=1先执行(如果两字很重要)，那么执行j=2时就能看到i已经改变了。但是由于这两条指令相互没有任何影响，可以进行重排序，所以不一定先执行i=1这条指令。即hapens-before与操作的执行顺序完全是两码事，只不过对于:i=2;j=i+i; **因为程序判定两条指令之间有依赖关系从而不会重排序而已**

```


程序顺序规则：一个线程中的每个操作，happens-before于随后该线程中的任意后续操作
监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的获取
volatile变量规则：对一个volatile域的写，happens-before于对这个变量的读
传递性：如果A happens-before B，B happens-before C，那么A happens-before C
start规则：如果线程A执行线程B的start方法，那么线程A的ThreadB.start()happens-before于线程B的任意操作
join规则：如果线程A执行线程B的join方法，那么线程B的任意操作happens-before于线程A从TreadB.join()方法成功返回。
1.  对线程interrup()方法的调用， happens-before 于被中断线程的代码检测到中断事件的发生
2.  一个对象的初始化完成（构造函数执行结束）happens-before finalize()方法的开始

看看start规则，假设这样一种情况，如果线程A在执行线程B的start方法之前修改了一些共享变量的值，那么当线程B执行start方法的时候，会去读取这些修改的共享变量的值（上面规则就是这么规定的），这就意味着线程A对共享变量的修改对线程B可见

下面看看join规则是怎么回事。join方法的本义是等待当前执行的线程终止。假设在线程B终止之前，修改了一些共享变量（完全可能啊），线程A从线程B的join方法成功返回后，就会读取这些修改的共享变量。这样也保证了线程B对共享变量的修改对线程A是可见的。







CPU设计者并不会保证程序的线程安全，但是提供了一些内存屏障给程序员，程序员可以使用内存屏障约束CPU某些行为——告诉CPU，哪些指令、哪些位置不需要进行优化。
内存屏障可以用于解决可见性和有序性问题，编译器会根据用户的程序，在适当的位置插入内存屏障，程序员不需要了解复杂的内存屏障指令，JMM通过happens-before原则告诉程序员“JMM为哪些地方可以做出线程安全保障”

>程序员根据happens-before原则指导，使用volatile、lock、start()等，编译器会在指令的周围插入内存屏障，最终内存屏障在底层会被翻译为附带lock前缀的汇编指令，来保证相应的同步语义。
说白了，底层很复杂，JMM屏蔽了底层是如何实现同步语义的，向上提供一组规则，程序员在规则指导下正确地使用相应的关键字和方法，来达到编写正确同步的多线程程序的目的

happens-before提供一组规则，如果A操作在B操作之前发生（即使两个操作来自不同的线程），那么执行操作B时，A操作中做出的改变B操作都是可以看见的，而且底层不会对这两个操作进行重排序——A对B可见。可以看作**JMM就重排序和可见性对程序员提供的保证**






### 理解同步和互斥(这就是为什么明明存在线程调度,但synchronized锁住的代码块逻辑上却是原子的)
同步就是步调一致，我在缓存区中修改了变量值，你也应该把你那一份和我的数据保证一种，线程同步就是让两个线程保持的信息一致。
如果一个现在正在执行某一组同步操作，那么这执行中途就不能被其他线程打断，如果线程被打断，另一线程读到“脏数据即不同步的数据”，那么两个线程同时执行完毕同步块的内容，最终两个线程的数据将是不一致的即不同步的。
因此访问同步代码时，线程需要互斥地访问，同一时间只能有一个线程正在执行同步块的内容。

>讨论java中的同步块，那么这个切换仅限于java线程，因为java层次无法阻止操作系统层次的线程进行切换和调度，有上层进行了同步，即使操作系统层面发送线程切换，被切换到的线程没有获取到锁于是进入就绪队列，仍然不影响上层同步块的语义。




# 理解volatile
volatile易变的。从语义上理解，volatile对某个变量声明为“易变的”，每个线程总是从主存中拿到最新的数值而不是从读取工作缓存中的，而每个线程修改完这个变量后也将不会保存这个易变的变量，而是写入主存。达到的效果就是——volatile修饰的变量，总是在线程之间可见

## volatile的使用
volatile是java提供的一种**轻量级的线程通信方案**，可以**代替锁保证线程对变量的可见性**，可以配合CAS指令实现乐观锁或悲观锁。**AQS框架底层就是基于读写volatile实现可见性语义的。**
>volatile读写的内存语义是从JSR-133(JDK5)加强之后才具有的，volatile读写与锁的获取/释放具有相同的内存语义

## volatile的特性
volatile两大特性：
【1】保证变量的内存可见性
【2】禁止重排序

内存语义：
【1】当写一个volatile变量时，JMM会将线程保存在本地内存的副本刷新入主内存，并使其他线程的变量副本无效化。
【2】当读一个volatile变量时，从主存中读取最新值。

注意：
volatile不是线程安全的，因为只能保证可见和有序，无法保证原子性。
volatile可以保证类似i=1读写的原子性，但是不能保证“存在关联关系”的原子性如i=i+1、a=b+c
>对于任意单个volatile变量的读/写具有原子性，即使是64位的long和double

# 琐碎

## MESI(缓存一致性)协议






### MESI协议介绍

缓存一致性：在多核CPU中，内存中的数据会在多个核心中存在数据副本，某一个核心发生修改操作，就产生了数据不一致的问题。而一致性协议正是用于保证多个CPU cache之间缓存共享数据的一致。


#### cache的写方式
MESI采取的是写失效
>写失效：当一个CPU修改了数据，如果其他CPU有该数据，则通知其为无效；



#### cache line

cache line是cache与内存数据交换的最小单位，根据操作系统一般是32byte或64byte。在MESI协议中，状态可以是M、E、S、I，地址则是cache line中映射的内存地址，数据则是从内存中读取的数据。

工作方式：当CPU从cache中读取数据的时候，会比较地址是否相同，如果相同则检查cache line的状态，再决定该数据是否有效，无效则从主存中获取数据，或者根据一致性协议发生一次cache-to--chache的数据推送（参见MESI协议，文章最后的链接）；

工作效率：当CPU能够从cache中拿到有效数据的时候，消耗几个CPU cycle，如果发生cache miss，则会消耗几十上百个CPU cycle；


#### 状态介绍
MESI协议将cache line的状态分成modify、exclusive、shared、invalid，分别是修改、独占、共享和失效。

modify：当前CPU cache拥有最新数据（最新的cache line），其他CPU拥有失效数据（cache line的状态是invalid），虽然当前CPU中的数据和主存是不一致的，但是以当前CPU的数据为准；

exclusive：只有当前CPU中有数据，其他CPU中没有改数据，当前CPU的数据和主存中的数据是一致的；

shared：当前CPU和其他CPU中都有共同数据，并且和主存中的数据一致；

invalid：当前CPU中的数据失效，数据应该从主存中获取，其他CPU中可能有数据也可能无数据，当前CPU中的数据和主存被认为是不一致的；

对于invalid而言，在MESI协议中采取的是写失效（write invalidate）。

#### cache操作
MESI协议中，每个cache的控制器不仅知道自己的操作（local read和local write），每个核心的缓存控制器通过监听也知道其他CPU中cache的操作（remote read和remote write），今儿再确定自己cache中共享数据的状态是否需要调整。

local read（LR）：读本地cache中的数据；

local write（LW）：将数据写到本地cache；

remote read（RR）：其他核心发生read；

remote write（RW）：其他核心发生write；

  
#### 状态转换和cache操作
如上文内容所述，MESI协议中cache line数据状态有4种，引起数据状态转换的CPU cache操作也有4种，因此要理解MESI协议，就要将这16种状态转换的情况讨论清楚。

初始场景：在最初的时候，所有CPU中都没有数据，某一个CPU发生读操作，此时必然发生cache miss，数据从主存中读取到当前CPU的cache，状态为E（独占，只有当前CPU有数据，且和主存一致），此时如果有其他CPU也读取数据，则状态修改为S（共享，多个CPU之间拥有相同数据，并且和主存保持一致），如果其中某一个CPU发生数据修改，那么该CPU中数据状态修改为M（拥有最新数据，和主存不一致，但是以当前CPU中的为准），其他拥有该数据的核心通过缓存控制器监听到remote write行文，然后将自己拥有的数据的cache line状态修改为I（失效，和主存中的数据被认为不一致，数据不可用应该重新获取）。

  
##### modify
场景：当前CPU中数据的状态是modify，表示当前CPU中拥有最新数据，虽然主存中的数据和当前CPU中的数据不一致，但是以当前CPU中的数据为准；

LR：此时如果发生local read，即当前CPU读数据，直接从cache中获取数据，拥有最新数据，因此状态不变；

LW：直接修改本地cache数据，修改后也是当前CPU拥有最新数据，因此状态不变；

RR：因为本地内存中有最新数据，当本地cache控制器监听到总线上有RR发生的时，必然是其他CPU发生了读主存的操作，此时为了保证一致性，当前CPU应该将数据写回主存，而随后的RR将会使得其他CPU和当前CPU拥有共同的数据，因此状态修改为S；

RW：同RR，当cache控制器监听到总线发生RW，当前CPU会将数据写回主存，因为随后的RW将会导致主存的数据修改，因此状态修改成I；

##### exclusive
场景：当前CPU中的数据状态是exclusive，表示当前CPU独占数据（其他CPU没有数据），**并且和主存的数据一致**；

LR：从本地cache中直接获取数据，状态不变；

LW：修改本地cache中的数据，状态修改成M（因为其他CPU中并没有该数据，因此不存在共享问题，不需要通知其他CPU修改cache line的状态为I）；

RR：本地cache中有最新数据，当cache控制器监听到总线上发生RR的时候，必然是其他CPU发生了读取主存的操作，而RR操作不会导致数据修改，因此两个CPU中的数据和主存中的数据一致，此时cache line状态修改为S；

RW：同RR，当cache控制器监听到总线发生RW，发生其他CPU将最新数据写回到主存，此时为了保证缓存一致性，当前CPU的数据状态修改为I；

##### shared
场景：当前CPU中的数据状态是shared，表示当前CPU和其他CPU共享数据，且数据在多个CPU之间一致、多个CPU之间的数据和主存一致；

LR：直接从cache中读取数据，状态不变；

LW：发生本地写，并不会将数据立即写回主存，而是在稍后的一个时间再写回主存，因此为了保证缓存一致性，当前CPU的cache line状态修改为M，并通知其他拥有该数据的CPU该数据失效，其他CPU将cache line状态修改为I；

RR：状态不变，因为多个CPU中的数据和主存一致；

RW：当监听到总线发生了RW，意味着其他CPU发生了写主存操作，此时本地cache中的数据既不是最新数据，和主存也不再一致，因此当前CPU的cache line状态修改为I；

  
##### invalid
场景：当前CPU中的数据状态是invalid，表示当前CPU中是脏数据，不可用，其他CPU可能有数据、也可能没有数据；

LR：因为当前CPU的cache line数据不可用，因此会发生读内存，此时的情形如下。

	A. 如果其他CPU中无数据则状态修改为E；

	B. 如果其他CPU中有数据且状态为S或E则状态修改为S；

	C. 如果其他CPU中有数据且状态为M，那么其他CPU首先发生RW将M状态的数据写回主存并修改状态为S，随后当前CPU读取主存数据，也将状态修改为S；

LW：因为当前CPU的cache line数据无效，因此发生LW会直接操作本地cache，此时的情形如下。

	A. 如果其他CPU中无数据，则将本地cache line的状态修改为M；

	B. 如果其他CPU中有数据且状态为S或E，则修改本地cache，通知其他CPU将数据修改为I，当前CPU中的cache line状态修改为M；

	C. 如果其他CPU中有数据且状态为M，则其他CPU首先将数据写回主存，并将状态修改为I，当前CPU中的cache line转台修改为M；

RR：监听到总线发生RR操作，表示有其他CPU读取内存，和本地cache无关，状态不变；

RW：监听到总线发生RW操作，表示有其他CPU写主存，和本地cache无关，状态不变；

  
#### 总结
MESI协议为了保证多个CPU cache中共享数据的一致性，定义了cache line的四种状态，而CPU对cache的4种操作可能会产生不一致状态，因此cache控制器监听到本地操作和远程操作的时候，需要对地址一致的cache line状态做出一定的修改，从而保证数据在多个cache之间流转的一致性。

  
### 琐碎

[关于volatile、MESI、内存屏障、#Lock (360doc.com)](http://www.360doc.com/content/20/0907/19/835902_934447060.shtml)
MESI协议规定：对一个共享变量的读操作可以是多个处理器并发执行的，但是如果是对一个共享变量的写操作，只有一个cpu可以执行，其实也会通过排他锁的机制保证就一个处理器能写  

MESI协议规定了一组消息，各个cpu在操作内存数据的时候，都会往总线发送消息，而且各个cpu还会不停的从总线嗅探最新的消息，通过这个总线的消息传递来保证各个cpu的协作  

上面的特性保证了可见性和有序性问题.(但是排他锁的机制降低了性能)

在MESI协议中，每个Cache line有4个状态，可用2个bit表示，它们分别是：
```markdown
（1）invalid：无效的，标记为I，这个意思就是当前cache entry无效，里面的数据不能使用
 
（2）shared：共享的，标记为S，这个意思是当前cache entry有效，而且里面的数据在各个处理器中都有各自的副本，但是这些副本的值跟主内存的值是一样的，各个处理器就是并发的在读而已
 
（3）exclusive：独占的，标记为E，这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中。
 
（4）modified：修改过的，标记为M，只能有一个处理器对共享数据更新，所以只有更新数据的处理器的cache到内存中，才是exclusive状态，表明当前线程更新了这个数据，这个副本的数据跟主内存是不一样的

```


CPU缓冲区为了保证数据一致性，遵循MESI缓存一致性协议，某个CPU更新数据时，若数据状态为S，那么需要invalidate消息到总线，尝试让其他的处理器的该数据对应的cache line状态变为I .得到其他CPU确认信号后才会进行写缓冲区操作.
其他的cpu会从总线嗅探到invalidate消息，若缓存中存在对应的cache line，就将其状态设为I,然后返回invalidate ack消息到总线
**修改数据的cpu要修改的数据对应的 cache line状态设为 E，这样在独占期间别的cpu发出 invalidate消息时，该cpu是不会返回 invalidate ack消息的，这就保证了有序性**
当cpu修改完这条数据后，将其状态设置为M，或是将数据强制写回主内存中。
然后其他cpu此时这条数据的状态就都是I了，如果需要读的话，就要重新发送read消息，从主内存（或其他处理器）来加载


**旧版本的MESI存在的串行化问题**
可能存在性能问题.上面的机制相当于串行执行写数据了.如果每次写数据的时候都要发送invalidate消息等待所有处理器返回ack，然后获取独占锁后才能写数据，那可能就会导致性能很差了，因为这个对共享变量的写操作，实际上在硬件级别变成串行的了
**如何解决呢? 硬件层面引入了写缓冲器和无效队列**

写缓冲器：将数据先写入写缓冲器，同时发送 invalidate消息，就可以去干别的事，不会阻塞在这里，收到其他cpu的ack消息后，再将数据取出并写入本地缓存。
（引入写缓冲器后，一个写指令发出后，放入缓冲器后就直接往下执行了，也就是说写操作不是立即生效的）
查询数据的时候，会先从写缓冲器里查，因为有可能刚修改的值在这里，然后才会从高速缓存里查，这个就是存储转发,同样提升了效率

无效队列：引入无效队列,其他处理器在接收到了invalidate消息之后，不需要立马过期本地缓存，直接把消息放入无效队列，就返回ack给那个写处理器了，这就进一步加速了性能，然后之后从无效队列里取出来消息，过期本地缓存即可

【1】对于收到的所有invalidate请求，必须立即返回确认
【2】invalidate并不会真正执行，而是放入失效队列，在方便的时候才回去执行
【3】处理器不会发送任何消息给所处理的缓存条目，直到处理invalidate请求
（相当于使用一个队列，临时存储invalidate请求，收到请求后立刻回复，之后CPU再异步处理这些请求，失效队列的引入导致线程读到**“本应该失效却还没有失效”的脏数据**）


**上面的写缓冲器和无效队列中的数据不能立马刷回高速缓存.会导致可见性问题和有序性问题.如何解决呢?**

有序性例子：
>第一个a=1是Store，第二个是b=c是Load。但是可能处理器对store操作先写入了写缓冲器，此时这个写操作相当于没执行，然后就执行了第二行代码，第二行代码的b是局部变量，那这个操作等于是读取a的值，是load操作  
这就导致好像第二行代码的load先执行了，第一行代码的store后执行  
第一个store操作写到写缓冲器里去了，导致其他的线程是读不到的，看不到的，好像是第一个写操作没执行一样；

写缓冲器和无效队列导致的原因：
- 写数据不一定立马写入自己的高速缓存（或者主内存），是因为可能写入了写缓冲器；  
- 读数据不一定立马从别人的高速缓存（或者主内存）刷新最新值过来，invalidate消息在无效队列里面,有可能获得的值还是高速缓存和主内存的旧值.

可见性问题也是一样的，写入写缓冲器之后，没刷入高速缓存，导致别人读不到；读数据的时候，可能invalidate消息在无效队列里，导致没法立马感知到过期的缓存，立马加载最新的数据


写缓存器的引入使得指令看起来是乱序执行的——写缓冲器和本地缓存行的数据是不一致的。

另一方面，机器指令本身也会被处理器重排序，因为CPU无法确定多线程环境下哪些变量具有相关性（**只能保证单线程情况下，重排序不会影响最终结果**），但是CPU设计者提供了内存屏障供程序员规范CPU行为。

>内存屏障是CPU设计者为程序员提供的一组方法，可以约束CPU的行为，不同的CPU具有不同的内存屏障，相当于为程序员提供相应工具，将保证线程安全的责任交给程序员。  
程序员通过使用内存屏障，告诉CPU哪些部分不应该被（重排序）优化，底层就是通过临时禁用失效队列、写缓冲器等实现的。







# 并发基础理论：缓存可见性、MESI协议、内存屏障、JMM
## CPU缓存导致的可见性问题
[并发基础理论：缓存可见性、MESI协议、内存屏障、JMM - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/84500221)

我们现在都知道了CPU加缓存都是为了提升CPU的利用率， 但是也衍生了一个问题，多核CPU的情况下，每个CPU都有着自己独立的缓存，它们各自之间是不可见的，这就会导致对应CPU读取的数据都是自己缓存的，无法看到别人对共享数据的修改，从而导致并发BUG。

**缓存一致性问题的思考**

导致缓存一致性问题的核心主要是两个问题：

问题一： 在一个CPU修改了内存数据的时候，其它CPU是不知道的，所以导致一个CPU改了，另外一个CPU看不见，从而使用了旧的数据，导致了程序不正确的结果。

问题二： 在多个CPU同时读取和修改CPU的时候，如何保证这几个CPU操作的顺序性，一旦不能保证整个修改操作的顺序，那么就可能导致先写后读的两个请求，结果反映到内存就成了先读后写的结果，从而没有读取到最新的数据，又或者两个写数据的请求顺序被调换了，那么就可能会造成脏写。


## 基于总线的一致性解决方案

CPU要和存储设备进行交互，必须要通过总线设备，在获取到总线控制权后才能启动数据信息的传输，而CPU要想从主存读写数据，那么就必须向总线发起一个总线事务（读事务或写事务）来从主存读取或者写入数据。

**总线嗅探**

缓存一致性的第一个问题在于，在多CPU缓存的情况下，一个CPU修改了主存的共享变量，其它CPU是不知道的，所以解决这个问题最直接的办法就是**使用通知机制**，当一个CPU修改了主存的数据时，其它CPU都会收到相应的数据变更通知，收到通知的CPU如果发现自己也缓存了对应的数据，那么就会将自己缓存的数据所在缓存行标记为失效，当下次读取该数据时发现自己的缓存行已过期，那么就会选择从主存加载最新的数据。 而实现这个功能的机制就叫“总线嗅探”，总线嗅探是通过CPU侦听总线上发生的数据交换操作，当总线上发生了数据操作，那么总线就会广播对应的通知。


**总线仲裁**

导致缓存不一致的另外一个问题在于，**CPU操作共享数据的顺序性**，想让并发的操作变得有序，那么常用的方式就是**让操作的资源具备独占性**，这也就是我们常用的的方式加锁，当一个CPU对操作的资源加了锁，那么其它CPU就只能等待，只有等前一个释放了锁（资源占用权），后面的才能获得执行权，从而保证整体操作的顺序性。

而实现这个机制的功能就叫“总线仲裁”，在多个CPU同时申请对总线的使用权时，为避免产生总线冲突，需由总线仲裁来合理地控制和管理系统中需要占用总线的申请者，在多个申请者同时提出总线请求时，以一定的优先算法仲裁哪个应获得对总线的使用权。


### 总线机制的性能问题

完全基于总线来保证缓存数据的一致性虽然简单可行，但是却有很大的性能瓶颈，为了保证数据的正确性，发生在总线上的事务操作必须是排它顺序执行的，这也就造成了只要一个CPU占用了总线，那么其它的CPU就无法与主存进行通讯而只能等待前一个执行完成，总线资源的这种独占性我们也常称为总线锁，而基于总线加锁的波及范围太大,所以存在很大的性能问题。




## 总线性能问题优化方案

总线性能瓶颈在于基于在总线与主存打交道会造成阻塞，那么反过来想如果不通过总线与内存发生数据交互就可以避免总线加锁，所以优化这个问题的核心在于如何减少必须通过总线与主存交互的操作，换而言之就是“如非必要，就不要通过总线与主存打交。

至于如何减少CPU通过主线和主存打交道的次数，这里可以分为两个方向，一方面尽量避免通过主线从主存读取的请求。另一个方面是减少修改数据后而把数据同步到主存的频率。


### 减少从主存读取数据频率

减少从主存读取数据频率的核心思想在于：CPU读取一个自己缓存没有的数据时，不是直接向主存读取，而是优先从其它已经缓存了对应数据的CPU缓存获。

当一个CPU读取数据时，首先从自己的缓存里面读取对应的缓存行，如果此时自己的缓存里面没有，那么它会向总线发起一个读取事务， 此时其它CPU会收到一个来自总线读取的消息，如果其它CPU的缓存有持有对应缓存行的数据时，它会把缓存行的地址放到总线上， 那么读取数据的CPU只需要通过地址拷贝对应的缓存行到自己的CPU缓存即可。

### 减少数据同步主存频率

如果当前数据只有自己一个人缓存了，那么就不存在多个CPU缓存的一致性问题， 所以无论当前CPU修改多少次当前缓存行的值，也不会影响到其他人，所以这种情况下的数据变更可以不必马上同步到主存去，而只有在其他CPU也需要读取对应的数据时候，那么此时数据就会由一个人独占变成共享了，可以这个时候再把数据同步到主存去。



## MESI协议

总线性能的优化思路已经有了，那么就需要为这个思路指定解决具体的实时方案了，读取数据的时候如果要从其它CPU缓存获取数据的话，首先得知道其他CPU有没有这个数据，然后其他CPU的这个数据又是不是最新的，同样修改数据的时候，又如何知道其他CPU有没有缓存同样的数据，只有在确认大家都没有的时候，我才能放心的修改数据并且不用考虑同步到主存上去，但是CPU之间有没办法直接通信，所以所以它们之间就必须商定一套机制，**如何通过自己的数据状态就能知道其他CPU的缓存情况**，**从而做出对应的策略，而这套机制就是缓存一致性协议。**

缓存一致性协议有多种，有MSI、MESI、MESOI，大家总体的思路是一样的，不同的是后面的协议通过增加了某些状态，从而在某些场景能进一步减少通过总线与主存打交道的操作，我们这里来了解的是其中比较出名的MESI。


### MESI状态描述

了解了MESI协议的核心目的之后，我们再来看MESI对应的几个状态所代表的意思。MESI协议通过对共享数据进行不同状态的标识，来决定CPU何时把缓存的数据同步到主存，何时可以从缓存读取数据，何时又必须从主存读取数据。MESI 每个字母就代表着一种数据状态，分别是Modified 、Exclusive 、Share 、Invalid，每个CPU读取共享数据之前先要识别数据的对象状态，然后根据这几个状态分别执行不同的策略，下面我们分别了解下每个状态所代表的含义。

[并发基础理论：缓存可见性、MESI协议、内存屏障、JMM - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/84500221)


**状态的转换**

**从Share到 Modified:** 当前缓存行需要修改处于Share状态的数据时，那么首先会发送一条Invalid指令给其他缓存了对应缓存的CPU，直到其他CPU都响应了Invalid ack指令后，再对数据进行修改，此时数据状态会由Share变为Modified。

**从Share到 Invalid:** 其它CPU修改了当前缓存数据时，会收到一个对于该缓存行的Invalid指令，收到Invalid指令后会把缓存状态从Share变为Invalid，并响应这条Invalid指令的ACK，表示自己已经成功收到并除了Invalid指令。

### Modified （修改状态）

Modified是一个中间状态，处于Modified状态的数据说明该数据已经被修改，还没有同步到主存去，这个状态下的数据是最新的，并且其它CPU缓存都是Invalid状态。

1、当数据处于Modify状态缓存的数据是最新的，可以直接读取。

2、当数据处于Modify状态，那么说明其它CPU的要么没有缓存该变量，要么其它缓存都是Invalid（失效）状态，所以也可以直接修改，并且可以不用马上同步到主存。

3、当数据处于Modify状态，如果其他CPU需要读取该缓存行，那么在这个之前当前CPU必须把当前缓存行的数据同步到主存才行，避免其他CPU从主存读取到旧的值。


MESI 最终由4个状态表示着缓存数据处于的场景，在读取、修改操作时配合不同的数据状态，最终制定了一套大家可以协作保证数据一致性的机制。



## MESI性能优化空间
  
而在共享状态下，因为一个缓存行的数据在多个 CPU 核心的 Cache 里都有。所以，当我们想要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他 CPU 核心里面的 缓存行都变成无效的状态，等其他CPU都响应对于invalid 操作的ACK 后，修改数据的CPU才能更新当前 Cache 里面的数据。这个广播操作，一般叫作 RFO（Request For Ownership），也就是获取当前对应 Cache Block 数据的所有权，也就是所谓的缓存锁。

从广播指令，到收到所有其他CPU的ACK之后才能继续后面的操作，整个过程都处于阻塞状态，而对于CPU来说这个时间是很漫长的，所以就从两个方向进行了优化。一方面是在CPU等待其他CPU 回复的过程中可以去干一些其它的事情，所以就有了Store Buffere 。 另一方面是尽量缩短其他CPU回复invalid ack的时间，所以就有了Invalidate Queue。

### Store Buffere

修改数据的时候，必须先广播invalid指令给其它CPU，然后等其它CPU收到消息并把自己缓存行标记为失效，最后再响应ACK后，

再进行数据修改。 这个过程对于人来说时间很短，但对于CPU来说，可能相当于我们打开电饭煲开关，然后坐在那里等饭熟，既漫长，又无聊，而且还感觉很傻。 所以在等待饭熟（其它CPU响应ACK）的这段时间，何不去做点其它事呢，所以就有了store Buffere。**整个过程最耗时的就是等待所有cpu响应**

此时，CPU广播了通知之后，不再傻等着其它CPU回复了，而是把广播invalid指令发出去以后，然后直接把要修改的数据放到 Store Bufferes里，然后就去干其它事情了，当等到其他CPU都响应了ACK之后，然后再回头从Store Bufferes读取出来执行最后的数据修改操作。

  

### Store Forward(存储转发)

Store Bufferes 的确提高了CPU的资源利用率，不过优化了带来了新的问题，回到上面CPU修改数据的第一步，如果第一步完成了之后（这个时候数据还在strore Bufferes中，自己的缓存中还是旧值），如果此时CPU-1接到了一个读取a共享变量的指令，那么CPU这时候会从自己的缓存中去读取共享变量的数据，而当前缓存中的数据并不是最新的，那么这是一个很明显的问题。所以没办法，要解决这个问题就必须要求CPU读取数据时得先看Store Buferes里面有没有，如果有则直接读取Store Buferes里的值，如果没有才能读取自己缓存里面的数据，这也就是所谓的“Store Forward”。

### Invalidate Queue（失效队列）

从CPU广播，到其他CPU收到广播消息、到其他CPU标记自己的缓存行为invalid，到响应消息，这个过程最慢的一环在于CPU标记自己的缓存行为invalid的过程，尤其是CPU在执行其它指令的期间并不能马上来处理invalid的广播消息，所以就有了失效队列的优化。

当指令过多Store Buffere满了之后，那么新来的指令还是得同步发送指令给其他CPU

收到广播的CPU为了尽快响应 invalid ACK，所以就增加了一个失效队列，当收到其他CPU广播的invalid 消息后，不一定要马上处理，而是把放这个“失效队列里面”，然后就马上返回 invalid ack 。然后当自己有时间的时候再去处理失效队列里的消息，最后通过这种异步的方式，加快了CPU整个修改数据的过程。



### MESI 优化带来的问题

任何优化都是有代价的，这里经过了 store buffer 和invalid queue 优化后性能的确是有了提升，不过随之而来的也面临着一个问题，最初的MESI虽然整个过程是同步进行的，但是这样可以确保每个操作都真正意义上的执行了，从而保证了数据的强一致性。

但是加入了store buffer之后，就使得在修改操作完成后并不能保证缓存和内存的数据得到即时更新。 而在加入invalid queue之后，也使得其它CPU在修改了共享变量之后，并不能即时的把数据标记失效，这就可能造成在某一段时间内，不通过处理器之间还是会存在数据的不一致，整个数据变更的过程变成了弱一致性，而这两个问题就是导致并发问题的根源。


## 内存屏障

从上面得出的结论来看，我估计你会想到内存屏障会是个什么东西了，内存屏障就可以简单的认为它就是用来禁用我们的CPU缓存优化的，使用了内存屏障后，写入数据时候会保证所有的指令都执行完毕，这样就能保证修改过的数据能即时的暴露给其他的CPU。在读取数据的时候保证所有的“无效队列”消息都已经被读取完毕，这样就保证了其他CPU修改的数据消息都能被当前CPU知道，然后根据Invalid消息判断自己的缓存是否处于无效状态，这样就读取数据的时候就能正确的读取到最新的数据。

  

### Store Barrier(写屏障

强制所有在store屏障指令之前的store指令，都在该store屏障指令执行之前被执行，并把store缓冲区的数据都刷到CPU缓存。

结合上面的场景，这个指令其实就是告诉CPU，执行这个指令的时候需要把store buffer的数据都同步到内存中去。

  

### Load Barrier(读屏障

强制所有在load屏障指令之后的load指令，都在该load屏障指令执行之后被执行，并且一直等到load缓冲区被该CPU读完才能执行之后的load指令。

这个指令的意思是，在读取共享变量的指令前，先处理所有在失效队列中的消息，这样就保证了在读取数据之前所有失效的消息都得到了执行，从而保证自己是读取到的树是最新的。

  

### Full Barrier（全能屏障）

包含了Store Barrier 和Load Barrier的功能。


## JMM对内存屏障的支持

内存屏障提供了一套解决CPU缓存优化而导致的顺序性和可见性问题的方案，但是由于不同的硬件系统提供给的“内存屏障”指令都不一样，所以作为软件开发人员来说需要熟悉每个内存屏障的指令实在没必要，所以我们的JAVA语言把不同的内存屏障指令统一进行了封装，让我们的程序员不需要关心到系统的底层，只需要关心他们的自己的程序逻辑开发和如何使用这套规范即可，而封装这套解决方案的模型就是我们常说的Java内存模型JMM(Java Memory Model)。


## 总结：

好了最后我们把这些知识串在一起，以便我们理解这么多技术概念的衍生关系和内在逻辑。

**1、缓存可见性的问题：** 解决缓存可见性问题，本质上是要解决一个CPU修改了数据如何让其他CPU知道，然后多个CPU同时修改缓存数据如何保证他们操作的有序性。

**2、通过总线保证一致性**：通过总线嗅探机制，一个CPU修改了缓存其它CPU会收到对应的通知，从而解决了一个CPU修改了数据其他CPU不知道的问题，通过总线仲裁来保证多个CPU同时修改数据的顺序性，并且总线天然的独占性也保证了多个操作的互斥。

**3、MESI协议：** 因为通过总线来从主存读取数据的性能太慢，所以需要减少通过总线去读取主存的数据，尽量保证读取数据从自己或其他CPU缓存获得，修改数据尽量把多个操作合并为一个操作，所以MESI就通过对缓存数据的4个状态标记，来标识当前缓存行所处于的场景，针对不同的场景来执行不同的策略，最后达到缓存数据的一致性。

**4、MESI协议的优化**：MESI在修改数据的时候必须先广播，然后等待其他所有CPU都把数据标记失效后，才能进行数据的修改操作，这个过程比较耗费时间，所以为了提升CPU的利用率，同时减少广播等待的时间，就增加了store buffer 和失效队列来进行优化。

**5、内存屏障：** 虽然对于MESI优化，提升了整体的性能，但是同样也带来了一个问题，又原来的数据强一致性变成了弱一致性，从而导致在某些时候CPU缓存任然会存在不一致的情况。所以就需要一种机制来手动的禁用这种因为MESI优化带来的某些场景数据不一致的情况。 但是我们还是要乐观一点，因为绝大部分情况我们程序都不会存在问题，所以这种优化还是有意义的，这种个别的场景就需要我们的程序员来识别，然后通过内存屏障来保证数据的一致性。

**6、JMM：** 因为内存屏障是操作系统级别的指令，而不同的操作系统，内存屏障的指令又不一样，为了避免程序员花费太多的精力在这些内存屏障指令上，所以Java就封装了一套java的内存屏障，把不同操作系统的指令都封装在内，对程序员暴露的是一套统一的指令规范。java的内存模型(JMM)中的规范


单核共用的一个cpu缓存，不会有缓存可见性问题。

**将通知的过程由同步改为异步,但也将数据的强一致性变成弱一致性**

加入了store buffer之后，就使得在修改操作完成后并不能保证缓存和内存的数据得到即时更新。 而在加入invalid queue之后，也使得其它CPU在修改了共享变量之后，并不能即时的把数据标记失效，这就可能造成在某一段时间内，不通过处理器之间还是会存在数据的不一致，整个数据变更的过程变成了弱一致性，而这两个问题就是导致并发问题的根源。

```markdown
这里的优化是指对于mesi的优化，禁用缓存优化是指用内存屏障指令告诉mesi，更新数据是必须立即更新到主存（也就是把store buffer里的指令全部执行完），读取数据时必须读取最新的数据（也就是必须先把失效队列的数据先读取应用完）。
因为缓存锁的协议有多种，所以内存屏障体现的是一种语义，使用了内存屏障指令后，就是告诉计算机，不管你底层的缓存锁是什么协议实现的，你必须保证我写入的数据会立即更新到主存，我读取数据的时候必须保证我读取的是最新的数据。
```



# volatile
因为并不是所有程序都需要数据达到实时一致性，如果一个共享变量不存在多个线程并发读取和修改操作时，使用优化过的MESI协议是完全没有问题的，而共享变量是否会被多个线程并发访问和修改只有写程序的人可以判断，所以内存屏障就是提供给软件程序员的一套解决方案，当共享变量可能存在并发问题时，那么软件程序员就需要在对应共享变量的操作上前后加上对应的内存屏障，从而保证程序运行的正确性。

因为不同硬件架构下的内存屏障指令都有所不同，所以在Java为了屏蔽底层系统的复杂性就做了统一的封装，在JAVA里我们不需要知道底层系统那么多的内存屏障指令，只需要在共享变量的定义上加上volatile关键字，volatile关键字定义的共享变量在修改后立马就会暴露给其他CPU，并且能保证在读取volatile定义的共享变量一定是当前最新的数据。其原理就在于操作volatile定义的变量指令前后都会加上内存屏障。

  
最后总结一下，volatile 定义的变量其实包含三层语义：

1、用volatile定义的共享变量生成的指令不允许进行重排序，从而保证指令的顺序性。

2、在对volatile定义的变量进行修改时，会加上写屏障（或则全能屏障），从而保证修改的共享变量会马上对其他CPU暴露。

3、在对volatile定义变量进行读取的时候，会加上读屏障，从而保证读取的共享变量值是最新的。




# 理解锁
锁是什么？一个变量。线程A看见这个变量已经有主人了，它要么等待、要么去sleep、要么放弃，线程B释放锁就是将这个锁变量的主人重新置空。那么无论是获取锁的操作还是释放锁的操作，本身都是应该是原子的，应该是一个事务！我们平时更关心的是加锁和解锁之间的代码，那么上锁和解锁本身如何保证原子性？我只能说方式有很多，不过主流的实现方案是基于CAS指令。

应用层次的锁，解决的是多个进/线程并发访问同一块内存的问题，而CPU层面的锁解决了多个核心并发访问同一块内存的问题。由于应用层面的锁是对底层的封装与抽象，因此一旦锁获取失败，操作系统都可以通过系统调用挂起一个线程，让出CPU。
>起初程序是原子的，为了程序实现并发来提升效率，引入了“执行到一半”的第三种状态，而上锁的操作，本质上是使用一个原子指令来将锁变量置位，即保证程序原子性的原理就是**使用一个原子性指令来保证另外一堆非原子性指令的原子性**。

总结，锁就是一个变量，访问一个变量前先抢占锁，这种访问策略也称为悲观锁策略。阻塞和非阻塞主要指的是“抢占锁失败后”的处理策略。阻塞锁底层依赖系统调用（mutex互斥量），非阻塞锁一般都会继续尝试，这种锁也称为自旋锁。（既然都“上锁”了，那么无法上锁肯定是不退出的，尝试失败则退出一般称为tryLock，上锁的中途能够被外界中断则称为lockInterruptibly，这些都是Lock接口提供的行为，synchronized是没有的）

CAS是什么？比较与交换，它是一个指令，不管从低级的cpu指令还是高级的代码都能看见它的身影。
jdk层面的 CAS API 可以由unsafe类提供或者使用JUC的Atomic原子类提供的CAS相关方法。
CAS底层实现依赖处理器的指令集（cmpxchg），jdk的CAS方法无疑都采用本地实现，处理器的CAS是一条原子指令，也就是说比较和交换整个动作可以一次性完成。

CAS指令集需要三个操作数：需要修改资源的内存地址，预期值，目标值。
CPU访问内存地址，当资源的实际值等于预期值时，CPU将内存地址上的资源修改为目标值。由于CAS是处理器的单条指令，不会被打断，因此可以保证原子性。

CAS是无阻塞同步的一种解决方案，它可以实现乐观锁。我的理解：**CAS可以完成上锁操作本身**。
>CAS是实现“上锁”的一种方法，也可以使用关中断、testAndSet等

## 乐观锁和悲观锁
什么是悲观锁？
访问一个资源之前，一定要加锁，否则可能出现读写冲突或者写写冲突等线程安全问题。
这个锁可以是自旋锁——定义一个锁变量，CAS自旋去修改这个变量，如果修改失败就一直自旋着，其中上锁成功的线程就相当于进入了同步代码块。
这个锁也可以是互斥锁/阻塞锁——仍然是有一个锁变量，此时不再是无限CAS自旋，而是自旋若干次，如果修改失败就调用阻塞函数/系统调用，将线程阻塞起来，主动放弃CPU——这便是synchronized的基本原理

什么是乐观锁？
访问一个资源之前，认为没有竞争发生，不使用锁变量，而是直接CAS修改资源本身（读操作不需要加锁）。如果失败了如何进行后序处理看具体业务场景。


## 内存语义
CAS不仅可以用于线程同步，而且可以用于线程通信。
**CAS操作同时具有volatile读和volatile写的内存语义**，编译器不能对CAS前面和后面的任意指令进行重排序。如果程序运行在多处理器计算机，那么CAS操作被翻译为汇编指令时会加上lock前缀

**为什么cas操作的变量要加上，CAS不是有volatile写和读的效果吗**
	因为其他并发操作对该变量的读取可能是从工作内存中读取的，所以加上volatile强制让其他线程读该变量都是去主内存中读取最新值

CAS在x86处理器的大致写法是lock cmpxchg a,b,c 。
**单核处理器**是没必要写lock前缀的，因为cmpxchg本身是一个原子指令，这意味着执行这条指令时，能够一次性完成一次内存读和内存写，中间不会打断。
但是多核处理器下，加上lock前缀，意味着：
【1】将当前处理器缓存行的数据回写内存（写入内存前，通过锁总线，或者锁缓存行的方式保证同步）
【2】其他核心存储该变量的相应缓存行标记为invalidate（MESI协议）

>volatile和CAS可以说是JUC实现的基石  
（注意：cmpxchg不是一个特权指令，不需要切换内核态）






## lock前缀
volatile、CAS（synchronized底层也是基于CAS上锁的）被编译为汇编指令后（即时编译器），都会在相应指令前增加一个lock前缀，lock前缀正是这些关键字实现有序性和可见性的基础。
LOCK前缀在多核处理器中引发两件事
【1】让当前处理器缓存行的数据回写入主存
【2】其他核心维护该变量相应的缓存行过期/无效，下次取需要从主存中获取

CPU为了提升效率，通过增加高速缓存来缓解读写内存造成的（CPU计算和访存之间的）速度差，而告诉缓存的最小单位是缓存行，因此CPU读数据都是一块块读的，多核处理机中，每个核心都有自己独立的缓存（L1/L2），各个核心通过总线连接在一起，并且嗅探总线上信号，为了保证各个核心缓存行中的数据都是一致的，有两种解决思路（这也是lock前缀执行上的，两种实现方式）。
【1】锁总线
执行指令期间，核心发出Lock信号，总线仲裁机构该核心独占总线，而其他核心必须等待，代价很大，非主流方案。
【2】锁缓存，而且缓存之间需要遵守一个缓存一致性协议
各个CPU核心都是通过总线连接在一起的，每个核心都维护自己缓存的状态，一旦某个核心修改了自己缓存的内容，就会通过总线向其他核心发出信号，其他核心根据MESI协议修改相应的缓存行状态。

Lock前缀的汇编指令会强制写入主存，也可以避免前后指令的CPU重排序，并且及时让其他核心中的相应缓存行失效（从而利用MESI达到符合预期的效果）。
非lock前缀的汇编指令执行写操作时，可能不会立刻生效，因为存在写缓存区，lock前缀的指令在功能上可以等价内存屏障，让写操作立刻生效（或者说jvm插入内存屏障，平台通过CPU指令实现对应效果）。

总结：为什么volatile、synchronized、CAS等能保证可见性、有序性，因为它们共同的底层实现lock前缀，满足了MESI缓存一致性协议的触发条件，才使得变量具有缓存一致性。而普通的读写涉及各种优化，如写缓存、失效延迟处理等导致MESI条件无法触发，进而产生一系列数据不一致的问题。


#### 特点
这里我们讨论API层面的CAS。因为CAS可以分为CAS修改锁变量和CAS乐观锁，我们这里讨论CAS乐观锁，而这里乐观锁指代CAS自旋乐观锁。
在竞争不激烈的情况下，CAS可以提高系统的吞吐量——说白了，就是在一段时间内，让CPU多执行用户代码，少执行操作系统代码如（系统调用、切换上下文等）。如果竞争特别激烈，或者同步代码执行时间特别长，那么就使用自旋CAS乐观锁就是白白浪费CPU资源——虽然CPU一直在执行用户代码，但是执行循环啥也不干，还不如把CPU让给别人呢——这种情况不如主动申请阻塞、转让CPU给其他线程。
>当多个核心针对同一内存地址指向CAS指令时，其实他们是在试图修改每个核心自己维护的缓存行，假如两个核心同时同时对同一内存地址执行CAS指令，则他们都会尝试向其他核心发出invalidate，仲裁获胜的核心将先一步发出invalid，失败者则需要对自己的缓存行invalidate，读取胜利者修改后的内存值，CAS指令执行失败。
>因此**锁并没有消失，只是转嫁到了环总线上的总线仲裁协议上，而多核同时针对一个地址CAS会导致对应的缓存行频繁失效，降低性能，因此CAS不能滥用**

另外，CAS指令提供的总是**一个变量的内存地址**，也就是说乐观锁只能CAS修改某一个变量的值——不如独占锁变量，想怎么修改就怎么修改来的爽快啊。
>也可以将多个变量包装为一个对象（结构体），通过JUC的atomicReference来实现。当然了，肯定还是加锁更方便



## ABA
ABA问题：
首先，CAS指令的三个参数实际上都是内存地址，比较两个内存地址的值，然后考虑要不要把第一个内存地址的值修改为第三个内存地址的值，既然涉及到寻址，那么两次寻址之间必然具有时间间隔，我们只能保证CAS指令执行是原子的，在CPU寻址过程中（三个地址的寻址过程），源地址上的值从A变成B，再变成A是可能的。

造成以上问题的主要原因，是因为我们使用CAS时的逻辑就是:**值相同，就交换**。如果我们的业务禁止ABA问题，我们完全可以将CAS的逻辑更改为：**值+时间戳或版本号 相同，则交换。**

ABA解决思路就是：CAS输入不但考虑值本身，还附带具有标识意义的字段。例如JUC的atomicStampedReference（额外维护了一个时间戳）、mysql可以维护一个version字段（mybatis plus 提供了乐观锁功能，本质上就是维护额外版本号）

_总结：  
造成ABA问题的不是CAS指令本身，因为它只是一个原子指令，出现ABA问题不是执行CAS的时候，而是CPU为CAS指令加载值的过程中。_

```markdown
既然有MESI协议可以是保证cache一致性，为什么还需要volatile来保证可见性（内存屏障）？
有人回答，MESI协议是需要触发的，真的是这样吗？

volatile使用内存屏障来解决write buffer和invalidate queue带来的问题
首先volatile不单单能够保证可见性，还能保证有序性。此外，缓存一致性只作用于缓存，即L1、L2、L3 cache，不能作用于寄存器。

那是因为cpu写缓冲器不可见，以及无效队列的原因，你可以去了接下相关内容，cpu等待无效，和更新主存是写到写缓冲器的，至于执行时机是需要等到收到其他核心无效队列执行结果才会写入的，cpu把执行写入缓存指令，和无效队列中指令的时机交给开发者。volatite通过读写内存屏障保证了那一块过程的一致性




你好博主，有个缓存一致性的问题想向您请教一下。比如这样一个场景：有一个变量a=0；有两个线程同时在多核环境下去执行a++去更改这个变量。 按照缓存一致性是不是应该是这样一个过程：两个cpu都把a读到缓存，此时a是一个S的状态，现在两个cpu都要去修改a，但是只能有一个cpu（假设是cpu1）把a修改为M状态，另一个会变为I（失效状态），当cpu2再要去执行a++的时候，会去主存读取a(因为缓存中的a已经是失效状态)，这就会先触发cpu1缓存中的M状态的a（此时a=1）写回主存，然后cpu2才会读取到主存中a=1到缓存，这时两个cpu缓存中的a都变成S状态，然后cpu2再去做修改。---------过程如果是这样的话，最后输出的a一定是2了（测试结果并不是-。-）。我理解的这个过程什么地方有问题啊？


缓存一致性和原子性无关。a++操作由三个步骤组成：读a，加1，写a，如果两个线程同时执行a++，那么可能的顺序是：线程1执行读a，线程1执行加1，线程2执行读a，线程2执行加1，线程1写a，线程2写a，结果a=1。上述过程中，缓存一致性始终生效，但由于a++的非原子性，导致线程不安全。
MESI协议只是保证每个缓存中使用的共享变量的副本是一致的，而不会保证程序运行的原子性和内存可见性，你这种情况是需要考虑程序并发运行过程中的原子性和可见性问题，比如使用synchronized、lock、volatile等
```


## 写一个自旋锁
不要把自旋锁和乐观锁搞混，一般说乐观锁普遍指的是CAS自旋修改目标变量（不加锁直接修改资源），这里采用循环的方式更改锁变量

自旋锁使用场景：**并发度不高，临界代码执行时间不长的场景**。

这里使用计数器count实现可重入效果，如果直接使用布尔值表示状态那么就是不可重入锁，不可重入锁一旦连着调用两次lock()就会死锁，因此推荐使用可重入锁——避免死锁

这里使用计数器count实现可重入效果，如果直接使用布尔值表示状态那么就是不可重入锁，不可重入锁一旦连着调用两次lock()就会死锁，因此推荐使用可重入锁——避免死锁

```java
    private AtomicReference<Thread> owner = new AtomicReference<>();//保证内存可见性
    private int count=0;//重入次数

//上锁
    public void lock(){
        Thread cur = Thread.currentThread();
        if(owner.get()==cur){
            //重入
            count++;
            return;
        }
        //自旋获取锁:如果当前owner等于期望值null，则CAS设置为cur
        while (!owner.compareAndSet(null,cur)){
            System.out.println("自旋");
        }
    }

//解锁
    public void unlock(){
        Thread cur =Thread.currentThread();
        //持有该锁的线程才可以解锁
        if(owner.get()==cur){
            if(count>0){
                count--;
            }else {
                owner.set(null);
            }
        }

    }


```

Volatile修饰数组或者集合只能保证指针（地址）的内存可见性，如果某一个线程将指针修改指向，其他线程可以立即知道。
而元素修改的可见性应该使用atomic变量来保证——atomicArray和atomicReference
atomic类封装了unsafe类提供的CAS、putObjectVolatile等方法，便于非框架开发用户（普通程序员）的使用。

**保证内存可见性的arr[i]=newValue**
```java
public final void set(int i, E newValue) { 
    unsafe.putObjectVolatile(array, checkedByteOffset(i), newValue);
}

```

**CAS:如果arr[i]等于期望值except，则更新为update**
```java
private boolean compareAndSetRaw(long offset, E expect, E update) {
    return unsafe.compareAndSwapObject(array, offset, expect, update);
}

```


JUC的atomic就是基于unsafe类实现的，而且封装了unsafe类的各种方法，其中原子操作就是基于CAS自旋实现的（乐观锁）
Atomic调用unsafe方法，unsafe方法调用C语言，C语言再调用汇编语言，最终生成一条CPU指令cmpxchg，因此CAS是具有原子性，不会被打断。

atomicLong在**高并发**下，大量线程同时竞争更新同一个原子变量（因为long是64位的，底层会被拆分为两个32位，分别为高位和低位），CAS成功率小，失败的线程尝试自旋，会浪费很多CPU资源。（atomicDouble也有这样的问题）
**LongAdder是jdk8引入的，是对atomicLong的改进，在高并发场景更加高效。**

LongAdder可以概括成这样：内部核心数据value分离成一个数组(Cell)，每个线程访问时,通过哈希等算法映射到其中一个数字进行计数，而最终的计数结果，则为这个数组的求和累加。

**简单来说就是将一个值分散成多个部分，每个线程操作这个值的一部分，最后值相加，在并发的时候就可以分散压力（只有在线程哈希冲突时才会产生竞争），性能有所提高。**